{
    "step_size" : 0.001,
    "gradient_clip" : 1,
    "margin" : 0.05,
    "max_vocab_size" : 50000,
    "max_seq_length" : 200,
    "rnn_units" : 200,
    "hidden_dense_units" : 64,
    "hidden_fusion_units": 400,
    "embedding_size" : 100,
    "batch_size" : 128,
    "num_epochs" : 20,
    "optimizer" : "adam",
    "combine_type": "attention",
    "seq_embedding": "rnn",
    "kernel_size": 5,
    "loss_func": "cosine"
}
