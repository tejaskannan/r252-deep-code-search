returns total bytes written by this merge.
returns the first doc id greater than the provided after doc.
create a new binaryrange from a provided encoded binary range
highlights terms extracted from the provided query within the content of the provided field name
translate custom queries in queries that are supported by the unified highlighter.
must be called with increasing offset. see fieldhighlighter for usage.
can be invoked only after a call to preceding(offset+1). see fieldhighlighter for usage.
returns a breakiterator#getsentenceinstance(locale) bounded to maxlen. secondary boundaries are found using a breakiterator#getwordinstance(locale).
create a collapsing top docs collector on a org.apache.lucene.index.sorteddocvalues field. it accepts also org.apache.lucene.index.sortedsetdocvalues field but the collect will fail with an illegalstateexception if a document contains more than one value for the field. documents. document per collapsed key. this must be non-null, ie, if you want to groupsort by relevance use sort.relevance.
transform firstpassgroupingcollector#gettopgroups(int, boolean) output in of the first pass.
create a collapsing top docs collector on a org.apache.lucene.index.numericdocvalues field. it accepts also org.apache.lucene.index.sortednumericdocvalues field but the collect will fail with an illegalstateexception if a document contains more than one value for the field. documents. the collapsing keeps only the top sorted document per collapsed key. this must be non-null, ie, if you want to groupsort by relevance use sort.relevance.
returns a new collapsetopdocs, containing topn collapsed results across the provided collapsetopdocs, sorting by score. each collapsetopfielddocs instance must be sorted.
if we need to tie-break since score sort value are the same we first compare shard index (lower shard wins) and then iff shard index is the same we use the hit index.
add a byte to the sequence. the next byte in a sequence seen before as a sequence (only counts up to 255)
appends the separator if necessary.
creates a date using the calendar date format. specification reference: 5.2.3.
returns a formatter that outputs only those fields specified.  this method examines the fields provided and returns an iso-style formatter that best fits. this can be useful for outputting less-common iso styles, such as yearmonth (yyyy-mm) or monthday (--mm-dd).  the list provided may have overlapping fields, such as dayofweek and dayofmonth. in this case, the style is chosen based on the following list, thus in the example, the calendar style is chosen as dayofmonth is higher in priority than dayofweek:  monthofyear - calendar date style dayofyear - ordinal date style weekofweekyear - week date style dayofmonth - calendar date style dayofweek - week date style year weekyear  the supported formats are:  extended basic fields 2005-03-25 20050325 yearmonthofyeardayofmonth 2005-03 2005-03 yearmonthofyear 2005--25 2005--25 yeardayofmonth 2005 2005 year --03-25 --0325 monthofyeardayofmonth --03 --03 monthofyear ---03 ---03 dayofmonth 2005-084 2005084 yeardayofyear -084 -084 dayofyear 2005-w12-5 2005w125 weekyearweekofweekyeardayofweek 2005-w-5 2005w-5 weekyeardayofweek 2005-w12 2005w12 weekyearweekofweekyear -w12-5 -w125 weekofweekyeardayofweek -w12 -w12 weekofweekyear -w-5 -w-5 dayofweek 10:20:30.040 102030.040 hourminutesecondmilli 10:20:30 102030 hourminutesecond 10:20 1020 hourminute 10 10 hour -20:30.040 -2030.040 minutesecondmilli -20:30 -2030 minutesecond -20 -20 minute --30.040 --30.040 secondmilli --30 --30 second ---.040 ---.040 milli 10-30.040 10-30.040 hoursecondmilli 10:20-.040 1020-.040 hourminutemilli 10-30 10-30 hoursecond 10--.040 10--.040 hourmilli -20-.040 -20-.040 minutemilli plus datetime formats like datettime  indicates that this is not an official iso format and can be excluded by passing in strictiso as true.  this method can side effect the input collection of fields. if the input collection is modifiable, then each field that was added to the formatter will be removed from the collection, including any duplicates. if the input collection is unmodifiable then no side effect occurs.  this side effect processing is useful if you need to know whether all the fields were converted into the formatter or not. to achieve this, pass in a modifiable list, and check that it is empty on exit. updated by the method call unless unmodifiable, removing those fields built in the formatter
creates a date using the ordinal date format. specification reference: 5.2.2.
creates a date using the calendar date format. specification reference: 5.2.1.
checks that the iso only flag is not set, throwing an exception if it is.
adds the time fields to the builder. specification reference: 5.3.1.
check that the current stack has specialpermission access according to the securitymanager.
extracts a sorted list of declared version constants from a class. the argument would normally be version.class but is exposed for testing with other classes-containing-version-constants.
return the version of elasticsearch that has been used to create an index given its settings. @value indexmetadata#setting_version_created
returns the minimum compatible version based on the current version. ie a node needs to have at least the return version in order to communicate with a node running the current version. the returned version is in most of the cases the smallest major version release unless the current version is a beta or rc release then the version itself is returned.
returns the version given its string representation, current version if the argument is null or empty
returns true iff both version are compatible. otherwise false
if the specified cause is an unrecoverable error, this method will rethrow the cause on a separate thread so that it can not be caught and bubbles up to the uncaught exception handler.
unwrap the specified throwable looking for any suppressed errors or errors as a root cause of the specified throwable.
throws the specified exception. if null if specified then true is returned.
throws a runtime exception with all given exceptions added as suppressed. if the given list is empty no exception is thrown
deduplicate the failures by exception message and index.
adds a new piece of metadata with the given key. if the provided key is already present, the corresponding metadata will be replaced
parses the output of #generatefailurexcontent(xcontentbuilder, params, exception, boolean)
returns a underscore case name for the given exception. this method strips elasticsearch prefixes from exception names.
serializes the given exceptions stacktrace elements as well as it's suppressed exceptions to the given output stream.
adds a new header with the given key. this method will replace existing header if a header with the same key already exists
render any exception as a xcontent, encapsulated within a field or object named "error". the level of details that are rendered depends on the value of the "detailed" parameter: when it's false only a simple message based on the type and message of the exception is rendered. when it's true all detail are provided including guesses root causes, cause and potentially stack trace. this method is usually used when the exception is rendered as a full xcontent object, and its output can be parsed by the #failurefromxcontent(xcontentparser) method.
returns the root cause of this exception or multiple if different shards caused different exceptions. if the given exception is not an instance of org.elasticsearch.elasticsearchexception an empty array is returned.
returns true iff the given class is a registered for an exception to be read.
retrieve the innermost cause of this exception, if none, returns the current exception.
generate a elasticsearchexception from a xcontentparser. this does not return the original exception type (ie nodeclosedexception for example) but just wraps the type, the reason and the cause of the exception. it also recursively parses the tree structure of the cause, returning it as a tree structure of elasticsearchexception instances.
returns the root cause of this exception or multiple if different shards caused different exceptions
return the detail message, including the message from the nested exception if there is one.
deserializes stacktrace elements as well as suppressed exceptions from the given output stream and adds it to the given exception.
returns an array of all registered pairs of handle ids and exception classes. these pairs are provided for every registered exception.
check whether this exception contains an exception of the given type: either it is of the given class itself or it contains a nested cause of the given type.
test that some values like arrays of numbers are ignored when parsing back an exception.
deletes all meta state directories recursively for the given data locations
reads the state from a given file and compares the expected version against the actual version of the state.
tries to load the latest state from the given data-locations. it tries to load the latest state determined by the states version from one or more data directories and if none of the latest states can be loaded an exception is thrown to prevent accidentally loading a previous state and silently omitting the latest state.
writes the given state to the given directories. the state is written to a state directory (@value #state_dir_name) underneath each of the given file locations and is created if it doesn't exist. the state is serialized to a temporary file in that directory and is then atomically moved to it's target filename of the pattern prefixversion.st.
verifies that when there is a full match (syncid and files) we allocate it to matching node.
when there is no decision or throttle decision across all nodes for the shard, make sure the shard moves to the ignore unassigned list.
when we can't find primary data, but still find replica data, we go ahead and keep it unassigned to be allocated. this is today behavior, which relies on a primary corruption identified with adding a replica and having that replica actually recover and cause the corruption to be identified see corruptfiletest#
verifies that when there is primary data, but no matching data at all on other nodes, the shard keeps unassigned to be allocated later on.
verifies that when there is no sync id match but files match, we allocate it to matching node.
verifies that when there is primary data, but no data at all on other nodes, the shard keeps unassigned to be allocated later on.
tests when the node to allocate to due to matching is being throttled, we move the shard to ignored to wait till throttling on it is done.
verifies that when there is a sync id match but no files match, we allocate it to matching node.
verifies that when we are still fetching data in an async manner, the replica shard moves to ignore unassigned.
verifies that for anything but index creation, fetch data ends up being called, since we need to go and try and find a better copy for the shard.
verifies that on index creation, we don't go and fetch data, but keep the replica shard unassigned to let the shard allocator to allocate it. there isn't a copy around to find anyhow.
returns a prioritycomparator that uses the routingallocation index metadata to access the index setting per index.
writes the index state. this method is public for testing purposes.
loads the full state, which includes both the global state and all the indices meta state.
writes the global state, without the indices states.
loads all indices states available on disk
tests that when there is a node to allocate to, but it is throttling (and it is the only one), it will be moved to ignore unassigned until it can be allocated to.
tests that when there is a node to allocate the shard to, it will be allocated to it.
tests that when there is a node to be allocated to, but it the decider said "no", we still force the allocation to it.
tests that when the nodes with prior copies of the given shard all return a decision of no, and returns a no or throttle decision for a node, then we do not force allocate to that node.
tests when the node returns that no data was found for it (null for allocation id), it will be moved to ignore unassigned.
tests that when the nodes with prior copies of the given shard return a throttle decision, then we do not force allocate to that node but instead throttle.
tests that when there was a node that previously had the primary, it will be allocated to that same node again.
tests that when restoring from a snapshot and we find a node with a shard copy and allocation deciders say yes, we allocate to that node.
tests when the node returns data with a shard allocation id that does not match active allocation ids, it will be moved to ignore unassigned.
tests that when one node returns a shardlockobtainfailedexception and another properly loads the store, it will select the second node as target
tests that when async fetch returns that there is no data, the shard will not be allocated.
tests when the node returns that no data was found for it, it will be moved to ignore unassigned.
tests that when restoring from a snapshot and we find a node with a shard copy but allocation deciders say no, we still allocate to that node.
tests that when the nodes with prior copies of the given shard all return a decision of no, but returns a yes decision for at least one of those no nodes, then we force allocate to one of them
tests that when restoring from a snapshot and we find a node with a shard copy and allocation deciders say throttle, we add it to ignored shards.
tests that when restoring from a snapshot and we don't find a node with a shard copy, the shard will remain in the unassigned list to be allocated later.
tests that when the node returns a shardlockobtainfailedexception, it will be considered as a valid shard copy
is the allocator responsible for allocating the given shardrouting?
split the list of node shard states into groups yesnothrottle based on allocation deciders
builds a map of nodes to the corresponding allocation decisions for those nodes.
builds a list of nodes. if matchanyshard is set to false, only nodes that have an allocation id matching insyncallocationids are added to the list. otherwise, any node that has a shard is added to the list, but entries with matching allocation id are always at the front of the list.
finds the store for the assigned shard in the fetched data, returns null if none is found.
is the allocator responsible for allocating the given shardrouting?
process existing recoveries of replicas and see if we need to cancel them if we find a better match. today, a better match is one that has full sync id match compared to not having one in the previous recovery.
determines if the shard can be allocated on at least one node based on the allocation deciders. returns the best allocation decision for allocating the shard on any node (i.e. yes if at least one node decided yes, throttle if at least one node decided throttle, and no if none of the nodes decided yes or throttle). if in explain mode, also returns the node-level explanations as the second element in the returned tuple.
takes the store info for nodes that have a shard store and adds them to the node decisions, leaving the node explanations untouched for those nodes that do not have any store information.
process any changes needed to the allocation based on this fetch result.
called by the response handler of the async action to fetch data. verifies that its still working on the same cache generation, otherwise the results are discarded. it then goes and fills the relevant data for the shard (response + failures), issuing a reroute at the end of it to make sure there will be another round of allocations taking this new data into account.
returns the number of async fetches that are currently ongoing.
are there any nodes that are fetching data?
finds all the nodes that need to be fetched. those are nodes that have no data, and are not in fetch mode.
fetches the data for the relevant shard. if there any ongoing async fetches going on, or new ones have been initiated by this call, the result will have no data.  the ignorenodes are nodes that are supposed to be ignored for this round, since fetching is async, we need to keep them around and make sure we add them back when all the responses are fetched and returned.
fills the shard fetched data with new (data) nodes and a fresh nodeentry, and removes from it nodes that are no longer part of the state.
test peer reuse on recovery. this is shared between recoverfromgatewayit and recoverybackwardscompatibilityit. settings for the index to test runnable that will restart the cluster under test logger for logging should this use synced flush? can't use synced from in the bwc tests
ensure we can read a pre-generated cluster state.
allocate unassigned shards to nodes (if any) where valid copies of the shard already exist. it is up to the individual implementations of #makeallocationdecision(shardrouting, routingallocation, logger) to make decisions on assigning shards to nodes.
builds decisions for all nodes in the cluster, so that the explain api can provide information on allocation decisions for each node, while still waiting to allocate the shard (e.g. due to fetching shard data).
computes and returns the design for allocating a single unassigned shard. if called on an assigned shard,
elasticsearch 2.0 removed several deprecated features and as well as support for lucene 3.x. this method calls metadataindexupgradeservice might also update obsolete settings if needed. allows upgrading global custom meta data via metadataupgrader#custommetadataupgraders
loads the current meta state for each index in the new cluster state and checks if it has to be persisted. each index state that should be written to disk will be returned. this is only run for data only nodes. it will return only the states for indices that actually have a shard allocated on the current node.
throws an iae if a pre 0.19 state is detected
this test really tests worst case scenario where we have a broken setting or any setting that prevents an index from being allocated in our metadata that we recover. in that case we now have the ability to check the index on local recovery from disk if it is sane and if we can successfully create an indexservice. this also includes plugins etc.
this test really tests worst case scenario where we have a missing analyzer setting. in that case we now have the ability to check the index on local recovery from disk if it is sane and if we can successfully create an indexservice. this also includes plugins etc.
this test ensures that when an index deletion takes place while a node is offline, when that node rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.
allocates the provided list of the dangled indices by sending them to the master node for allocation.
cleans dangling indices if they are already allocated on the provided meta data.
finds new dangling indices by iterating over the indices and trying to find indices that have state on disk, but are not part of the provided meta data, or not detected as dangled already.
process dangling indices based on the provided meta data, handling cleanup, finding new dangling indices, and allocating outstanding ones.
asserts that the cluster state contains nbtasks tasks that verify the given predicate
cancels a locally running task using the task manager api
notifies the master node to create new persistent task and to assign it to a node.
waits for a given persistent task to comply with a given predicate, then call back the listener accordingly.
executes an asynchronous persistent task action using the client.  the origin is set in the context and the listener is wrapped to ensure the proper context is restored
waits for persistent tasks to comply with a given predicate, then call back the listener accordingly.
notifies the master node about the completion of a persistent task.  when failure is null, the persistent task is considered as successfully completed.
notifies the master node that the state of a persistent task has changed.  persistent task implementers shouldn't call this method directly and use
reassigns the task to another node
updates the task state
checks if the task is currently present in the list and has the right allocation id
adds a new task to the builder  after the task is added its id can be found by calling #getlastallocationid() method.
removes the task
finds the least loaded node that satisfies the selector criteria
returns the node id where the params has to be executed,  the default implementation returns the least loaded data node
returns true if the cluster state change(s) require to reassign some persistent tasks. it can happen in the following situations: a node left or is added, the routing table changed, the master node changed, the metadata changed or the persistent tasks changed.
restarts a record about a running persistent task from cluster state
removes the persistent task
update the state of a persistent task
evaluates the cluster state and tries to assign tasks to nodes.
creates a new assignment for the given persistent task.
creates a new persistent task on master node
waits for a given persistent task to comply with a given predicate, then call back the listener accordingly.
unregisters and then cancels the locally running task using the task manager. no notification to master will be send upon cancellation.
creates a persistenttasksclusterservice with a single persistenttasksexecutor implemented by a bifunction
returns a assignmentdecision whether the given persistent task can be assigned to a node of the cluster. the decision depends on the current value of the setting
test that the enableassignmentdecider#cluster_tasks_allocation_enable_setting setting correctly prevents persistent tasks to be assigned after a cluster restart.
format permission type, name, and actions into a string
printsconfirms policy exceptions with the user
parses plugin policy into a set of permissions. each permission is formatted for output to users.
construct plugin info from a stream.
reads the plugin descriptor file.
the path to the native controller for a plugin with native components.
return the platform name based on the os name and - darwin-x86_64 - linux-x86-64 - windows-x86_64 for nix platforms this is more-or-less `uname -s`-`uname -m` converted to lower case. however, for consistency between different operating systems on the same architecture "amd64" is replaced with "x86_64" and "i386" with "x86". for windows it's "windows-" followed by either "x86" or "x86_64".
test that we can parse the set of permissions correctly for a simple policy
test that we can parse the set of permissions correctly for a complex policy
test that we can format some simple permissions properly
remove the plugin specified by pluginname.
creates a test environment with bin, config and plugins directories.
creates a fake jar file with empty class files
convenience method to write a plugin properties file
verify the signature of the downloaded plugin zip. the signature is obtained from the source of the downloaded plugin by appending ".asc" to the url. it is expected that the plugin is signed with the elastic signing key with id d27d666cd88e42b4.
downloads the plugin and returns the file it was downloaded to.
check a candidate plugin for jar hell before installing it
copies the files from tmpconfigdir into destconfigdir. any files existing in both the source and destination will be skipped.
load information about the plugin, and verify it can be installed with no errors.
downloads a zip from the url. this method also validates the downloaded plugin zip via the following means:   for an official plugin we download the sha-512 checksum and validate the integrity of the downloaded zip. we also download the armored signature and validate the authenticity of the downloaded zip.   for a non-official plugin we download the sha-512 checksum and fallback to the sha-1 checksum and validate the integrity of the downloaded zip.  
returns all the official plugin names that look similar to pluginid.
moves the plugin directory into its final destination.
installs the plugin from tmproot into the plugins dir. if the plugin has a bin dir andor a config dir, those are moved.
moves bin and config directories from the plugin if they exist
returns the url for an official elasticsearch plugin.
copies the files from tmpbindir into destbindir, along with permissions from dest dirs parent.
constructs a new pluginservice
returns all classes injected into guice by plugins which extend lifecyclecomponent.
extracts all installed plugin directories from the provided rootpath.
verify the given plugin is compatible with the current elasticsearch installation.
mark an analysisprovider as requiring the index's settings.
defines a azureunicasthostsprovider for testing purpose that is able to resolve network addresses for azure instances running on the same host but different ports.
register an existing node as a azure node, exposing its address and details htrough
attempts to do a scattergather request that expects unique responses per sub-request.
run a request that receives a predictably randomized number of deprecation warnings.  re-running this back-to-back helps to ensure that warnings are not being maintained across requests.
for an options request to a valid rest endpoint, verify that a 200 http response code is returned, and that the response 'allow' header includes a list of valid http methods for the endpoint (see  href="https:tools.ietf.orghtmlrfc2616#section-9.2">http1.1 - 9.2 - options).
test if a post request to index_settings matches the update settings handler for index_settings, and returns a 405 error (see  href="https:github.comelasticelasticsearchissues17853">issue 17853 for more information).
for requests to a valid rest endpoint using an unsupported http method, verify that a 405 http response code is returned, and that the response 'allow' header includes a list of valid http methods for the endpoint (see  href="https:tools.ietf.orghtmlrfc2616#section-10.4.6">http1.1 - 10.4.6 - 405 method not allowed).
turns the given bytesreference into a bytebuf. note: the returned bytebuf will reference the internal pages of the bytesreference. don't free the bytes of reference before the bytebuf goes out of scope.
returns whether the input origin is allowed by this configuration.
returns http response headers that should be added to a cors preflight response.
creates a new builder instance with the origin passed in.
builds a niocorsconfig with settings specified by previous method calls.
creates a niocorsconfigbuilder instance with the specified origin.
sole constructor.
create a niocorsconfigbuilder instance with the specified pattern origin.
returns http response headers that should be added to a cors preflight response. an intermediary like a load balancer might require that a cors preflight request have certain headers set. this enables such headers to be added.
creates a new instance with the specified niocorsconfig.
returns whether the input origin is allowed by this configuration.
returns http response headers that should be added to a cors preflight response.
creates a new builder instance with the origin passed in.
builds a netty4corsconfig with settings specified by previous method calls.
creates a netty4corsconfigbuilder instance with the specified origin.
sole constructor.
create a netty4corsconfigbuilder instance with the specified pattern origin.
returns http response headers that should be added to a cors preflight response. an intermediary like a load balancer might require that a cors preflight request have certain headers set. this enables such headers to be added.
creates a new instance with the specified netty4corsconfig.
like assertacessisdenied, but for _bulk requests since the entire request will not be failed, just the individual ones
this test tries to truncate some of larger files in the index to trigger leftovers on the recovery target. this happens during recovery when the last chunk of the file is transferred to the replica we just throw an exception to make sure the recovery fails and we leave some half baked files on the target. later we allow full recovery to ensure we can still recover and don't run into corruptions.
resolve the given index names to index ids, creating new index ids for new indices in the repository.
reads an instance of repositorydata from x-content, loading the snapshots and indices metadata.
writes the snapshots metadata and the related indices metadata to x-content, omitting the incompatible snapshots.
returns an immutable collection of all the snapshot ids in the repository, both active and incompatible snapshots.
remove a snapshot and remove any indices that no longer exist in the repository due to the deletion of the snapshot.
resolve the index name to the index id specific to the repository, throwing an exception if the index could not be resolved.
returns an immutable collection of the snapshot ids for the snapshots that contain the given index.
resolve the given index names to index ids.
reads the incompatible snapshot ids from x-content, loading them into a new instance of repositorydata that is created from the invoking instance, plus the incompatible snapshots that are read from x-content.
add a snapshot and its indices to the repository; returns a new instance. if the snapshot already exists in the repository data, this method throws an illegalargumentexception.
writes the incompatible snapshot ids to x-content.
unregisters repository in the cluster  this method can be only called on the master node. it removes repository information from cluster metadata.
registers new repository in the cluster  this method can be only called on the master node. it tries to create a new repository on the master and if it was successful it adds new repository to cluster metadata.
creates repository holder
returns registered repository  this method is called only on the master node
checks if new repositories appeared in or disappeared from cluster metadata and updates current list of repositories accordingly.
creates a new repository and adds it to the list of registered repositories.  if a repository with the same name but different types or settings already exists, it will be closed and replaced with the new repository. if a repository with the same name exists but it has the same type and settings the new repository is ignored.
constructs a shared file system repository.
parse and read all settings available under the azure.client. namespace
parse settings for a single client.
azure enumerationresults response
retrieves the object name from all derived paths named pathx where 0 <= x  10.  this is the counterpart of #objectspaths(string)
decline a path like "http:host:portbucket" into 10 derived paths like: - http:host:portbucketpath0 - http:host:portbucketpath0path1 - http:host:portbucketpath0path1path2 - etc
azure error  https:docs.microsoft.comen-usrestapistorageservicesstatus-and-error-codes2
builds the default request handlers
creates a azurestoragefixture with a custom endpoint
test if the given string starts with the specified prefix, ignoring upperlower case.
creates a cloudblobclient on each invocation using the current client settings. cloudblobclient is not thread safe and the settings can change, therefore the instance is not cache-able and should only be reused inside a thread for logically coupled ops. the operationcontext is used to specify the proxy, but a new context is required for each call.
extract the blob name from a uri like https:myservice.azure.netcontainerpathtomyfile it should remove the container part (first part of the path) and gives pathtomyfile
makes sure that the url is white listed or if it points to the local file system it matches one on of the root path in path.repo
constructs a read-only url-based repository
this method registers 3 snapshotrestore repositories: - repository-fs: this fs repository is used to create snapshots. - repository-url: this url repository is used to restore snapshots created using the previous repository. it uses the urlfixture to restore snapshots over http. - repository-file: similar as the previous repository but using a file: prefix instead of http:.
loads information about shard snapshot
checks if snapshot file already exists in the list of blobs
snapshot individual file
this is a bwc layer to ensure we update the snapshots metadata with the corresponding hashes before we compare them. the new logic for storefilemetadata reads the entire .si and segments.n files to strengthen the comparison of the files on a per-segment per-commit level.
maintains single lazy instance of blobstore
constructs new context
configures ratelimiter based on repository and global settings
constructs new blobstorerepository
finds the next available blob number
create snapshot from index commit point
delete shard snapshot
writes the incompatible snapshot ids list to the `incompatible-snapshots` blob in the repository. package private for testing.
restores a file this is asynchronous method. upon completion of the operation latch is getting counted down and any failures are added to the failures list
loads all available snapshots in the repository
performs restore operation
writes a new index file for the shard and removes all unreferenced files from the repository. we need to be really careful in handling index files in case of failures to make sure we don't have index file that points to files that were deleted.
reads blob with specified name without resolving the blobname using using #blobname method.
create a repository with a random name
restoring a snapshot that contains multiple files must succeed even when some files already exist in the shard's store.
attempts to retrieve a client by name from the cache. if the client does not exist it will be created.
uploads a blob using multipart upload requests.
returns the number parts of size of partsize needed to reach totalsize, along with the size of the last (or unique) part. the size of the last part
uploads a blob using a single upload request
constructs canned acl from string
retrieves the object name from all derives paths named pathx where 0 <= x < 10. this is the counterpart of #objectspaths(string)
decline a path like "http:host:portbucket" into 10 derived paths like: - http:host:portbucketpath0 - http:host:portbucketpath0path1 - http:host:portbucketpath0path1path2 - etc
s3 error https:docs.aws.amazon.comamazons3latestapierrorresponses.html
s3 deleteresult response
builds the default request handlers
s3 listbucketresult response
creates a amazons3fixture
constructs an s3 backed repository
creates a new s3blobstore with random settings.  the blobstore uses a mockamazons3 client.
load all client settings from the given settings. note this will always at least return a client named "default".
parse settings for a single client.
executes the provided operation against this store
locates the keytab file in the environment and verifies that it exists. expects keytab file to exist at $config_dir$repository-hdfskrb5.keytab
performs a two-phase leading namenode transition.
get a given setting from the repository settings, throwing a repositoryexception if the setting does not exist or is empty.
generates a given number of googlecloudstorageclientsettings along with the settings to build them from
generates a random googlecredential along with its corresponding service account file provided as a byte array
generates a random googlecloudstorageclientsettings along with the settings to build it
loads the service account file corresponding to a given client name. if no file is defined for the client, a null credential is returned. the settings the client name
uploads a blob using the "resumable upload" method (multiple requests, which can be independently retried in case of failure, see https:cloud.google.comstoragedocsjson_apiv1how-tosresumable-upload
return true iff the given bucket exists
deletes multiple blobs from the specific bucket using a batch request
deletes the blob from the specific bucket
returns an java.io.inputstream for the given blob name
uploads a blob using the "multipart upload" method (a single 'multipartrelated' request containing both data and metadata. the request is gziped), see: https:cloud.google.comstoragedocsjson_apiv1how-tosmultipart-upload
list all blobs in the specific bucket with names prefixed base path of the blobs to list. this path is removed from the names of the blobs returned.
storage error json representation
builds a json response
storage object json representation as defined in https:cloud.google.comstoragedocsjson_apiv1objects#resource
builds the default request handlers
creates a googlecloudstoragefixture
pins the tls trust certificates and, more importantly, overrides connection urls in the case of a custom endpoint setting because some connections don't fully honor this setting (bugs in the sdk). the default connection factory opens a new connection for each request. this is required for the storage instance to be thread-safe.
refreshes the client settings and clears the client cache. subsequent calls to using the parameter settings.
attempts to retrieve a client from the cache. if the client does not exist it will be created from the latest settings and will populate the cache. the returned instance should not be cached by the calling code. instead, for each use, the (possibly updated) instance should be requested by calling this method. (blobs)
creates a client that can be used to manage google cloud storage objects. the client is thread-safe. (blobs)
converts an array of strings to an array of paths, adding an additional child if specified
returns all index paths.
tries to lock the given shards id. a shard lock is required to perform any kind of write operation on a shards data directory like deleting files, creating a new index writer or recover from a different shard instance into it. if the shard lock can not be acquired a shardlockobtainfailedexception is thrown
tries to lock all local shards for the given index. if any of the shard locks can't be acquired a shardlockobtainfailedexception is thrown and all previously acquired locks are released.
returns all shard paths excluding custom shard path. note: shards are only allocated on one of the returned paths. the returned array may contain paths to non-existing directories.
returns all currently lock shards. note: the shard ids return do not contain a valid index uuid
returns an array of all of the nodepaths.
this is a best effort to ensure that we actually have write permissions to write in all our data directories. this prevents disasters if nodes are started under the wrong username etc.
resolves all existing paths to indexfoldername in $data.pathsnodesnode.idindices
deletes a shard data directory iff the shards locks were successfully acquired.
acquires, then releases, all write.lock files in the given shard paths. the "write.lock" file is assumed to be under the shard path's "index" directory as used by elasticsearch.
find all the shards for this index, returning a map of the nodepath to the number of shards on that path
tries to find all allocated shards for the given index on the current node. note: this methods is prone to race-conditions on the filesystem layer since it might not see directories created concurrently or while it's traversing.
returns all folder names in $data.pathsnodesnode.idindices folder
resolve the custom path for a index's shard. uses the indexmetadata.setting_data_path setting to determine the root path for the index.
deletes an indexes data directory recursively. note: this method assumes that the shard lock is acquired
scans the node paths and loads existing metadata file. if not found a new meta data will be generated and persisted into the nodepaths
deletes a shard data directory. note: this method assumes that the shard lock is acquired. this method will also attempt to acquire the write locks for the shard's paths before deleting the data, but this is best effort, as the lock is released before the deletion happens in order to allow the folder to be deleted
this method tries to write an empty file and moves it using an atomic move operation. this method throws an illegalstateexception if this operation is not supported by the filesystem. this test is executed on each of the data directories. this method cleans up all files even in the case of an error.
returns an array of all of the nodes data locations.
return all directory names in the nodesnode.idindices directory for the given node path.
returns the unique uuid describing this node. the uuid is persistent in the data folder of this node and remains across restarts.
returns the nodepath.path for this shard.
ensure the configured temp directory is a valid directory
checks if the specified url is pointing to the local file system and if it does, resolves the specified url against the list of configured repository roots if the specified url doesn't match any of the roots, returns null.
the lines from procselfcgroup. this file represents the control groups to which the elasticsearch process belongs. each line in this file represents a control group hierarchy of the form   with the first field representing the hierarchy id, the second field representing a comma-separated list of the subsystems bound to the hierarchy, and the last field representing the control group.
reads a file containing a single line.
returns the amount of free physical memory in bytes.
a map of the control groups to which the elasticsearch process belongs. note that this is a map because the control groups can vary from subsystem to subsystem. additionally, this map can not be cached because a running process can be reclassified.
the cpu time statistics for all tasks in the elasticsearch control group.
returns the lines from cpu.stat for the control group to which the elasticsearch process belongs for the cpu subsystem. these lines represent the cpu time statistics and have the form  nr_periods \d+ nr_throttled \d+ throttled_time \d+  where nr_periods is the number of period intervals as specified by cpu.cfs_period_us that have elapsed, nr_throttled is the number of times tasks in the given control group have been throttled, and throttled_time is the total time in nanoseconds for which tasks in the given control group have been throttled.
basic cgroup stats.
the system load averages as an array. on windows, this method returns null. on linux, this method returns the 1, 5, and 15-minute load averages. on macos, this method should return the 1-minute load average.
returns the total amount of swap space in bytes.
returns the amount of free swap space in bytes.
returns the total amount of physical memory in bytes.
read from a stream.
read from a stream.
returns the size (in bytes) of virtual memory that is guaranteed to be available to the running process
returns the cpu time (in milliseconds) used by the process on which the java virtual machine is running, or -1 if not supported.
returns the maximum number of file descriptors allowed on the system, or -1 if not supported.
returns the number of opened file descriptors associated with the current process, or -1 if not supported.
runs the command with the given args. output can be found in #terminal. the command created can be found in #command.
prints a help message for the command to the terminal.
parses options for this command from args and executes it.
executes the command, but all errors are thrown.
wipes the input and output.
prints message to the terminal at verbosity level, without a newline.
prompt for a yes or no answer from the user. this method will loop until 'y' or 'n' (or the default empty value) is entered.
create an environment for the command to use. overrideable for tests.
ensure the given setting exists, reading it from system properties if not already set.
with respect to license expiry date
calculates the delay for the next trigger time. when now is in a valid time bracket with respect to expirationdate, the delay is 0. when now is before the time bracket, than delay to the start of the time bracket and when now is passed the valid time bracket, the delay is null
parses license from json format to an instance of license
this method which chooses the license type randomly if the type is null. however, it will not randomly choose trial or basic types as those types can only be self-generated.
updates the current state of the license, which will change what features are available. may be null if they have never generated a trial license on this cluster, or the most recent trial was prior to this metadata being tracked (6.1)
extract the list of remote cluster aliases from the list of index names. remote index names are of the form
checks the specified clusters for license compatibility. the specified callback will be invoked once if all clusters are license-compatible, otherwise the specified callback will be invoked once on the first cluster that is not license-compatible.
constructs an error message for license incompatibility.
creates a license state with the given license type and active state, and checks the given method returns expected.
registers new license in the cluster master only operation. installs a new license on the master provided it is valid
notifies registered licensees of license state change andor new active license based on the license in currentlicensesmetadata. additionally schedules license expiry notifications and event callbacks relative to the current license's expiry
remove license from the cluster state metadata
when there is no global block on org.elasticsearch.gateway.gatewayservice#state_not_recovered_block notify licensees and issue auto-generated license if no license has been installedissued yet.
verifies the license content with the signature using the packaged public key
exception to be thrown when a feature action requires a valid license, but license has expired feature accessible through #expired_feature_metadata in the exception's rest header
checks if the signature of a self generated license with older version needs to be recreated with the new key
there will be only one license displayed per feature, the selected license will have the latest expiry_date out of all other licenses for the feature.  the licenses are sorted by latest issue_date
read public key file content
decrypts provided encrypteddata with passphrase
encrypts provided data with passphrase
read encrypted private key file content with provided passphrase
returns true if the license was auto-generated (by license plugin), false otherwise
the license mode file
returns true iff the license is a production licnese
generates a signature for the licensespec. signature structure:  | version | magic | pub_key_digest | signed_license_content | 
implements hashcode for map of string arrays the map of string arrays does't work with hashcode.
implements equals for a map of string arrays the map of string arrays is used in some xpack protocol classes but does't work with equal.
write just the given user, but not the inner #authenticateduser.
compare all the fields.
compare all the fields.
equality test
compare all the fields.
compare all the fields and embedded anomaly records (if any)
compare all the fields and embedded anomaly records (if any)
parse a categorization_analyzer configuration. a custom parser is needed due to the complexity of the format, with many elements able to be specified as either the name of a built-in element or an object containing a custom definition.
this method is only used in the unit tests - in production code this config is always parsed as a fragment.
builds a job.
returns the default description for the given detector
appends to the given stringbuilder the default description for the given detector
parse out a date object given the current parser and field name.
the lists of indices and types are compared for equality but they are not sorted first so this test could fail simply because the indices and types lists are in different orders.
the lists of indices and types are compared for equality but they are not sorted first so this test could fail simply because the indices and types lists are in different orders.
deletes users after every test just in case any test adds any.
re-enables watcher after every test just in case any test disables it. one does.
check that we are connected to a cluster named "elasticsearch".
create an index and index some docs
parse the script configured in the given settings.
helper method to throw an exception if more than one type of script is specified.
this will build scripts into the following xcontent structure: "<(id, source)>" : "", "lang" : "", "options" : "option0" : "", "option1" : "", ... , "params" : "param0" : "", "param1" : "", ... example: "source" : "return math.log(doc.popularity) params.multiplier;", "lang" : "painless", "params" : "multiplier" : 100.0 note that lang, options, and params will only be included if there have been any specified. this also handles templates in a special way. if the script#content_type_option option is provided and the scripttype#inline is specified then the template will be preserved as a raw field. "source" : "query" : ... , "lang" : "", "options" : "option0" : "", "option1" : "", ... , "params" : "param0" : "", "param1" : "", ...
since inline scripts can accept code rather than just an id, they must also be able to handle template parsing, hence the need for custom parsing code. templates can consist of either an string or a json object. if a json object is discovered then the content type option must also be saved as a compiler option.
validates the parameters and creates an script. at run-time this way in case a legacy default language is used from previously stored queries.
constructor for a script that requires the use of compiler options. for scripttype#stored scripts this should be null, but can be specified to access scripts stored as part of the stored scripts deprecated api. the code for this script if the scripttype is scripttype#inline. is scripttype#inline, null otherwise.
creates a script read from an input stream.
this will parse xcontent into a script. the following formats can be parsed: the simple format defaults to an scripttype#inline with no compiler options or user-defined params: example: "return math.log(doc.popularity) 100;" the complex format where scripttype and idorcode are required while lang, options and params are not required. exactly one of "id" or "source" must be specified "id" : "", or "source": "", "lang" : "", "options" : "option0" : "", "option1" : "", ... , "params" : "param0" : "", "param1" : "", ... example: "source" : "return math.log(doc.popularity) params.multiplier", "lang" : "painless", "params" : "multiplier" : 100.0 this also handles templates in a special way. if a complexly formatted query is specified as another complex json object the query is assumed to be a template, and the format will be preserved. "source" : "query" : ... , "lang" : "", "options" : "option0" : "", "option1" : "", ... , "params" : "param0" : "", "param1" : "", ... the one defined by script#default_script_lang due to backwards compatibility requirements related to stored queries using previously default languages.
check whether there have been too many compilations within the last minute, throwing a circuit breaking exception if so. this is a variant of the token bucket algorithm: https:en.wikipedia.orgwikitoken_bucket it can be thought of as a bucket with water, every time the bucket is checked, water is added proportional to the amount of time that elapsed since the last time it was checked. if there is enough water, some is removed and the request is allowed. if there is not enough water the request is denied. just like a normal bucket, if water is added that overflows the bucket, the extra watercapacity is discarded - there can never be more water in the bucket than the size of the bucket.
this configures the maximum script compilations per five minute window.
compiles a script using the given context.
reads an int from the input stream and converts it to a scripttype. if no scripttype is found based on the id.
return the score of the current document.
set the current document to run the script on next.
not recommended but we test anyway
returns a json version of this exception for debugging.
deserializes a scriptexception from a streaminput
ensure the script stack is immutable
test that our elements are present in the json output
ensure we can round trip in serialization
ensure no parameters can be null
returns a method with the given name, or throws an exception if multiple are found.
construct a context with the related instance and compiled classes.
this will write xcontent from scriptmetadata. the following format will be written: "" : "< storedscriptsource#toxcontent(xcontentbuilder, params)>", "" : "< storedscriptsource#toxcontent(xcontentbuilder, params)>", ...
delete a script from the existing stored scripts based on a user-specified id.
this will parse xcontent into scriptmetadata. the following format will be parsed: "" : "< storedscriptsource#fromxcontent(xcontentparser, boolean)>", "" : "< storedscriptsource#fromxcontent(xcontentparser, boolean)>", ... when loading from a source prior to 6.0, if multiple scripts using the old namespace id format of [lang#id] are found to have the same id but different languages an error will occur.
convenience method to build and return a new
add a new script to the existing stored scripts based on a user-specified id. if a script with the same id already exists it will be overwritten.
convenience method to build and return a new
validates the parameters and creates an storedscriptsource. this allow empty templates to be loaded for backwards compatibility. this allow empty templates to be loaded for backwards compatibility.
this will write xcontent from a storedscriptsource. the following format will be written: "script" : "lang" : "", "source" : "", "options" : "option0" : "", "option1" : "", ... note that the 'source' parameter can also handle templates written as complex json.
writes a storedscriptsource to a stream. will write all of the lang, source, and options parameters.
since stored scripts can accept templates rather than just scripts, they must also be able to handle template parsing, hence the need for custom parsing source. templates can consist of either an string or a json object. if a json object is discovered then the content type option must also be saved as a compiler option.
reads a storedscriptsource from a stream. version 5.3+ will read all of the lang, source, and options parameters. for versions prior to 5.3, only the source parameter will be read in as a bytes reference.
this will parse xcontent into a storedscriptsource. the following formats can be parsed: the simple script format with no compiler options or user-defined params: example: "script": "return math.log(doc.popularity) 100;" the above format requires the lang to be specified using the deprecated stored script namespace (as a url parameter during a put request). see scriptmetadata for more information about the stored script namespaces. the complex script format using the new stored script namespace where lang and source are required but options is optional: "script" : "lang" : "", "source" : "", "options" : "option0" : "", "option1" : "", ... example: "script": "lang" : "painless", "source" : "return math.log(doc.popularity) params.multiplier" the use of "source" may also be substituted with "code" for backcompat with 5.3 to 5.5 format. for example: "script" : "lang" : "", "code" : "", "options" : "option0" : "", "option1" : "", ... note that the "source" parameter can also handle template parsing including from a complex json object.
converts a parseexception at compile-time or link-time to a scriptexception
this is a hack for filter scripts, which must return booleans instead of doubles as expression do. see https:github.comelasticelasticsearchissues26429.
parses a restrequest body and returns a multisearchtemplaterequest
for simplicity we create a minimal response, as there is already a dedicated test class for search response parsing and serialization.
note that we can't rely on normal equals and hashcode checks, since searchresponse doesn't currently implement equals and hashcode. instead, we compare the template outputs for equality, and perform some sanity checks on the search response instances.
compile a template string to (in this case) a mustache object than can later be re-used for execution to fill in missing parameter values.
test that template can be expressed as a single escaped string.
test that template can contain conditional clause. in this case it is at the beginning of the string.
test that template can contain conditional clause. in this case it is at the end of the string.
sets how many search requests specified in this multi search requests are allowed to be ran concurrently.
at compile time, this function extracts the name of the variable: #tojsonvariable_nametojson
ask this shard to check now whether it is inactive, and reduces its indexing buffer if so.
shard calls this on each indexingdelete op
called by indexshard to record estimated bytes written to translog for the operation
ask this shard to refresh, in the background, to free up heap
tests that if an index structure creation fails on relocation to a new node, the shard is not stuck but properly failed.
deletes an index that is not assigned to this node. this method cleans up all disk folders relating to the index but does not deal with in-memory structures. for those call #removeindex(index, indexremovalreason, string)
creates a new mapper service for the given index, in order to do administrative work like mapping updates. this should not be used for document parsing. doing so will result in an exception. note: the returned mapperservice should be closed when unneeded.
this method verifies that the given metadata holds sane values to create an indexservice. this method tries to update the meta data of the created indexservice if the given metadataupdate is different from the given metadata. this method will throw an exception if the creation or the update fails. the created indexservice will not be registered and will be closed immediately.
deletes the shard with an already acquired shard lock.
cache something calculated at the shard level.
this creates a new indexservice without registering it
processes all pending deletes for the given index. this method will acquire all locks for the given index and will process all pending deletes for this index. pending deletes might occur if the os doesn't allow deletion of files because they are used by a different process ie. on windows where files might still be open by a virus scanner. on a shared filesystem a replica might not have been closed when the primary is deleted causing problems on delete calls so we schedule there deletes later.
returns an indexservice for the specified index if exists otherwise a indexnotfoundexception is thrown.
clears the caches for the given shard id if the shard is still allocated on this node
loads the cache result, computing it if needed by executing the query phase and otherwise deserializing the cached value into the searchcontext#queryresult() context's query result. the combination of load + compute allows to have a single load operation that will cause other requests with the same key to wait till its loaded an reuse the same cache.
can the shard request be cached at all?
checks if changes (adding removing) indices, shards and so on are allowed.
creates a new pending delete of an index
this method deletes the shard contents on disk for the given shard id. this method will fail if the shard deleting is prevented by #candeleteshardcontent(shardid, indexsettings) of if the shards lock can not be acquired. on data nodes, if the deleted shard is the last shard folder in its index, the method will attempt to remove the index folder as well.
returns sharddeletioncheckresult signaling whether the shards content for the given shard can be deleted.
deletes the index store trying to acquire all shards locks for this index. this method will delete the metadata for the index even if the actual shards can't be locked. package private for testing
adds a pending delete for the given index shard.
verify that the contents on disk for the given index is deleted; if not, delete the contents. this method assumes that an index is already deleted in the cluster state andor explicitly through index tombstones.
creates a new indexservice for the given metadata. per-index listeners
returns an indexservice for the specified index if exists otherwise returns null.
this method returns true if the current node is allowed to delete the given index. this is the case if the index is deleted in the metadata or there is no allocation on the local node and the index isn't on a shared file system.
clear all entries that belong to the given index.
get usage statistics for the given shard.
this test checks an edge case where, if a node had an index (lets call it a with uuid 1), then deleted it (so a tombstone entry for a will exist in the cluster state), then created a new index a with uuid 2, then shutdown, when the node comes back online, it will look at the tombstones for deletions, and it should proceed with trying to delete a with uuid 1 and not throw any errors that the index still exists in the cluster state. this is a case of ensuring that tombstones that have the same name as current valid indices don't cause confusion by trying to delete an index that exists. see https:github.comelasticelasticsearchissues18054
tests that teh mapperservice created by indicesservice#createindexmapperservice(indexmetadata) contains custom types and similarities registered by plugins
read from a stream.
issues a cache clear and waits 30 seconds for the field data breaker to be cleared
returns true if any of the nodes used a noop breaker
relocate a shard and block cluster state processing on the relocation target node to activate the shard
test that we can safely concurrently index and get stats. this test was inspired by a serialization issue that arose due to a race getting doc stats during heavy indexing. the race could lead to deleted docs being negative which would then be serialized as a variable-length long. since serialization of negative longs using a variable-length format was unsupported ( org.elasticsearch.common.io.stream.streamoutput#writevlong(long)), the stream would become corrupted. here, we want to test that we can continue to get stats while indexing.
closes the current recovery target and waits up to a certain timeout for resources to be freed. returns true if resetting the recovery was successful, false if the recovery target is already cancelled failed or marked as done.
creates a new recovery target object that represents a recovery to the provided shard. version; necessary for primary relocation so that new primary knows about all other ongoing replica recoveries when replicating documents (see recoverysourcehandler)
mark the current recovery as done
creates an org.apache.lucene.store.indexoutput for the given file name. note that the indexoutput actually point at a temporary file.  note: you can use #getopenindexoutput(string) with the same filename to retrieve the same indexoutput at a later stage
fail the recovery and call listener
cancel the recovery. calling this method will clean temporary files and release the store unless this object is in use (in which case it will be cleaned once all ongoing users call  if #cancellablethreads() was used, the threads will be interrupted.
fail the recovery with the given id (if found) and remove it from the recovery collection
cancel the recovery with the given id (if found) and remove it from the recovery collection
gets the recoverytarget for a given id. the recoverystatus returned has it's ref count already incremented to make sure it's safe to use. however, you must call recoverytarget#decref() when you are done with it, typically by using this method in a try-with-resources clause.  returns null if recovery is not found
mark the recovery with the given id as done (if found)
cancel all ongoing recoveries for the given shard
resets the recovery and performs a recovery restart on the currently recovering index shard
starts are new recovery for the given shard, source node and state
similar to #getrecovery(long) but throws an exception if no recovery is found
percent of bytes recovered out of total files bytes to be recovered
percent of recovered (i.e., not reused) files out of the total files to be recovered
total bytes of files to be recovered (potentially not yet done)
number of file that were recovered (excluding on ongoing files)
total number of bytes in th shard
total number of files to be recovered (potentially not yet done)
total number of bytes recovered so far, including both existing and reused
construct a request for starting a peer recovery.
performs the recovery from the local engine to the target
send the given snapshot's operations with a sequence number greater than the specified staring sequence number to this handler's target node.  operations are bulked into a single request depending on an operation count limit or size-in-bytes limit. total number of operations sent
perform phase two of the recovery process.  phase two uses a snapshot of the current translog without acquiring the write lock (however, the translog snapshot is point-in-time view of the translog). it then sends each translog operation to the target node so it can be replayed into the new shard. ops should be sent
perform phase1 of the recovery operations. once this indexcommit snapshot has been performed no commit operations (files being fsync'd) are effectively allowed on this index until all recovery phases are done  phase1 examines the segment files on the target node and copies over the segments that are missing. only segments that have the same size and checksum can be reused
determines if the source translog is ready for a sequence-number-based peer recovery. the main condition here is that the source translog contains all operations above the local checkpoint on the target. we already know the that translog contains or will contain all ops above the source local checkpoint, so we can stop check there.
tests scenario where recovery target successfully sends recovery request to source but then the channel gets closed while the source is working on the recovery process.
obtains a snapshot of the store metadata for the recovery target.
get the starting sequence number for a sequence-number-based request. failed
prepare the start recovery request.
this test makes sure that there is no infinite loop of flushing (the condition `shouldperiodicallyflush` eventually is false) in peer-recovery if a primary sends a fully-baked index commit.
adds recovery source handler.
blocking version of syncedflushservice#sendpresyncrequests(list, clusterstate, shardid, actionlistener)
blocking version of syncedflushservice#attemptsyncedflush(shardid, actionlistener)
send presync requests to all started copies of the given shard
returns the number of in flight operations on primary. -1 upon error.
a utility method to perform a synced flush for all shards of multiple indices. see #attemptsyncedflush(shardid, actionlistener) for more details.
success constructor
test that a breaker correctly redistributes to a different breaker, in this case, the request breaker borrows space from the fielddata breaker
validate that child settings are valid
checks whether the parent breaker has been tripped
allows to register a custom circuit breaker. warning: will overwrite any existing custom breaker with the same name.
tests that plugins can register pre-configured token filters that vary in behavior based on elasticsearch version, lucene version, and that do not vary based on version at all.
tests that plugins can register pre-configured token filters that vary in behavior based on elasticsearch version, lucene version, and that do not vary based on version at all.
tests that plugins can register pre-configured char filters that vary in behavior based on elasticsearch version, lucene version, and that do not vary based on version at all.
loads the hunspell dictionary for the given local.
scans the hunspell directory and loads all found dictionaries
returns the hunspell dictionary for the given locale.
each hunspell dictionary directory may contain a settings.yml which holds dictionary specific settings. default values for these settings are defined in the given default settings.
creates a new global scope analysis provider without index specific settings not settings for the provider itself. this can be used to get a default instance of an analysis factory without binding to an index. true
map containing pre-configured token filters that should be available after installing this plugin. the map is from the name of the token filter to the class of the lucene tokenfilterfactory that it is emulating. if the lucene tokenfilterfactory is there is no lucene tokenfilterfactory then the right hand side should be void.
map containing pre-configured tokenizers that should be available after installing this plugin. the map is from the name of the token filter to the class of the lucene tokenizerfactory that it is emulating. if the lucene tokenizerfactory is if there is no lucene tokenizerfactory then the right hand side should be void.
checks if cluster state matches internal state of indicesclusterstateservice instance
finds the routing source node for peer recovery, return null if its not found. note, this method expects the shard routing to require peer recovery, use shardrouting#recoverysource() to check if its needed or not.
deletes indices (with shard data).
returns shard for the specified id if it exists otherwise returns null.
removes shard entries from the failed shards cache that are no longer allocated to this node by the master. sends shard failures for shards that are marked as actively allocated to this node but don't actually exist on the node. resends shard failures for shards that are still marked as allocated to this node but previously failed.
notifies master about shards that don't exist but are supposed to be active on this node.
removes shards that are currently loaded by indicesservice but have disappeared from the routing table of the current node. this method does not delete the shard data.
removes indices that have no shards allocated to this node. this does not delete the shard data as we wait for enough shard copies to exist in the cluster before deleting shard data (triggered by org.elasticsearch.indices.store.indicesstore).
in rare cases it is possible that a nodes gets an instruction to replace a replica shard that's in post_recovery with a new initializing primary with the same allocation id. this can happen by batching cluster states that include the starting of the replica, with closing of the indices, opening it up again and allocating the primary shard to the node in question. the node should then clean it's initializing replica and replace it with a new initializing primary.
this test ensures that when a node joins a brand new cluster (different cluster uuid), different from the cluster it was previously a part of, the in-memory index data structures are all removed but the on disk contents of those indices remain so that they can later be imported as dangling indices. normally, the first cluster state update that the node receives from the new cluster would contain a cluster block that would cause all in-memory structures to be removed (see indicesclusterstateservice#applyclusterstate(clusterchangedevent)), but in the case where the node joined and was a few cluster state updates behind, it would not have received the cluster block, in which case we still need to remove the in-memory structures while ensuring the data remains on disk. this test executes this particular scenario.
prepares the settings by gathering all elasticsearch system properties, optionally loading the configuration settings.
finish preparing settings by replacing forced settings and any defaults that need to be added.
checks all settings values to make sure they do not have the old prompt settings. these were deprecated in 6.0.0. this check should be removed in 8.0.0.
initializes the builder with the given input settings, and applies settings from the specified map (these settings typically come from the command line).
prepares the settings by gathering all elasticsearch system properties and setting defaults.
test that we add the appropriate mock services when their plugins are added. this is a very heavy test for a testing component but we've broken it in the past so it is important.
returns a map of node id to the ranking of the nodes based on the adaptive replica formula
creates a new circuitbreakerservice based on the settings provided.
start the node. if the node is already started, this method is no-op.
adds a default node name to the given setting, if it doesn't already exist
get custom name resolvers list based on a discovery plugins list
writes a file to the logs dir containing the ports for the given transport type
creates a new the searchservice. this method can be overwritten by tests to inject mock implementations.
decodes the provided byte[] to a utf-8 char[]. this is done while avoiding conversions to string. the provided byte[] is not modified by this method, so the caller needs to take care of clearing the value if it is sensitive.
constant time equality check of strings to avoid potential timing attacks.
encodes the provided char[] to a utf-8 byte[]. this is done while avoiding conversions to string. the provided char[] is not modified by this method, so the caller needs to take care of clearing the value if it is sensitive.
tests if a char[] contains a sequence of characters that match the prefix. this is like
create a new ewma with a given alpha and initialavg. a smaller alpha means that new data points will have less weight, where a high alpha means older data points will have a lower influence.
provides a secure source of randomness. this acts exactly similar to #get(), but returning a new securerandom.
provides a reproducible source of randomness seeded by a long seed in the settings with the key setting.
provides a source of randomness that is reproducible when running under the elasticsearch test suite, and otherwise produces a non-reproducible source of randomness. reproducible sources of randomness are created when the system property "tests.seed" is set and the security policy allows reading this system property. otherwise, non-reproducible sources of randomness are created. to acquire an instance of random from randomizedcontext or tests are running but tests.seed is not set
return an informative string describing all tasks performed for custom reporting, call gettaskinfo() and use the task info directly.
return the name of the last task.
return an array of the data for tasks performed.
start a named task. the results are undefined if #stop() or timing methods are called without invoking this method.
stop the current task. the results are undefined if timing methods are called without invoking at least one pair
return a string with a table describing all tasks performed. for custom reporting, call gettaskinfo() and use the task info directly.
return the time taken by the last task.
determine the name of the package of the given class: e.g. "java.lang" for the java.lang.string class. is defined in the default package
creates a new fieldmemorystats instance from a stream
adds merges the given field memory stats into this stats instance
generates x-content into the given builder for each of the fields in this stats instance
split a string at the first occurrence of the delimiter. does not include the delimiter in the result. index 1 being after the delimiter (neither element includes the delimiter); or null if the delimiter wasn't found in the given input string
format the double value with a single decimal points, trimming trailing '.0'.
trim all occurrences of the supplied leading character from the given string.
take a string which is a delimited list and convert it to a string array. a single delimiter can consists of more than one character: it will still be considered as single delimiter string, rather than as bunch of potential delimiter characters - in contrast to tokenizetostringarray. rather than a bunch individual delimiter characters) line breaks: e.g. "\r\n\f" will delete all new lines and line feeds in a string.
return substring(beginindex, endindex) that is impervious to string length.
convenience method to convert a csv string list to a set. note that this will suppress duplicates.
convenience method to return a collection as a delimited (e.g. csv) string. e.g. useful for tostring() implementations.
convenience method to return a string array as a delimited (e.g. csv) string. e.g. useful for tostring() implementations.
replace all occurrences of a substring within a string with another string.
splits a backslash escaped string on the separator.  current backslash escaping supported:  \n \t \r \b \f are escaped the same as a java string  other characters following a backslash are produced verbatim (\c => c)
check whether the given charsequence has actual text. more specifically, returns true if the string not null, its length is greater than 0, and it contains at least one non-whitespace character.  stringutils.hastext(null) = false stringutils.hastext("") = false stringutils.hastext(" ") = false stringutils.hastext("12345") = true stringutils.hastext(" 12345 ") = true  its length is greater than 0, and it does not contain whitespace only
return a string that is the json representation of the provided toxcontent. wraps the output into an anonymous object if needed. allows to control whether the outputted json needs to be pretty printed and human readable.
tokenizes the specified string to a collection using the specified delimiters as the token delimiters. this method trims whitespace from tokens and ignores empty tokens.
copy the given collection into a string array. the collection must contain string elements only. collection was null)
delete any character in a given string. e.g. "az\n" will delete 'a's, 'z's and new lines.
truncates string to a length less than length. backtracks to throw out high surrogates.
test whether the given string matches the given substring at the given index.
tokenize the given string into a string array via a stringtokenizer. trims tokens and omits empty tokens. the given delimiters string is supposed to consist of any number of delimiter characters. each of those characters can be used to separate tokens. a delimiter is always a single character; for multi-character delimiters, consider using delimitedlisttostringarray (each of those characters is individually considered as delimiter).
match a string against the given pattern, supporting the following simple pattern styles: "xxx", "xxx", "xxx" and "xxxyyy" matches (with an arbitrary number of pattern parts), as well as direct equality.
add a sequence of validation errors to the accumulating validation errors
return the long that n stores, or throws an exception if the stored value cannot be converted to a long that stores the exact same value.
return the long that stringvalue stores or throws an exception if the stored value cannot be converted to a long that stores the exact same value and coerce is false.
return the byte that n stores, or throws an exception if the stored value cannot be converted to a byte that stores the exact same value.
return the short that n stores, or throws an exception if the stored value cannot be converted to a short that stores the exact same value.
converts an int to a byte array.
converts a long to a byte array.
converts an int to a byte array.
test that rounded values are always greater or equal to last rounded value if date is increasing. the example covers an interval around 2011-10-30t02:10:00+01:00, time zone cet, interval: 2700000ms
test dst end with interval rounding cet: 25 october 2015, 03:00:00 clocks were turned backward 1 hour to 25 october 2015, 02:00:00 local standard time
tests for dst transition with overlaps and day roundings.
special test for intervals that don't fit evenly into rounding interval. in this case, when interval crosses dst transition point, rounding in local time can land in a dst gap which results in wrong utc rounding values.
test dst start with offset not fitting interval, e.g. asiakathmandu adding 15min on 1986-01-01t00:00:00 the interval from 1986-01-01t00:15:00+05:45 to 1986-01-01t00:20:00+05:45 to only be 5min long
randomized test on org.elasticsearch.common.rounding.rounding.timeintervalrounding with random interval and time zone offsets
test dst start with interval rounding cet: 27 march 2016, 02:00:00 clocks were turned forward 1 hour to 27 march 2016, 03:00:00 local daylight time
test for #10025, strict local to utc conversion can cause joda exceptions on dst start
test for a time zone whose days overlap because the clocks are set back across midnight at the end of dst.
randomized test on timeunitrounding. test uses random chooses test dates that are exactly on or close to offset changes (e.g. dst) in the chosen time zone. it rounds the test date down and up and performs various checks on the rounding unit interval that is defined by this. assumptions tested are described in
to be even more nasty, go to a transition in the selected time zone. in one third of the cases stay there, otherwise go half a unit back or forth
perform a number on assertions and checks on org.elasticsearch.common.rounding.rounding.timeunitrounding intervals
test for half day rounding intervals scrossing dst.
returns true iff the sequence of chars is one of "true","false".
returns false if text is in "false", "0", "off", "no"; else, true.
parses a char[] representation of a boolean value to boolean. provided default value iff either text is null or length == 0.
parses a string representation of a boolean value to boolean.
does fieldname match this field? the field name to match against this parsefield names for this parsefield.
the primary name for this field. this will be returned by names for this field which are deprecated and will not be accepted when strict matching is used.
returns a base64 encoded version of a version 4.0 compatible uuid randomly initialized by the given java.util.random instance as defined here: http:www.ietf.orgrfcrfc4122.txt
parses the provided string as a ratiovalue, the string can either be in percentage format (eg. 73.5%), or a floating-point ratio format (eg. 0.735)
parse the provided string as a memory size. this method either accepts absolute values such as the heap is 1g, 10% will be parsed as 100mb.
parse a distance from a given string if not unit is provided in the first argument
convert a string to a distanceunit
converts a distance value given in a specific distanceunit into a value equal to the specified value but in a other distanceunit.
parses the suffix of a given distance string and return the corresponding distanceunit
read a distanceunit from a streaminput.
able to be parsed using unlike #tostring() this method will not output fractional or rounded values so this method should be preferred when serialising the value to json.
read from a stream.
copy the contents of the given reader into a string. closes the reader when done.
copy the contents of the given string to the given output writer. closes the write when done.
copy the contents of the given inputstream to the given outputstream. closes both streams when done.
copy the contents of the given byte array to the given outputstream. closes the stream when done.
copy the contents of the given reader to the given writer. closes both when done.
check that a directory exists, is a directory and is readable by the current user
appends the path to the given base and strips n elements off the path if strip is > 0.
check whether the file denoted by the given path is hidden. in practice, this will check if the file name starts with a dot. this should be preferred to files#ishidden(path) as this does not depend on the operating system.
returns an inputstream the given url if the url has a protocol of 'file' or 'jar', no host, and no port.
returns a path from a uri  this works just like paths.get().  remember: this should almost never be used. usually resolve a path against an existing one!
tries to resolve the given path against the list of available roots. if path starts with one of the listed roots, it returned back by this method, otherwise null is returned.
read from a file channel into a byte buffer, starting at a certain position. that will fit in the destination byte buffer.
writes part of a byte array to a java.nio.channels.writablebytechannel
read length bytes from position of a file channel. an eofexception will be thrown if you attempt to read beyond the end of file.
writes a java.nio.bytebuffer to a java.nio.channels.writablebytechannel
read from a file channel into a byte buffer, starting at a certain position. an eofexception will be thrown if you attempt to read beyond the end of file.
read length bytes from position of a file channel
flushes the internal bytes buffer.
writes a portion of a string.
writes the specified character sequence.
sets the output stream to use for writing until this writer is closed. for example:[code] writer writer = new utf8streamwriter().setoutputstream(out); [code] is equivalent but writes faster than [code] writer writer = new java.io.outputstreamwriter(out, "utf-8"); [code] it has not been #close closed or #reset reset.
closes and #reset resets this writer for reuse.
flushes the stream. if the stream has saved any characters from the various write() methods in a buffer, write them immediately to their intended destination. then, if that destination is another character or byte stream, flush it. thus one flush() invocation will flush all the buffers in a chain of writers and outputstreams.
writes a collection of generic objects via a writer
write a map of k-type keys to v-type.  map<string, string> map = ...; out.writemap(map, streamoutput::writestring, streamoutput::writestring); 
writes a list of writeable objects
writes a list of streamable objects
writes a non-negative long in a variable-length format. writes between one and ten bytes. smaller values take fewer bytes. negative numbers use ten bytes and trip assertions (if running in tests) so prefer #writelong(long) or #writezlong(long) for negative numbers.
write map to stream with consistent order to make sure every map generated bytes order are same. this method is compatible with streaminput.readmap and streaminput.readgenericvalue this method only will handle the map keys order, not maps contained within the map
writes an optional bytes reference including a length header. use this if you need to differentiate between null and empty bytes references. use #writebytesreference(bytesreference) and streaminput#readbytesreference() if you do not.
notice: when serialization a map, the stream out map with the stream in map maybe have the different key-value orders, they will maybe have different stream order. if want to keep stream out map and stream in map have the same stream order when stream, can use writemapwithconsistentorder
writes the bytes reference, including a length header.
writes the specified array to the stream using the specified writer for each element in the array. this method can be seen as writer version of streaminput#readarray(writeable.reader, intfunction). the length of array encoded as a variable-length integer is first written to the stream, and then the elements of the array are written to the stream.
serializes a potential null value.
constructs a new registry from the given entries.
returns a reader for a namedwriteable object identified by the name provided as argument and its category.
creates a new inputstreamstreaminput with a size limit
read a map of k-type keys to v-type lists.  map<string, list<string>> map = in.readmapoflists(streaminput::readstring, streaminput::readstring); 
reads an array from the stream using the specified org.elasticsearch.common.io.stream.writeable.reader to read array elements from the stream. this method can be seen as the reader version of streamoutput#writearray(writeable.writer, object[]). it is assumed that the stream first contains a variable-length integer representing the size of the array, and then contains that many elements that can be read from the stream.
reads a list of namedwriteables.
reads an enum with type e that was serialized based on the value of it's ordinal
reads a long stored in variable-length format. reads between one and ten bytes. smaller values take fewer bytes. negative numbers are encoded in ten bytes so prefer #readlong() or #readzlong() for negative numbers.
reads a collection of objects
read a list of streamable objects, using the constructor to instantiate each instance.  this is expected to take the form:  list<mystreamableclass> list = in.readstreamlist(mystreamableclass::new); 
reads an enum with type e that was serialized based on the value of it's ordinal
serializes a potential null value.
reads a bytes reference from this stream, might hold an actual reference to the underlying bytes of the stream.
reads an int stored in variable-length format. reads between one and five bytes. smaller values take fewer bytes. negative numbers will always use all 5 bytes and are therefore better serialized using #readint
read a timevalue from the stream
reads a namedwriteable from the current stream with the given name. it is assumed that the caller obtained the name from other source, so it's not read from the stream. the name is used for looking for the corresponding entry in the registry by name, so that the proper object can be read and returned. default implementation throws unsupportedoperationexception as streaminput doesn't hold a registry. use filterinputstream instead which wraps a stream and supports a namedwriteableregistry too. prefer streaminput#readnamedwriteable(class) and streamoutput#writenamedwriteable(namedwriteable) unless you have a compelling reason to use this method instead.
reads a vint via #readvint() and applies basic checks to ensure the read array size is sane. this method uses #ensurecanreadbytes(int) to ensure this stream has enough bytes to read for the read array size.
checks for a deprecated setting and logs the correct alternative
checks for a removed setting and logs the correct alternative
wrap two recyclers and forward to calls to smallobjectrecycler when size < minsize and to defaultrecycler otherwise.
wrap the provided recycler so that calls to recycler#obtain() and recycler.v#close() are protected by a lock.
return a recycler based on a deque.
create a concurrent implementation that can support concurrent access from concurrencylevel threads with little contention.
converts an object value to bytesref.
converts a value to a string, taking special care if its a bytesref to call
register a leafreader. this is necessary so that the core cache key of this reader can be found later using #getcorekeysforindex(string).
get the set of core cache keys associated with the given index.
returns an iterable that allows to iterate over all files in this segments info
check whether there is one or more documents matching the provided query.
this method removes all files from the given directory that are not referenced by the given segments file. this method will open an indexwriter and relies on index file deleter to remove all unreferenced files. segment files that are newer than the given segments file are removed forcefully to prevent problems with indexwriter opening a potentially broken commit point leftover. note: this method will fail if there is another indexwriter open on the given directory. this method will also acquire a write lock from the directory while pruning unused files. this method expects an existing index in the given directory that has the given segments file.
return a scorer that throws an elasticsearchillegalstateexception on all operations with the given message.
given a scorersupplier, return a bits instance that will match all documents contained in the set. note that the returned bits instance must be consumed in order.
this method removes all lucene files from the given directory. it will first try to delete all commit points segments files to ensure broken commits or corrupted indices will not be opened in the future. if any of the segment files can't be deleted this operation fails.
returns the number of documents in the index referenced by this segmentinfos
tries to extract a segment reader from the given index reader. if no segmentreader can be extracted an illegalstateexception is thrown.
adds the given listener to the provided directory reader. the reader must contain an elasticsearchdirectoryreader in it's hierarchy otherwise we can't safely install the listener.
test that core cache key (needed for nrt) is working
initialize lookup for the provided segment
returns the internal lucene doc id for the given id bytes.
return null if id is not found.
return null if id is not found. we pass the leafreadercontext as an argument so that things still work with reader wrappers that hide some documents while still using the same cache key. otherwise we'd have to disable caching entirely for these readers.
load the internal doc id and sequence number for the uid from the reader, returning null if the uid wasn't found, a doc id and the associated seqno otherwise 
load the primaryterm associated with the given docidandseqno
load the internal doc id and version for the uid from the reader, returning null if the uid wasn't found, a doc id and a version otherwise 
test that version map cache behaves properly with a filtered reader
test that version map cache works, is evicted on close, etc
test version lookup with two documents matching the id
test version lookup actually works
returns the terms for each position in this phrase
add multiple terms at the next position in the phrase. any of the terms may match.
returns the relative positions of terms in this phrase.
allows to specify the relative position of terms within the phrase.
returns a hash code value for this object.
creates a new non-nested docs query
return a query that matches all documents but those that match the given query.
add to an existing boolean query the more like this query from this priorityqueue
determines if the passed term is likely to be of interest in "more like" comparisons
return a query that will return docs like the passed lucene document id.
find words for a more-like-this query former. the result is a priority queue of arrays with one entry for every word in the document. each array has 6 elements. the elements are:   the word (string)  the top field that this word comes from (string)  the score for this word (float)  the idf value (float)  the frequency of this word in the index (integer)  the frequency of this word in the source document (integer)  this is a somewhat "advanced" routine, and in general only the 1st entry in the array is of interest. this method is exposed so that you can identify the "interesting words" in a document. for an easier method to call see #retrieveinterestingterms retrieveinterestingterms().
convenience routine to make it easy to return the most interesting words in a document. more advanced users will call #retrieveterms(reader, string) retrieveterms() directly.
return a query that will return docs like the passed readers. this was added in order to treat multi-value fields.
return a query that will return docs like the passed fields.
adds term frequencies found by tokenizing text from reader into the map words
adds terms and frequencies found in vector into the map termfreqmap
find words for a more-like-this query former.
describe the parameters that control how the "more like this" query is formed.
create the more like query from a priorityqueue
return a query that will return docs like the passed terms.
create a priorityqueue from a word->tf map.
creates a functionscorequery with multiple score functions
sets the amount of time before an entry in the cache expires after it was written. greater than 0.
sets the amount of time before an entry in the cache expires after it was last accessed. be greater than 0.
put an entry into the segment
the cache statistics tracking hits, misses and evictions. these are taken on a best-effort basis meaning that they could be out-of-date mid-flight.
remove an entry from the segment
get an entry from the segment; expired entries will be returned as null but not removed from the cache until the lru list is pruned or a manual cache#refresh() is performed however a caller can take action using the provided callback
remove an entry from the segment iff the future is done and the value is equal to the expected value
an lru sequencing of the keys in the cache that supports removal. this sequence is not protected from mutations to the cache (except for iterator#remove(). the result of iteration under any other mutation is undefined.
if the specified key is not already associated with a value (or is mapped to null), attempts to compute its value using the given mapping function and enters it into this map unless null. the load method for a given key will be invoked at most once. use of different cacheloader implementations on the same key concurrently may result in only the first loader function being called and the second will be returned the result provided by the first including any exceptions thrown during the execution of the first.
force any outstanding size-based and time-based evictions to occur
an lru sequencing of the values in the cache. this sequence is not protected from mutations to the cache (except for iterator#remove(). the result of iteration under any other mutation is undefined.
invalidate all cache entries. a removal notification will be issued for invalidated entries with
true if the bytes were compressed with lzf: only used before elasticsearch 2.0
decompress the provided bytesreference.
create a compressedxcontent out of a toxcontent instance.
create a compressedxcontent out of a serialized toxcontent that may already be compressed.
return the uncompressed bytes.
and such that timezone.getoffset(t) != timezone.getoffset(utcmillis). if there is no such t, returns long.max_value.
determine whether the local instant is a valid instant in the given time zone. the logic for this is taken from `strict` mode case, but instead of throwing an flag indicating that the value is illegal in that time zone.
test for #10025, strict local to utc conversion can cause joda exceptions on dst start
test that time zones are correctly parsed. there is a bug with joda 2.9.4 (see https:github.comjodaorgjoda-timeissues373)
test for half day rounding intervals scrossing dst.
test that rounded values are always greater or equal to last rounded value if date is increasing. the example covers an interval around 2011-10-30t02:10:00+01:00, time zone cet, interval: 2700000ms
test dst end with interval rounding cet: 25 october 2015, 03:00:00 clocks were turned backward 1 hour to 25 october 2015, 02:00:00 local standard time
perform a number on assertions and checks on timeunitrounding intervals
tests for dst transition with overlaps and day roundings.
randomized test on timeunitrounding. test uses random chooses test dates that are exactly on or close to offset changes (e.g. dst) in the chosen time zone. it rounds the test date down and up and performs various checks on the rounding unit interval that is defined by this. assumptions tested are described in
to be even more nasty, go to a transition in the selected time zone. in one third of the cases stay there, otherwise go half a unit back or forth
test for a time zone whose days overlap because the clocks are set back across midnight at the end of dst.
special test for intervals that don't fit evenly into rounding interval. in this case, when interval crosses dst transition point, rounding in local time can land in a dst gap which results in wrong utc rounding values.
test dst start with offset not fitting interval, e.g. asiakathmandu adding 15min on 1986-01-01t00:00:00 the interval from 1986-01-01t00:15:00+05:45 to 1986-01-01t00:20:00+05:45 to only be 5min long
randomized test on timeintervalrounding with random interval and time zone offsets
test dst start with interval rounding cet: 27 march 2016, 02:00:00 clocks were turned forward 1 hour to 27 march 2016, 03:00:00 local daylight time
equivalent to #wrap(releasable...) but can be called multiple times without double releasing.
insert a value for the given path. if the path already exists, replace the value with:  value = updater.apply(oldvalue, newvalue);  allowing the value to be updated if desired.
read from a stream.
test that the same map written multiple times do not trigger the self-reference check in
parses a value from the given xcontentparser
parses a value from the given xcontentparser
adapts an array (or varags) setter into a list setter.
makes sure that current token is of type token#field_name and the field name is equal to the provided one
parse the current token depending on its token type. the following token types will be parsed by the corresponding parser methods:   token#value_string: xcontentparser#text()  token#value_number: xcontentparser#numbervalue() ()  token#value_boolean: xcontentparser#booleanvalue() ()  token#value_embedded_object: xcontentparser#binaryvalue() ()  token#value_null: returns null  token#start_object: xcontentparser#mapordered() ()  token#start_array: xcontentparser#listorderedmap() () 
makes sure that provided token is of the expected type
this method expects that the current field name is the concatenation of a type, a delimiter and a name (ex: terms#foo where "terms" refers to the type of a registered namedxcontentregistry.entry, "#" is the delimiter and "foo" the name of the object to parse). it also expected that following this field name is either an object or an array xcontent structure and the cursor points to the start token of this structure. the method splits the field's name to extract the type and name and then parses the object using the xcontentparser#namedobject(class, string, object) method. from the field's name
returns a binary content builder for the provided content type.
constructs a xcontent builder that will output the result into the provided output stream.
guesses the content type based on the provided bytes and returns the corresponding xcontent from headers. till we fixed the rest layer to read the content-type header, that should be the only place where guessing is needed.
guesses the content (type) based on the provided char sequence and returns the corresponding xcontent the rest layer should move to reading the content-type header instead. there are other places where auto-detection may be needed. this method is deprecated to prevent usages of it from spreading further without specific reasons.
returns the org.elasticsearch.common.xcontent.xcontent for the provided content type.
guesses the content type based on the provided input stream without consuming it. the rest layer should move to reading the content-type header instead. there are other places where auto-detection may be needed. this method is deprecated to prevent usages of it from spreading further without specific reasons.
guesses the content type based on the provided char sequence. the rest layer should move to reading the content-type header instead. there are other places where auto-detection may be needed. this method is deprecated to prevent usages of it from spreading further without specific reasons.
attempts to match the given media type with the known xcontenttype values. this match is done in a case-insensitive manner. the provided media type should not include any parameters. this method is suitable for parsing part of the content-type http header. this method will return null if no match is found
accepts either a format string, which is equivalent to xcontenttype#shortname() or a media type that optionally has parameters and attempts to match the value to an xcontenttype. the comparisons are done in lower case format and this method also supports a wildcard accept for application. this method can be used to parse the accept http header or a format query string parameter. this method will return null if no match is found
finish parsing the object.
creates the consumer that does the "field just arrived" behavior. if the targetobject hasn't been built then it queues the value. otherwise it just applies the value just like objectparser does.
call this to do the actual parsing. this implements bifunction for conveniently integrating with objectparser.
set a constructor argument and build the target object if all constructor arguments have arrived.
queue a consumer that we'll call once the targetobject is built. if targetobject has been built this will fail because the caller should have just applied the consumer immediately.
build the parser. objectparser. from external systems, never when parsing requests from users. the parser, casting the elements of the array to the arguments so they work with your favorite constructor. the objects in the array will be in the same order that you declared the #constructorarg()s and none will be null. the second argument is the value of the context provided to the #parse(xcontentparser, object) parse function. if any of the constructor arguments aren't defined in the xcontent then parsing will throw an error. we use an array here rather than a
this test ensures we can use a classic pull-parsing parser together with the object parser
if the humanreadable flag is set, writes both a formatted and unformatted version of the time value using the date transformer for the
writes a raw field with the value taken from the bytes in the stream
write a time-based value, if the value is null a null value is written, otherwise a date transformers lookup is performed.
tests the non-constructor fields are only set on time.
builds the object in random order and parses it.
helper to declare an object that will be parsed into a bytesreference
creates a parser based on the bytes provided
returns the bytes that represent the xcontent output of the provided toxcontent object, using the provided by the toxcontent#isfragment() method returns.
updates the provided changes into the source. if the key exists in the changes, it overrides the one in source unless both are maps, in which case it recursively updated it. unequal? this is just a .equals check on the objects, but that can take some time on long strings.
writes a "raw" (bytes) field, handling cases where the bytes are compressed, and tries to optimize writing using
converts the given bytes into a map that is optionally ordered. the provided xcontenttype must be non-null.
guesses the content type based on the provided bytes. the rest layer should move to reading the content-type header instead. there are other places where auto-detection may be needed. this method is deprecated to prevent usages of it from spreading further without specific reasons.
convert a string in some xcontent format to a map. throws an elasticsearchparseexception if there is any error. note that unlike #converttomap(bytesreference, boolean), this doesn't automatically uncompress the input.
merges the defaults provided as the second parameter into the content of the first. only does recursive merge for inner maps.
writes a "raw" (bytes) field, handling cases where the bytes are compressed, and tries to optimize writing using auto-detection
convert a string in some xcontent format to a map. throws an elasticsearchparseexception if there is any error.
creates a parser for the bytes using the supplied content-type
register a parser.
lookup a value from the registry by name while checking that the name matches the parsefield.
lookup a value from the registry by name while checking that the name matches the parsefield.
parse a named object, throwing an exception if the parser isn't found. throws an namedobjectnotfoundexception if the
low level implementation detail of xcontentgenerator#copycurrentstructure(xcontentparser).
copy the contents of the given inputstream to the given outputstream. closes both streams when done.
return the long that stringvalue stores or throws an exception if the stored value cannot be converted to a long that stores the exact same value and coerce is false.
extracts raw values (string, int, and so on) based on the path provided returning all of them as a single list.
returns a function that filters a document map based on the given include and exclude rules.
returns an array of string value from a node value. if the node represents an array the corresponding array of strings is returned. otherwise the node is treated as a comma-separated string.
evaluates if a property name matches one of the given filter paths.
construct a new evictingqueue that holds maximumsize elements.
add the given element to the queue, possibly forcing an eviction from the head if #remainingcapacity() is zero.
wraps the given map and prevent adding of null keys. the expected number of elements guaranteed not to cause buffer expansion (inclusive).
same as #copyandremove(object) but for an arbitrary number of entries.
remove the given key from this map. the current hash table is not modified.
return a copy of the provided map.
associate key with value and return a new copy of the hash table. the current hash table is not modified.
returns a direct iterator over the keys.
puts all the entries in the map to the builder.
builds a new instance of the
returns a direct iterator over the keys.
puts all the entries in the map to the builder.
builds a new instance of the
returns a direct iterator over the keys.
returns a direct iterator over the keys.
construct a prefix logger with the specified name and prefix.
called by log4j2 to initialize this converter.
remove the threadcontext used to add deprecation headers to network responses.  this is expected to only be invoked by the node's close method (therefore once outside of tests).
creates a new deprecation logger based on the parent logger. automatically prefixes the logger name with "deprecation", if it starts with "org.elasticsearch.", it replaces "org.elasticsearch" with "org.elasticsearch.deprecation" to maintain the "org.elasticsearch" namespace.
set the threadcontext used to add deprecation headers to network responses.  this is expected to only be invoked by the node's constructor (therefore once outside of tests).
assert that the specified string has the warning value equal to the provided warning value.
encode a string containing characters outside of the legal characters for an rfc 7230 quoted-string.
extracts the warning value from the value of a warning header that is formatted according to rfc 7234. that is, given a string warning value.
called by log4j2 to initialize this converter.
converts a binding for a key> to the value typeliteral. it's a bit awkward because we have to pull out the inner type in the type literal.
returns a just-in-time binding for key, creating it if necessary.
attempts to create a just-in-time binding for key in the root injector, falling back to other ancestor injectors until this injector is tried.
looks up thread local context. creates (and removes) a new context if necessary.
returns a new just-in-time binding created by resolving key. the strategies used to create just-in-time bindings are:  internalizing providers. if the requested binding is for provider, we delegate to the binding for t. converting constants. implementedby and providedby annotations. only for unannotated keys. the constructor of the raw type. only for unannotated keys.  if the binding cannot be created.
converts a constant string binding to the required type. if there was an error resolving the binding
gets a binding implementation. first, it check to see if the parent has a binding. if the parent has a binding and the binding is scoped, it will use that binding. otherwise, this checks for an explicit binding. if no explicit binding is found, it looks for a just-in-time binding.
creates a synthetic binding to provider, i.e. a binding to the provider from
creates a binding for a type annotated with @implementedby.
creates a binding for an injectable type with the given scope. looks for a scope on the type if none is specified.
indexes bindings by type.
creates a binding for a type annotated with @providedby.
returns parameter injectors, or null if there are no parameters.
like #assertinstancebinding(module, class, predicate), but filters the classes checked by the given annotation.
configures the module and checks a map<string, class> of the "to" class is bound to "theclass".
attempts to configure the module, and asserts an illegalargumentexception is caught, containing the given messages
configures the module and asserts "clazz" is not bound to anything.
configures the module, and ensures a map exists between the "keytype" and "valuetype", and that all of the "expected" values are bound.
configures the module and asserts "clazz" is bound to "to".
configures the module, and returns an instance bound to the "to" class.
configures the module and checks a set of the "to" class is bound to "classes". there may be more classes bound to "to" than just "classes".
returns an initializable for an instance that requires no initialization.
returns an array of parameter values.
creates an injector for the given set of modules, in a given development stage. construction
gets the strategy for an annotation type.
gets the strategy for an annotation.
construct an instance. returns object instead of t because it may return a proxy.
sets the stage for the created injector. if the stage is stage#production, this class will eagerly load singletons.
initialize and validate everything.
inject everything that can be injected. this method is intentionally not synchronized. if we locked while injecting members (ie. running user code), things would deadlock should the user code build a just-in-time binding from another thread.
performs creation-time injections on all objects that require it. whenever fulfilling an injection depends on another object that requires injection, we inject it first. if the two instances are codependent (directly or transitively), ordering of injection is arbitrary.
prepares member injectors for all injected instances. this prompts guice to do static analysis on the injected instances.
reentrant. if instance was registered for injection at injector-creation time, this method will ensure that all its members have been injected before returning.
registers an instance for member injection when that step is performed. @inject).
returns a copy of this configuration exception with the specified partial value.
we tolerate duplicate bindings only if one exposes the other.
scopes an internal factory.
replaces annotation scopes with instance scopes using the injector's annotation-to-instance map. if the scope annotation has no corresponding instance, an error will be added and unscoped will be retuned.
the injector is a special case because we allow both parent and child injectors to both have a binding for that key.
the logger is a special case because it knows the injection point of the injected member. it's the only binding that does this.
creates and returns the injector shells for the current modules. multiple shells will be returned if any modules contain binder#newprivatebinder private environments. the primary injector will be first in the returned list.
returns the injectors for the specified injection points.
creates a new members injector and attaches both injection listeners and method aspects.
creates a creationexception containing messages.
installs default converters for primitives, enums, and class literals.
returns the generic form of supertype. for example, if this is arraylist, this returns iterable given the input iterable.class.
returns the resolved generic exception types thrown by constructor.
returns the resolved generic type of field.
returns the resolved generic parameter types of methodorconstructor.
returns an immutable list of the resolved types.
returns the resolved generic return type of method.
returns the type from super class's type parameter in moretypes#canonicalize(type) canonical form.
creates a configurationexception containing messages.
returns a new multibinder that collects instances of type in a set that is itself bound with no binding annotation.
returns a new multibinder that collects instances of type in a set that is itself bound with annotation.
invoked by guice at injector-creation time to prepare providers for each element in this set. at this time the set's size is known, but its contents are only evaluated when get() is invoked.
returns a new multibinder that collects instances of type in a set that is itself bound with annotationtype.
returns a new mapbinder that collects entries of keytype valuetype in a
returns a new mapbinder that collects entries of keytype valuetype in a
returns a new mapbinder that collects entries of keytype valuetype in a
this creates two bindings. one for the map.entry provider> and another for v.
returns an instance of t, constructed using this constructor, with the supplied arguments.
returns the guice key for this parameter.
returns the unique binding annotation from the specified list, or
creates a constant binding to @named(key) for each property. this method binds all properties including those inherited from
creates a constant binding to @named(key) for each entry in
returns the dependencies from the given injection points.
returns the looked up members injector. the result is not valid until this lookup has been initialized, which usually happens when the injector is created. the members injector will throw an illegalstateexception if you try to use it beforehand.
sets the actual members injector.
returns a new injection point for the injectable constructor of type. or a no-arguments constructor that is not private. constructor, or if parameters of the injectable constructor are malformed, such as a parameter with multiple binding annotations.
returns all static method and field injection points on type. fields are returned and then all methods. within the fields, supertype fields are returned before subtype fields. similarly, supertype methods are returned before subtype methods. a field with multiple binding annotations. the exception's configurationexception#getpartialvalue() partial value is a set of the valid injection points.
returns all instance method and field injection points on type. fields are returned and then all methods. within the fields, supertype fields are returned before subtype fields. similarly, supertype methods are returned before subtype methods. a field with multiple binding annotations. the exception's configurationexception#getpartialvalue() partial value is a set of the valid injection points.
records the elements executed by modules.
creates a recording binder that's backed by prototype.
creates a private recording binder.
returns the module composed of elements.
sets the actual provider.
returns a new module that installs all of modules.
returns a provider which always provides instance. this should not be necessary to use in your application, but is helpful for several types of unit tests. permitted to be null, to enable aggressive testing, although in real life a guice-supplied provider will never return null.
returns the calling line of code. the selected line is the nearest to the top of the stack that is not skipped.
returns field.class, method.class or constructor.class.
returns a type that is functionally equal but not necessarily equal according to object#equals(object) object.equals().
returns an equivalent type that's safe for use in a key. the returned type will be free of primitive types. type literals of primitives will return the corresponding wrapper types.
returns true if a and b are equal.
returns the declaring class of typevariable, or null if it was not declared by a class.
returns the generic supertype for supertype. for example, given a class integerset, the result for when supertype is set.class is set and the result when the supertype is collection.class is collection.
formats a member as concise string, such as java.util.arraylist.size,
returns the hashcode of type.
returns a module which creates bindings for provider methods from the given object. this is useful notably for  href="http:code.google.compgoogle-gin">gin
returns a string that is equivalent to the specified string with its first character converted to uppercase as by string#touppercase. the returned string will have the same value as the specified string if its first character is non-alphabetic, if its first character is already uppercase, or if the specified string is of length 0.  for example:  capitalize("foo bar").equals("foo bar"); capitalize("2b or not 2b").equals("2b or not 2b") capitalize("foo bar").equals("foo bar"); capitalize("").equals("");  converted to uppercase
returns the scoping annotation, or null if there isn't one.
gets a key for the given type, member and annotations.
returns the binding annotation on member, or null if there isn't one.
adds an error if there is a misplaced annotations on type. scoping annotations are not allowed on abstract classes or interfaces.
appends the contents of map to appendable, with entries separated by entrydelimiter, and keys and values separated with  each key and value will be converted to a charsequence using note that this implies that null tokens will be appended as the four-character string "null". associated value the beginning or end
appends each of the tokens to appendable, separated by  each token will be converted to a charsequence using note that this implies that null tokens will be appended as the four-character string "null". beginning or end
returns a string containing the tokens, converted to strings if necessary, separated by delimiter. if tokens is empty, it returns an empty string.  each token will be converted to a charsequence using note that this implies that null tokens will be appended as the four-character string "null". beginning or end
returns a string containing the contents of map, with entries separated by entrydelimiter, and keys and values separated with  each key and value will be converted to a charsequence using note that this implies that null tokens will be appended as the four-character string "null". associated value the beginning or end map is empty
returns an instance that uses source as a reference point for newly added errors.
returns value if it is non-null allowed to be null. otherwise a message is added and an errorsexception is thrown.
returns the cause throwable if there is exactly one cause in messages. if there are zero or multiple messages with causes, null is returned.
returns the formatted message for an exception with the specified messages.
return an automaton that matches the given pattern.
match a string against the given pattern, supporting the following simple pattern styles: "xxx", "xxx", "xxx" and "xxxyyy" matches (with an arbitrary number of pattern parts), as well as direct equality.
return an automaton that matches the union of the provided patterns.
normalize the geo point for the given coordinates to lie within their respective normalized ranges.  you can control which coordinate gets normalized with the two flags.  note: a shift of 180 is applied in the longitude if necessary, in order to normalize properly the latitude. if normalizing latitude but not longitude, it is assumed that the longitude is in the form x+k360, with x in ]-180;180], and k is meaningful to the application. therefore x will be adjusted while keeping k preserved.
parse a geopoint with a xcontentparser. a geopoint has one of the following forms:  object: "lat": <latitude>, "lon": <longitude> string: "<latitude>,<longitude>" geohash: "<geohash>" array: [<longitude>,<latitude>] 
return a sortednumericdoublevalues instance that returns the distances to a list of geo-points for each document.
checks that the precision is within range supported by elasticsearch - between 1 and 12 returns the precision value if it is in the range and throws an illegalargumentexception if it is outside the range.
parses the value as a geopoint. the following types of values are supported:  object: has to contain either lat and lon or geohash fields  string: expected to be in "latitude, longitude" format or a geohash  array: two or more elements, the first element is longitude, the second is latitude, the rest is ignored if ignorezvalue is true
check if point is within a rectangle todo: move this to lucene rectangle class
parse a precision that can be expressed as an integer or a distance measure like "1km", "10m". the precision is expressed as a number between 1 and 12 and indicates the length of geohash used to represent geo points.
test an enveloping polygon around the max mercator bounds
get a geodistance according to a given name. valid values are  plane for geodistance.plane arc for geodistance.arc 
creates a geodistance instance from an input stream
encode to a geohash string from the geohash based long format
encode to a geohash string at a given level from a morton long
computes the bounding box coordinates from a given geohash
add all geohashes of the cells next to a given geohash to a list.
encode from geohash string to the geohash based long format (lonlat interleaved, 4 least significant bits = level)
calculate the geohash of a neighbor of a geohash
encode to a morton long value from a given geohash string
invokes geoutils.parseprecision parser on the value generated by tokengenerator  the supplied tokengenerator should generate a single field that contains the precision in one of the supported formats or malformed precision value if error handling is tested. the method return the parsed value or throws an exception, if precision value is malformed.
makes a closed ring out of the current coordinates by adding the starting point as the end point. will have no effect of starting and end point are already the same coordinate.
add a new coordinate to the collection
takes an input polygon and returns an identical one, only with opposing orientation setting. this is done so we don't have to expose a setter for orientation in the actual class
copy all coordinate to a new array
ctor from serialized stream input
ctor from list of coordinates
builds an array of coordinates to a xcontentbuilder
build an envelope from the top left and bottom right coordinates.
read from a stream.
add a new hole to the polygon
create a connected list of a list of coordinates array of point index of the first point number of points
the coordinates setup by the builder will be assembled to a polygon. the result will consist of a set of polygons. each of these components holds a list of linestrings defining the polygon: the first set of coordinates will be used as the shell of the polygon. the others are defined to holes within the polygon. this method also wraps the polygons at the dateline. in order to this fact the result may contains more polygons and less holes than defined in the builder it self.
this method sets the component id of all edges in a ring to a given id and shifts the coordinates of this component according to the dateline
create a multipolygon from a set of coordinates. each primary array contains a polygon which in turn contains an array of linestrings. these line strings are represented as an array of coordinates. the first linestring will be the shell of the polygon the others define holes within the polygon.
validates only 1 vertex is tangential (shared) between the interior and exterior of a polygon
concatenate a set of points to a polygon component id of the polygon direction of the ring list of points to concatenate index of the first point array of edges to write the result to index of the first edge in the result number of points to use
setup for the whole base test class
test that creates new shape from a random test shape and checks both for equality
read from a stream.
add a shallow copy of the polygon to the multipolygon. this will apply the orientation of the
read from a stream.
read from a stream.
decompose a linestring given as array of coordinates at a vertical line.
construct a new linestring. per geojson spec (http:geojson.orggeojson-spec.html#linestring) a linestring must contain two or more coordinates
recursive method which parses the arrays of coordinates used to define shapes parser that will be read from thrown if an error occurs while reading from the xcontentparser
parse the geometries array of a geometrycollection
create a new shapebuilder from xcontent to the shape construction process (e.g., orientation) todo: refactor to place build specific parameters in the spatialcontext if the parsers current token has been null
next word in the stream
throws an exception if the parsed geometry type does not match the expected shape type
next word in the stream
parse geometry from the stream tokenizer
builds url using base url and specified path
constructs new read-only url-based blob store  the following settings are supported  buffer_size - size of the read buffer, defaults to 100kb 
we adjust the instant by displayoffset to adjust for the offset that might have been added in
we adjust the instant by displayoffset to adjust for the offset that might have been added in
parses a joda based pattern, including some named ones (similar to the built in joda iso ones).
this will return a bytes ref composed of the bytes. if this is a direct byte buffer, the bytes will have to be copied.
returns an array of byte buffers from the given bytesreference.
writes the bytes directly to the output stream.
returns bytesreference composed of the provided bytebuffers.
convert an xcontentbuilder into a bytesreference. this method closes the builder, so no further fields may be added.
returns a compact array from the given bytesreference. the returned array won't be copied unless necessary. if you need to modify the returned array use bytesref.deepcopyof(reference.tobytesref() instead
returns a bytesrefiterator for this bytesreference. this method allows access to the internal pages of this reference without copying them. use with care!
compares the two references using the given int function.
the first value of the hit.
parse text, that potentially contains date math into the milliseconds since the epoch examples are 2014-11-18||-2y substracts two years from the input date nowm rounds the current time to minute granularity supported rounding units are y year m month w week (beginning on a monday) d day hh hour m minute s second
configure a specific time zone for a date formatter
configure defaults for missing values in a parser, then return a new compound date formatter
method used to trip the breaker
add a number of bytes, tripping the circuit breaker if the aggregated estimates are above the limit. automatically trips the breaker if the memory limit is set to 0. will never trip the breaker if the limit is set < 0, but can still be used to aggregate estimations.
add an exact number of bytes, not checking for tripping the circuit breaker. this bypasses the overheadconstant multiplication.
create a circuit breaker that will break if the number of estimated bytes grows above the limit. all estimations will be multiplied by the given overheadconstant. uses the given oldbreaker to initialize the starting offset.
create a circuit breaker that will break if the number of estimated bytes grows above the limit. all estimations will be multiplied by the given overheadconstant. uses the given oldbreaker to initialize the starting offset.
add a number of bytes, tripping the circuit breaker if the aggregated estimates are above the limit. automatically trips the breaker if the memory limit is set to 0. will never trip the breaker if the limit is set < 0, but can still be used to aggregate estimations.
method used to trip the breaker, delegates to the parent to determine whether to trip the breaker or not
add an exact number of bytes, not checking for tripping the circuit breaker. this bypasses the overheadconstant multiplication. also does not check with the parent breaker to see if the parent limit has been exceeded.
format a byte array as a hex string.
test that selecting by name is possible and properly matches the addresses on all interfaces and virtual interfaces. note that to avoid that this test fails when interfaces are down or they do not have addresses assigned to them, they are ignored.
test ordinary addresses sort before private addresses
test private addresses sort before link local addresses
test filtering out ipv4ipv6 addresses
convert a byte array into an inetaddress. exception "if ip address is of illegal length." we replace it with an unchecked exception, for use by callers who already know that addr is an array of length 4 or 16.
parse an ip address and its prefix length using the cidr notation.
convert a list of hextets into a human-readable ipv6 address. in order for "::" compression to work, the input should contain negative sentinel values in place of the elided zeroes.
returns the inetaddress having the given string representation. this deliberately avoids all nameservice lookups (e.g. no dns).
returns the string representation of an inetaddress. for ipv4 addresses, this is identical to follows  href="http:tools.ietf.orghtmlrfc5952">rfc 5952 section 4. the main difference is that this method uses "::" for zero compression, while java's version uses the uncompressed form. this method uses hexadecimal for all ipv6 addresses, including ipv4-mapped ipv6 addresses such as "::c000:201". the output does not include a scope id.
resolves (and deduplicates) host specification
resolves bindhosts to a list of internet addresses. the list will not contain duplicate addresses. such as _local_ (see the documentation). if it is null, it will fall back to _local_
resolves a single host specification
closes the channels.
perform actual logging: might throw exception if things go wrong
format network interface flags
log interface configuration at debug level, if its enabled
format internet address: java's default doesn't include everything useful
ensure specifying wildcard ipv6 address selects reasonable publish address
ensure specifying wildcard ipv4 address will bind to all interfaces
ensure exception if we publish to multicast ipv6 address
ensure we can bind to multiple addresses
ensure exception if we publish to multicast ipv4 address
ensure we can't bind to multiple addresses when using wildcard
ensure exception if we bind to multicast ipv6 address
ensure specifying wildcard ipv4 address selects reasonable publish address
ensure exception if we bind to multicast ipv4 address
returns all global scope addresses for interfaces that are up.
returns addresses for the given interface (it must be marked up)
returns all addresses (any scope) for interfaces that are up. this is only used to pick a publish address, when the user set network.host to a wildcard
helper for getinterfaces, recursively adds subinterfaces to target
sorts addresses by order of preference. this is used to pick the first one for publishing
returns all site-local scope (private) addresses for interfaces that are up.
returns all interface-local scope (loopback) addresses for interfaces that are up.
returns only the ipv4 addresses in addresses
returns only the ipv6 addresses in addresses
return all interfaces (and subinterfaces) on the system
sorts an address by preference. this way code like publishing can just pick the first one
parses an ipv4 address block in cidr notation into a pair of longs representing the bottom and top of the address block
round trip test code for both ipv4 and ipv6. inetaddress contains the getbyaddress and test.
adds a transport implementation that can be selected by setting #transport_type_key.
register an allocation command.  this lives here instead of the more aptly named clustermodule because the transport client needs these to be registered.  it is the name under which the command's reader is registered.
creates a network module that custom networking classes can be plugged into.
remove the entry which has this key in the hash table and return the associated value or null if there was no entry associated with this key.
get the value that is associated with key or null if key was not present in the hash table.
constructor.
return a rotated view of the given list with the given distance.
deeply inspects a map, iterable, or object array looking for references back to itself. more context to the handler of the exception
returns true iff the cache needs to be refreshed.
returns the currently cached object and potentially refreshes the cache before returning.
cancel all current running operations. future calls to #checkforcancel() will be failed with the given reason
run the interruptable, capturing the executing thread. concurrent calls to #cancel(string) will interrupt this thread causing the call to prematurely return.
called if #checkforcancel() was invoked after the operation was cancelled. the default implementation always throws an executioncancelledexception, suppressing any other exception that occurred before cancellation
run the interruptable, capturing the executing thread. concurrent calls to #cancel(string) will interrupt this thread causing the call to prematurely return.
tests custom data paths are upgraded
tests upgrade on partially upgraded index, when we crash while upgrading
test method for .
test method for org.apache.lucene.util.bytesrefhash#size().
test method for .
same as dataoutput#writevlong but accepts negative values (written on 9 bytes).
same as dataoutput#readvlong but can read negative values (read on 9 bytes).
constructor.
clears the value, if it has been previously created by calling value. the next call to #getorcompute() will recreate the value.
returns a value that was created by supplier. the value might have been previously created, if not it will be created now, thread safe of course.
constructor.
constructor.
return the key at 0 <= index <= capacity(). the result is undefined if the slot is unused. beware that the content of the bytesref may become invalid as soon as #close() is called
get the id associated with key
parse the given locale as language, language-country or either underscores or hyphens may be used as separators, but consistently, ie. you may not use an hyphen to separate the language from the country and an underscore to separate the country from the variant.
constructor.
constructor.
constructs uri pattern
get the id associated with key or -1 if the key is not contained in the hash.
renames indexfoldername index folders found in node paths and custom path iff #needsupgrade(index, string) is true. index folder in custom paths are renamed first followed by index folders in each node path.
moves the index folder found in source to target
upgrades all indices found under nodeenv. already upgraded indices are ignored.
grow an array to a size that is larger than minsize, preserving content, and potentially reusing part of the provided array.
resize the array to the exact provided size.
allocate a new intarray.
resize the array to the exact provided size.
allocate a new floatarray.
grow an array to a size that is larger than minsize, preserving content, and potentially reusing part of the provided array.
resize the array to the exact provided size.
resize the array to the exact provided size.
allocate a new doublearray.
grow an array to a size that is larger than minsize, preserving content, and potentially reusing part of the provided array.
return the next size to grow to that is >= mintargetsize. inspired from arrayutil#oversize(int, int) and adapted to play nicely with paging.
allocate a new bytearray.
resize the array to the exact provided size.
grow an array to a size that is larger than minsize, preserving content, and potentially reusing part of the provided array.
allocate a new longarray.
resize the array to the exact provided size.
adjust the circuit breaker with the given delta, if the delta is negative, or checkbreaker is false, the breaker will be adjusted without tripping. if the data was already created before calling this method, and the breaker trips, we add the delta without breaking to account for the created data. if the data has not been created yet, we do not add the delta to the breaker if it trips.
grow an array to a size that is larger than minsize, preserving content, and potentially reusing part of the provided array.
allocate a new objectarray.
grow an array to a size that is larger than minsize, preserving content, and potentially reusing part of the provided array.
produces two random sets consisting of elements from [0, endexclusive).
the relative complement, or difference, of the specified left and right set. namely, the resulting set contains all the elements that are in the left set but not in the right set. neither input is mutated by this operation, an entirely new set is returned.
the relative complement, or difference, of the specified left and right set, returned as a sorted set. namely, the resulting set contains all the elements that are in the left set but not in the right set, and the set is sorted using the natural ordering of element type. neither input is mutated by this operation, an entirely new set is returned.
fast forwards the count-down to zero and returns true iff the count down reached zero with this fast forward call otherwise false
decrements the count-down and returns true iff this call reached zero otherwise false
forces adding an element to the queue, without doing size checks.
calls future#get() without the checked exceptions.
calls future#get(long, timeunit) without the checked exceptions.
subclasses should invoke this method to set the result of the computation to an error, throwable. this will set the state of the future to state was successfully changed.
blocks until the task is complete or the timeout expires. throws a
implementation of the actual value retrieval. will return the value on success, an exception on failure, a cancellation on cancellation, or an illegal state if the synchronizer is in an invalid state.
subclasses should invoke this method to set the result of the computation to value. this will set the state of the future to state was successfully changed.
 the default basefuture implementation throws interruptedexception if the current thread is interrupted before or during the call, even if the value is already available. or during the call (optional but recommended).
 the default basefuture implementation throws interruptedexception if the current thread is interrupted before or during the call, even if the value is already available. or during the call (optional but recommended).
tries to acquire the lock for the given key and returns it. if the lock can't be acquired null is returned.
returns true iff the caller thread holds the lock for the given key
acquires a lock for the given key. the key is compared by it's equals method not by object identity. the lock can be acquired by the same thread multiple times. the lock is released by closing the returned releasable.
execute a blank task times times for the executor
return a new executor that will automatically adjust the queue size based on queue throughput.
adds a listener to this future. if the future has not yet completed, the listener will be notified of a response or exception in a runnable submitted to the executorservice provided. if the future has completed, the listener will be notified immediately without forking to a different thread.
test that accounting on whether or not a thread holds a releasable lock is correct. previously we had a bug where on a re-entrant lock that if a thread entered the lock twice we would declare that it does not hold the lock after it exits its first entrance but not its second entrance.
breaks this barrier if it has been reset or broken for any other reason.  note: this call is not atomic in respect to awaitreset calls. a breakifbroken() may be context switched to invoke a reset() prior to await(). this resets the barrier to its initial state - parties not currently waiting at the barrier will not be accounted for! an await that wasn't time limited, will block indefinitely.
sometimes wraps a runnable in an abstractrunnable.
adds the given item to the queue. the listener is notified once the item is processed
 this overrides the default behavior of onafter to add the caveat that it only runs if the #lifecycle is not stopped or closed.  note: this does not guarantee that it won't be stopped concurrently as it invokes #onafterinlifecycle(), but it's a solid attempt at preventing it. for those that use this for rescheduling purposes, the next invocation would be effectively cancelled immediately if that's the case.
 this invokes #doruninlifecycle() only if the #lifecycle is not stopped or closed. otherwise it exits immediately.
sets the element at position i to the given value.
copies the content of the underlying atomic array to a normal one.
returns the it as a non null list.
removes the current context and resets a new context that contains a merge of the current headers and the given headers. the removed context can be restored when closing the returned storedcontext. the merge strategy is that headers that are already existing are preserved unless they are defaults.
same as #newrestorablecontext(boolean) but wraps an existing context to restore.
saves the current thread context and wraps command in a runnable that restores that context before running command. if command has already been passed through this method then it is returned unaltered rather than wrapped twice.
returns the header for the given key or null if not present
just like #stashcontext() but no default context is set.
removes the current context and resets a default context. the removed context can be restored when closing the returned storedcontext
creates a new threadcontext instance
get a copy of all response headers.
returns all of the request contexts headers
timeout callback might remain in the timer scheduling queue for some time and it might hold the pointers to other objects. as a result it's possible to run out of memory if a large number of tasks are executed
returns a bytesreference view of the data.
add this setting to the builder if it doesn't exists in the source settings. the value added to the builder is taken from the given default settings object.
returns a string representation of the concrete setting key
returns a map of all namespaces to it's values give the provided settings
returns the value for this setting but falls back to the second provided settings object
returns a string representation of the concrete setting key
updates settings that depend on each other. see abstractscopedsettings#addsettingsupdateconsumer(setting, setting, biconsumer) and its usage for details.
build the updater responsible for validating new values, logging the new value, and eventually setting the value where it belongs.
returns a new copy of this object that is backed by its own char array. closing the new instance has no effect on the instance it was created from. this is useful for apis which accept a char array and you want to be safe about the api potentially modifying the char array. for example:  try (securestring copy = securestring.clone()) pass thee char[] to a external api passwordauthentication auth = new passwordauthentication(username, copy.getchars()); ... 
constant time equality to avoid potential timing attacks.
throw an exception if this string has been closed, indicating something is trying to access the data after being closed.
it is possible to retrieve the setting names even if the keystore is closed. this allows securesetting to correctly determine that a entry exists even though it cannot be read. thus attempting to read a secure setting after the keystore is closed will generate a "keystore is closed" exception rather than using the fallback setting.
set a string setting.
ensure the given setting name is allowed.
add the bootstrap seed setting, which may be used as a unique, secure, random value by the node
upgrades the format of the keystore, if necessary.
remove the given setting from the keystore.
set a file setting.
encrypt the keystore entries and return the encrypted data.
decrypts the underlying keystore data. this may only be called once.
write the keystore to the given config directory.
loads information about the elasticsearch keystore from the provided config directory. returns null if no keystore exists.
constructs a new keystore with the given password.
merge the given secure settings into this one.
adds a settings consumer for affix settings. affix settings have a namespace associated to it that needs to be available to the consumer in order to be processed correctly. this consumer will get a namespace to value map instead of each individual namespace and value as in #addaffixupdateconsumer(setting.affixsetting, biconsumer, biconsumer)
adds a settings consumer with a predicate that is only evaluated at update time.  note: only settings registered in settingsmodule can be changed dynamically.  this is useful to add additional validation to settings at runtime compared to at startup time.
returns true if the given key is a valid delete key
adds a settings consumer for affix settings. affix settings have a namespace associated to it that needs to be available to the consumer in order to be processed correctly.
archives invalid or unknown settings. any setting that is not recognized or fails validation will be archived. this means the setting is prefixed with @value archived_settings_prefix and remains in the settings object. this can be used to detect invalid settings via apis. associated value) associated value and an exception)
adds a settings consumer that accepts the values for two settings. the consumer is only notified if one or both settings change and if the provided validator succeeded.  note: only settings registered in settingsmodule can be changed dynamically.  this method registers a compound updater that is useful if two settings are depending on each other. the consumer is always provided with both values even if only one of the two changes.
applies the given settings to all the settings consumers or to none of them. the settings will be merged with the node settings before they are applied while given settings override existing node settings.
updates a target settings builder with new, updated or deleted settings from a given settings builder. removed from this builder true and a non-dynamic setting is updated an exception is thrown.
returns the setting for the given key or null if the setting can not be found.
returns a settings object that contains all settings that are not already set in the given source. the diff contains either the default value for each setting or the settings value in the given default settings.
validates that the settings is valid.
validates the given settings by running it through all update listeners without applying it. this method will not change any settings but will fail if any of the settings can't be applied.
returns the value for the given setting.
validates that all settings are registered and valid.
registers a settings filter pattern that allows to filter out certain settings that for instance contain sensitive information or if a setting is for internal purposes only. the given pattern must either be a valid settings key or a simple regexp pattern.
registers a new setting. this method should be used by plugins in order to expose any custom settings the plugin defines. unless a setting is registered the setting is unusable. if a setting is never the less specified the node will reject the setting during startup.
returns group settings for the given setting prefix.
sets all the provided settings.
the values associated with a setting key as an immutable list.  it will also automatically load a comma separated list under the settingprefix and merge with the numbered format.
returns the setting value (as int) associated with the setting key. if it does not exists, returns the default value provided.
loads settings from the actual string content that represents them using #fromxcontent(xcontentparser)
returns the fully qualified setting names contained in this settings object.
returns a parsed version.
returns the setting value (as double) associated with the setting key. if it does not exists, returns the default value provided.
sets the setting with the provided setting key and a list of values.
loads settings from a stream that represents them using #fromxcontent(xcontentparser)
returns the settings as delimited string.
returns the setting value (as float) associated with the setting key. if it does not exists, returns the default value provided.
checks that all settings in the builder start with the specified prefix. if a setting doesn't start with the prefix, the builder appends the prefix to such setting.
sets the setting group.
returns the setting value (as long) associated with the setting key. if it does not exists, returns the default value provided.
read from a stream.
read from a stream.
tests that docwriteresponse#toxcontent(xcontentbuilder, toxcontent.params) doesn't include forced_refresh unless it is true. we can't assert this in the yaml tests because "not found" is also "false" there....
parse the output of the #innertoxcontent(xcontentbuilder, params) method. this method is intended to be called by subclasses and must be called multiple times to parse all the information concerning if needed and then immediately returns.
return the relative uri for the location of the document suitable for use in the location header. the use of relative uris is permitted as of http1.1 (cf. https:tools.ietf.orghtmlrfc7231#section-7.1.2).
write a document write (indexdeleteupdate) request
read a document write (indexdeleteupdate) request
creates a listener that listens for a response (or failure) and executes the corresponding consumer when the response (or failure) is received.
notifies every given listener with the failure passed to #onfailure(exception). if a listener itself throws an exception all remaining listeners will be processed and the caught exception will be re-thrown.
converts a listener to a biconsumer for compatibility with the java.util.concurrent.completablefuture api.
notifies every given listener with the response passed to #onresponse(object). if a listener itself throws an exception the exception is forwarded to #onfailure(exception). if in turn #onfailure(exception) fails all remaining listeners will be processed and the caught exception will be re-thrown.
indicate that _source should be returned, with an "include" andor "exclude" set which can include simple wildcard elements.
indicate that _source should be returned, with an "include" andor "exclude" set which can include simple wildcard elements.
indicates whether the response should contain the stored _source
the list of field names to retrieve
controls the shard routing of the request. using this value to hash the shard and not the id.
returns a tuple of deleteresponses.  the left element is the actual deleteresponse to serialize while the right element is the expected deleteresponse after parsing.
test setting the source for the request with different available setters
returns a tuple of indexresponses.  the left element is the actual indexresponse to serialize while the right element is the expected indexresponse after parsing.
index the map as the provided content type.
sets the content source to index.  note: the number of objects passed to this method as varargs must be an even number. also the first argument in each pair (the field name) must have a valid string representation. 
sets a string representation of the #optype(optype). can be either "index" or "create".
controls the shard routing of the request. using this value to hash the shard and not the id.
sets the type of operation to perform.
the index name of the document.
the type of the document.
the id of the document.
indicates whether the response should contain the stored _source.
indicate that _source should be returned, with an "include" andor "exclude" set which can include simple wildcard elements.
indicate that _source should be returned, with an "include" andor "exclude" set which can include simple wildcard elements.
this method can be used to parse a getresponse object when it has been printed out as a xcontent using the #toxcontent(xcontentbuilder, params) method.  for forward compatibility reason this method might not fail if it tries to parse a field it doesn't know. but before returning the result it will check that enough information were parsed to return a valid getresponse instance and throws a parsingexception otherwise. this is the case when we get a 404 back, which can be parsed as a normal of this method needs a way to figure out whether we got back a valid get response, which can be done by catching parsingexception.
read from a stream.
read from a stream.
read from a stream.
test behavior when the named analysis component isn't defined on the index. in that case we should build with defaults.
this test is equivalent of calling _analyze against a specific index. the index specific value for the maximum token count is used.
this test is equivalent of calling _analyze without a specific index. the default value for the maximum token count is used.
utility method which computes total memory by adding fielddata, percolatorcache, segments (memory, index writer, version map)
setting the "ignoreunavailable" option prevents indexclosedexception
set statuses to filter shards to get stores info on. see clusterhealthstatus for details. defaults to "yellow" and "red" status
returns the string value for the specified index and setting. if the includedefaults flag was not set or set to false on the getindexrequest, this method will only return a value where the setting was explicitly set on the index. if the includedefaults flag was set to true on the getindexrequest, this method will fall back to return the default value if the setting was not explicitly set.
adds mapping that will be added when the index gets created.
sets the aliases that will be associated with the index when it gets created
sets the settings and mappings as a single source.
adds mapping that will be added when the index gets created.
sets the aliases that will be associated with the index when it gets created
the settings to create the index with (either jsonyamlproperties format)
tests that we can manually recover from a failed allocation due to shards being moved away etc.
test setting the source with available setters
test setting the settings with available setters
the upgrade request works against all shards.
finds all indices that have not all primaries available
set the default routing.
add the action to this request and validate it.
aliases to use with this action.
validate that the action is sane. called when the action is added to the request because actions can be invalid while being built.
set the index this action is operating on.
set the alias this action is operating on.
read from a stream.
allows to read an alias from the provided input stream
associates a filter to the alias
associates a filter to the alias
parses an alias and returns its parsed representation
fills alias result with empty entries for requested indices when no specific aliases were requested.
adds mapping that will be added when the index gets created.
the template source definition.
sets the aliases that will be associated with the index when it gets created
adds mapping that will be added when the index gets created.
the template source definition.
the settings to create the index template with (either json or yaml format).
sets the aliases that will be associated with the index when it gets created
the mapping type consisting of fieldproperties pairs (e.g. "field1", "type=string,store=true") if the number of the source arguments is not divisible by two
the mapping source definition.
the mapping source definition.
returns a random putmappingrequest.
returns the mappings of a specific field.
other attribute extract object. extracted object group by attributeclassname
sets the settings to be updated (either json or yaml format)
returns the string value for the specified index and setting. if the includedefaults flag was not set or set to false on the getsettingsrequest, this method will only return a value where the setting was explicitly set on the index. if the includedefaults flag was set to true on the getsettingsrequest, this method will fall back to return the default value if the setting was not explicitly set.
if the newly created index matches with an index template whose aliases contains the rollover alias, the rollover alias will point to multiple indices. this causes indexing requests to be rejected. to avoid this, we make sure that there is no duplicated alias in index templates before creating a new index.
adds condition to check if the index has at least numdocs
adds a size-based condition to check if the index size is at least size.
adds condition to check if the index is at least age old
sets the number of shard copies that must be active before getting the health status. defaults to activeshardcount#none, meaning we don't wait on any active shards. set this value to activeshardcount#all to wait for all shards (primary and all replicas) to be active across all indices in the cluster. otherwise, use total number of shard copies that would exist across all indices in the cluster.
sets the number of shard copies that must be active across all indices before getting the health status. defaults to activeshardcount#none, meaning we don't wait on any active shards. set this value to activeshardcount#all to wait for all shards (primary and all replicas) to be active across all indices in the cluster. otherwise, use total number of shard copies to wait for.
for xcontent parser and serialization tests
allows to explicitly override the derived cluster health status.
build from lists of information about each node.
build from looking at a list of node statistics.
build the stats from information about each node.
test that empty transporthttp types are not printed out as part of the cluster stats xcontent output.
sets the repository settings.
parses repository definition.
returns an ordered list based on modules name
returns an ordered list based on plugins name
fetch the task status from the list tasks api using it's "fallback to get from the task index" behavior. asserts some obvious stuff about the fetched task and returns a map of it's status.
test waiting for a task that times out.
test wait for completion.
wait for the test task to be running on all nodes and return the taskid of the primary task.
registers recording task event listeners with the given action mask on all nodes
very basic "is it plugged in" style test that indexes a document and makes sure that you can fetch the status of the process. the goal here is to verify that the large moving parts that make fetching task status work fit together rather than to verify any particular status results from indexing. for that, look at transportreplicationactiontests. we intentionally don't use the task recording mechanism used in other places in this test so we can make sure that the status fetching works properly over the wire.
returns all events that satisfy the criteria across all nodes
resets all recording task event listeners with the given action mask on all nodes
this test starts nodes actions that blocks on all nodes. while node actions are blocked in the middle of execution it executes a tasks action that targets these blocked node actions. the test verifies that task actions are only getting executed on nodes that are not listed in the node filter.
returns the list of tasks by node
presents a flat list of tasks
convert this task response to xcontent grouping by executing nodes.
convert this response to xcontent grouping by parent tasks.
called after waiting for the task to complete. attempts to load the results of the task from the tasks index. if it isn't in the index then returns a snapshot of the task taken shortly after completion.
send a getrequest to the tasks index looking for a persisted copy of the task completed task. it'll only be found only if the task's result was stored. called on the node that once had the task if that node is still part of the cluster or on the coordinating node if the node is no longer part of the cluster.
called with the getresponse from loading the task from the results index. called on the node that once had the task if that node is part of the cluster or on the coordinating node if the node wasn't part of the cluster.
executed on the coordinating node to forward execution of the remaining work to the node that matches that requested
executed on the node that should be running the task to find and return the running task. falls back to
sets repository-specific restore settings  see repository documentation for more information.
parses restore definition
sets settings that should be addedchanged in all restored indices
sets repository-specific snapshot settings.  see repository documentation for more information.
parses snapshot definition.
returns http status   reststatus#accepted if snapshot is still in progress  reststatus#ok if snapshot was successful or partially successful  reststatus#internal_server_error if snapshot failed completely 
generate snapshot state from code
reads snapshot status from stream input
returns list of snapshot indices
sets the transient settings to be updated. they will not survive a full cluster restart
sets the persistent settings to be updated. they will get applied cross restarts
partitions the settings into those that are known and valid versus those that are unknown or invalid. the resulting tuple contains the known and valid settings in the first component and the unknown or invalid settings in the second component. note that archived settings contained in the settings to partition are included in the first component.
returns the string value of the setting for the specified index. the order of search is first in persistent settings the transient settings and finally the default settings.
the index name of the document.
the type of the document.
the id of the document.
constructs a new term vector request for a document that will be fetch from the provided index. use #type(string) and
populates a request object (pre-populated with defaults) based on a parser.
header information as bytesref.
generate test documentsthe returned documents are already indexed.
invokes #accept(bulkrequest, actionlistener). backs off on the provided exception and delegates results to the provided listener. retries will be scheduled using the class's thread pool.
completes the operation without doing anything on the primary
indicates that the operation needs to be failed as the required mapping didn't arrive in time
builds the bulk shard response to return to the user
the current operation has been executed on the primary with the specified result
finishes the execution of the current request, with the response that should be returned to the user
closes the processor. if flushing by time is enabled, then it's shutdown. any remaining bulk actions are flushed.  if concurrent requests are not enabled, returns true immediately. if concurrent requests are enabled, waits for up to the specified timeout for all bulk requests to complete then returns true, if the specified waiting time elapses before all bulk requests complete, false is returned.
adds the data from the bytes to be processed by the bulk processor
flush pending delete or index requests.
sets a custom backoff policy. the backoff policy defines how the bulk processor should handle retries of bulk requests internally in case they have failed due to resource constraints (i.e. a thread pool was full). the default is to back off exponentially.
the failure message, null if it did not fail.
the type of the action.
reads a bulkitemresponse from a xcontentparser. the item in the bulkresponse#getitems array.
the version of the action.
read from a stream.
the index name of the action.
the id of the action.
abort this request, and store a org.elasticsearch.action.bulk.bulkitemresponse.failure response.
this tests that the transportbulkaction evaluates alias routing values correctly when dealing with an alias pointing to multiple indices, while a write index exits.
returns the sliced bytesreference. if the xcontenttype is json, the byte preceding the marker is checked to see if it is a carriage return and if so, the bytesreference is sliced so that the carriage return is ignored
add a request to the current bulkrequest.
executes bulk item requests and handles request execution exceptions
determines whether a bulk item request should be executed on the replica. when primary execution resulted in noop (only possible for write requests from pre-6.0 nodes)
creates a new bulk item result from the given requests and result of performing the update operation on the shard.
executes index operation on primary shard after updates mapping if dynamic mappings are found
has anything failed with the execution.
see: #frombyte(byte)
fail if there is wildcard usage in indices and the named is required for destructive operations.
should the index be auto created?
waits on the specified number of active shards to be started before executing the
returns true iff the active shard count in the shard routing table is enough to meet the required shard count represented by this instance.
returns true iff the given cluster state's routing table contains enough active shards for the given indices to meet the required shard count represented by this instance.
parses the active shard count from the given string. valid values are "all" for all shard copies, null for the default value (which defaults to one shard copy), or a numeric value greater than or equal to 0. any other input will throw an illegalargumentexception.
get an activeshardcount instance for the given value. the value is first validated to ensure it is a valid shard count and throws an illegalargumentexception if validation fails. valid values are any non-negative number. directly use activeshardcount#default for the default value (which is one shard copy) or activeshardcount#all to specify all the shards.
returns true iff the given number of active shards is enough to meet the required shard count represented by this instance. this method should only be invoked with activeshardcount objects created from #from(int), or #none or #one.
use this method when the transport action should continue to run in the context of the current task
execute the transport action on the local node, returning the task used to track its execution and accepting a
use this method when the transport action call should result in creation of a new task associated with the call. this is a typical behavior.
creates a new listener
parse the string representation of a refresh policy, usually from a request parameter.
syncs operation result to the translog or throws a shard not available failure
calls the response listener if all pending operations have returned otherwise it just decrements the pending opts counter.
respond if the refresh has occurred and the listener is ready. always called while synchronized on this.
respond if the refresh has occurred and the listener is ready. always called while synchronized on this.
checks whether we can perform a write based on the required active shard count setting. returns null if ok to proceed, or a string describing the reason to stop
sends the specified replica request to the specified node.
sets the current phase on the task if it isn't null. pulled into its own method because its more convenient that way.
tries to acquire reference to indexshard to perform a primary operation. released after performing primary operation locally and replication of the operation to all replica shards is completed failed (see replicationoperation).
resolves derived values in the request. for example, the target shard id of the incoming request, if not set at request construction. additional processing or validation of the request should be done here.
this test ensures that replication operations adhere to the indexmetadata#setting_wait_for_active_shards setting when the request is using the default value for waitforactiveshards.
transport channel that is needed for replica operation testing.
when relocating a primary shard, there is a cluster state update at the end of relocation where the active primary is switched from the relocation source to the relocation target. if relocation source receives and processes this cluster state before the relocation target, there is a time span where relocation source believes active primary to be on relocation target and relocation target believes active primary to be on relocation source. this results in replication requests being sent back and forth.  this test checks that replication request is not routed back from relocation target to relocation source in case of stale index routing table on relocation target.
test that a replica request is rejected if it arrives at a shard with a wrong allocation id
test that a primary request is rejected if it arrives at a shard with a wrong allocation id or term
test throwing a org.elasticsearch.action.support.replication.transportreplicationaction.retryonreplicaexception causes a retry
creates cluster state with an index that has #(numberofprimaries) primary shards in the started state and no replicas. the cluster state contains #(numberofnodes) nodes and assigns primaries to those nodes.
creates cluster state with the given indices, each index containing #(numberofprimaries) started primary shards and no replicas. the cluster state contains #(numberofnodes) nodes and assigns primaries to those nodes.
creates cluster state with and index that has one shard and as many replicas as numberofreplicas. primary will be started in cluster state. some (unassignedreplicas) will be unassigned and some (assignedreplicas) will be one of initializing, started or relocating.
creates a cluster state where local node and master node can be specified
creates cluster state with several shards and one replica and all shards started.
creates cluster state with and index that has one shard and #(replicastates) replicas
creates a cluster state with no index
creates cluster state with several indexes, shards and replicas and all shards started.
resolve node ids to concrete nodes of the incoming request
map the responses into noderesponseclass responses and failednodeexceptions.
lazily build and get a map of node id to node response.
rethrow task failures if there are any.
allows to directly call transportmasternodeaction#masteroperation(masternoderequest, clusterstate, actionlistener) which is a protected method.
indexing operations which entail mapping changes require a blocking request to the master node to update the mapping. if the master node is being disrupted or if it cannot commit cluster state changes, it needs to retry within timeout limits. this retry logic is implemented in transportmasternodeaction and tested by the following master failover scenario.
parse the current token and update the parsing context appropriately.
returns a tuple of updateresponses.  the left element is the actual updateresponse to serialize while the right element is the expected updateresponse after parsing.
add a script parameter.
indicate that _source should be returned with every hit, with an "include" andor "exclude" set which can include simple wildcard elements. an optional include (optionally wildcarded) pattern to filter the returned _source an optional exclude (optionally wildcarded) pattern to filter the returned _source
the script to execute. note, make sure not to send different script each times and instead use script params if possible with the same (automatically compiled) script.
controls the shard routing of the request. using this value to hash the shard and not the id.
indicate that _source should be returned, with an "include" andor "exclude" set which can include simple wildcard elements. an optional list of include (optionally wildcarded) pattern to filter the returned _source an optional list of exclude (optionally wildcarded) pattern to filter the returned _source
the script to execute. note, make sure not to send different script each times and instead use script params if possible with the same (automatically compiled) script. the script to execute the script language the script type the script parameters
indicates whether the response should contain the updated _source.
applies updaterequest#fetchsource() to the _source of the updated document to be returned in a update response.
prepare the request for merging the existing document with a new one, can optionally detect a noop change. returns a result containing a new indexrequest to be executed on the primary and replicas.
prepare the request for updating an existing document using a script. executes the script and returns a result containing either a new indexrequest or deleterequest (depending on the script's returned "op" value) to be executed on the primary and replicas.
calculate the version to use for the update request, using either the existing version if internal versioning is used, or the get result document's version if the version type is "force".
prepares an update request by converting it into an index or delete request or an update response (no action, in the event of a noop).
execute a scripted upsert, where there is an existing upsert document and a script to be executed. the script is executed and a new tuple of operation and updated _source is returned.
calculate a routing value to be used, either the included index request's routing, or retrieved document's routing when defined.
prepare the request for upsert, executing the upsert script if present, and returning a result containing a new
prepares an update request by converting it into an index or delete request or an update response (no action).
sets a threshold that enforces a pre-filter roundtrip to pre-filter search shards based on query rewriting if the number of shards the search request expands to exceeds the threshold. this filter roundtrip can limit the number of shards significantly if for instance a shard can not match any documents based on it's rewrite method ie. if date filters are mandatory to match but the shard bounds and the query are disjoint. the default is 128
constructs a new search request from reading the specified stream.
sets the number of shard results that should be reduced at once on the coordinating node. this value should be used as a protection mechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.
sets the number of shard requests that should be executed concurrently on a single node. this value should be used as a protection mechanism to reduce the number of shard requests fired per high level search request. searches that hit the entire cluster can be throttled with this number to reduce the cluster load. the default is 5
constructs a new search request against the provided indices with the given search source.
executes a single request from the queue of requests. when a request finishes, another request is taken from the queue. when a request is executed, a permit is taken on the specified semaphore, and released as each request completes.
returns the profile results for this search response (including all shards). an empty map is returned if profiling was not enabled
returns a new arraysearchphaseresults instance. this might return an instance that reduces search responses incrementally.
builds an array, with potential null elements, with docs to load.
creates a new queryphaseresultconsumer the buffer is used to incrementally reduce aggregation results before all shards responded.
enriches search hits and completion suggestion hits from sorteddocs using fetchresultsarr, merges suggestions, aggregations and profile results expects sorteddocs to have top search docs across all shards, optionally followed by top suggest docs for each named completion suggestion ordered by suggestion name
reduces the given query results and consumes all aggregations and profile results. from all non-null query results. from all non-null query results.
returns a score doc array of top n search docs across all shards, followed by top suggest docs for each named completion suggestion across all shards. if more than one named completion suggestion is specified in the request, the suggest docs for a named suggestion are ordered by the suggestion name. note: the order of the sorted score docs depends on the shard index in the result array if the merge process needs to disambiguate the result. in oder to obtain stable results the shard index (index of the result in the result array) must be the same. enabled only for scroll search, because that only retrieves hits of length 'size' in the query phase.
performs an intermediate reduce phase on the aggregations. for instance with this reduce phase never prune information that relevant for the final reduce step. for final reduce see #reduceaggs(list, list, reducecontext)
forcefully counts down an operation and executes the provided runnable if all expected operations where executed
sets the result to the given array index and then runs #countdown()
escalates the failure via searchphasecontext#onshardfailure(int, searchshardtarget, exception) and then runs #countdown()
releases shard targets that are not used in the docsidstoload.
this method collects nodes from the remote clusters asynchronously if any of the scroll ids references a remote cluster. otherwise the action listener will be invoked immediately with a function based on the given discovery nodes.
this method should be called if a search phase failed to ensure all relevant search contexts and resources are released. this method will also notify the listener and sends back a failure to the user.
this is the main entry point for a search. this method starts the search execution of the initial phase.
add a search request to execute. note, the order is important, the search response will be returned in the same order as the search requests.  if ignoreindices has been set on the search request, then the indicesoptions of the multi search request will not be used (if set).
add a search request to execute. note, the order is important, the search response will be returned in the same order as the search requests.
parse a search scroll request from a request body provided through the rest layer. values that are already be set and are also found while parsing will be overridden.
returns true iff the search request has inner hits and needs field collapsing
sets how many search requests specified in this multi search requests are allowed to be ran concurrently.
constructs search type based on the internal id.
the a string representation search type to execute, defaults to searchtype#default. can be one of "dfs_query_then_fetch""dfsquerythenfetch", "dfs_query_and_fetch""dfsqueryandfetch", "query_then_fetch""querythenfetch" and "query_and_fetch""queryandfetch".
returns a connection to the given node on the provided cluster. if the cluster alias is null the node will be resolved against the local cluster.
used by transportsearchaction to send the expand queries (field collapsing).
if minimal is set, don't include search hits, aggregations, suggest etc... to make test simpler
the "_shardtotalfailures" section makes if impossible to directly compare xcontent, because the failures in the parsed searchresponse are wrapped in an extra elasticsearchexception on the client side. because of this, in this special test case we compare the "top level" fields for equality and the subsections xcontent equivalence independently
clears the caches for the given shard id if the shard is still allocated on this node
return the shard with the provided id, or throw an exception if it doesn't exist.
return the shard with the provided id, or null if there is no such shard.
creates a new queryshardcontext. the context has not types set yet, if types are required set them via passing a null indexreader will return a valid context, however it won't be able to make
construct the index module for the index with the specified index settings. the index module contains extension points for plugins via org.elasticsearch.plugins.pluginsservice#onindexmodule(indexmodule).
registers the given similarity with the given name. the function takes as parameters: settings for this similarity version of elasticsearch when the index was created scriptservice, for script-based similarities 
adds an searchoperationlistener for this index. all listeners added here are maintained for the entire index lifecycle on this node. once an index is closed or deleted these listeners go out of scope.  note: an index might be created on a node multiple times. for instance if the last shard from an index is relocated to another node the internal representation will be destroyed which includes the registered listeners. once the node holds at least one shard of an index all modules are reloaded and listeners are registered again. listeners can't be unregistered they will stay alive for the entire time the index is allocated on a node. 
adds an indexeventlistener for this index. all listeners added here are maintained for the entire index lifecycle on this node. once an index is closed or deleted these listeners go out of scope.  note: an index might be created on a node multiple times. for instance if the last shard from an index is relocated to another node the internal representation will be destroyed which includes the registered listeners. once the node holds at least one shard of an index all modules are reloaded and listeners are registered again. listeners can't be unregistered they will stay alive for the entire time the index is allocated on a node. 
creates a new mapper service to do administrative work like mapping updates. this should not be used for document parsing. doing so will result in an exception.
adds a setting and it's consumer for this index.
adds a setting, it's consumer and validator for this index.
adds an indexingoperationlistener for this index. all listeners added here are maintained for the entire index lifecycle on this node. once an index is closed or deleted these listeners go out of scope.  note: an index might be created on a node multiple times. for instance if the last shard from an index is relocated to another node the internal representation will be destroyed which includes the registered listeners. once the node holds at least one shard of an index all modules are reloaded and listeners are registered again. listeners can't be unregistered they will stay alive for the entire time the index is allocated on a node. 
builds the sort order from the settings for this index or returns null if this index has no sort.
tests that an explicit request makes block_until_refresh return. it doesn't check that block_until_refresh doesn't return until the explicit refresh if the interval is -1 because we don't have that kind of control over refresh. it can happen all on its own.
updates the settings and index metadata and notifies all registered settings consumers with the new settings iff at least one setting has changed.
creates a new indexsettings instance. the given node settings will be merged with the settings in the metadata while index level settings will overwrite node settings.
constructs a new index component, with the index name and its settings.
expert: directly set the maximum number of merge threads and simultaneous merges allowed.
sets random aliases to the provided createindexrequest
returns a settings instance which include random values for
adds random mapping fields to the provided xcontentbuilder
returns a random createindexrequest. randomizes the index name, the aliases, mappings and settings associated with the index.
executes all registered async actions and notifies the listener once it's done. the value that is passed to the listener is always null. the list of registered actions is cleared once this method returns.
read from a stream.
add a span clause to the current list of clauses
constructs a new spangapquerybuilder term query.
read from a stream.
sets the type of the text query.
adds a field to run the multi match against.
constructs a new text query.
sets the operator to use when using a boolean query. defaults to or.
set the phrase slop if evaluated to a phrase query type.
when using fuzzy or prefix type query, the number of term expansions to use. defaults to unbounded so its recommended to set it to a reasonable value for faster execution.
sets the type of the text query.
adds a field to run the multi match against with a specific boost.
create new bounding box query.
parses the bounding box and returns bottom, top, left, right coordinates
sets the type of executing of the geo bounding box. can be either `memory` or `indexed`. defaults to `memory`.
read from a stream.
adds top left point.
adds points from a single geohash.
visible only for testing purposes.
adds a clause that should be matched by the returned documents. for a boolean query with no for the booleanquery to match. no null value allowed.
adds a query that must not appear in the matching documents. no null value allowed.
adds a query that must appear in the matching documents but will not contribute to scoring. no null value allowed.
adds a query that must appear in the matching documents and will contribute to scoring. no null value allowed.
read from a stream.
resolves the combined or'ed value for the given list of regular expression flags. the given flags must follow the following syntax:   where flag_name is one of the following:  intersection complement empty anystring interval none all   example: intersection|complement|empty
read from a stream.
number of terms that must match the generated query expressed in the common syntax for minimum should match. defaults to 30%.
sets the field names that will be used when generating the 'more like this' query.
convert this to a termvectorsrequest for fetching the terms of the document.
read from a stream.
constructor for a given item document request
converts an array of string ids to and item[].
parses and returns the given item.
constructor for an artificial document request, that is not present in the index.
constructs a new common terms query.
read from a stream.
create a new multi term query of a random type
create a new query of a random type
`fuzziness` is not allowed for `cross_fields`, `phrase` and `phrase_prefix` and should throw an error
a filter to filter indexed shapes that are not intersection with the query shape
a filter to filter indexed shapes that are contained by a shape
a filter to filter indexed shapes intersecting with shapes
helper method than can be used to add error messages to an existing queryvalidationexception. when passing null as the initial exception, a new exception is created.
helper method than can be used to add error messages to an existing queryvalidationexception. when passing null as the initial exception, a new exception is created.
construct a new simple query with this query string.
read from a stream.
add a field to run the query against with a specific boost.
add a field to run the query against.
specifying a timezone together with an unmapped field should throw an exception.
read from a stream.
sets a slop factor for phrase queries
sets query to use in case no query terms are available, e.g. after analysis removed them. defaults to zerotermsquery#none, but can be set to
read from a stream.
constructs a new fieldmaskingspanquerybuilder given an inner spanquerybuilder for a given field
read from a stream.
bwc serialization for parentchild innerhitbuilder. should only be used to send hasparent or haschild inner hits to nodes pre 5.5.
read from a stream.
bwc serialization for collapsing innerhitbuilder. should only be used to send collapsing inner hits to nodes pre 5.5.
bwc serialization for nested innerhitbuilder. should only be used to send nested inner hits to nodes pre 5.5.
sets the stored fields to load and return. if none are specified, the source of the document will be returned.
adds a field to load from the docvalue and return.
rewrites the given rewriteable and fetches pending async tasks for each round before rewriting again.
rewrites the given rewriteable into its primitive form. rewriteables that for instance fetch resources from remote hosts or can simplify optimize itself should do their heavy lifting during rewriteable until it doesn't change anymore. rewrite. see queryrewritecontext#executeasyncactions(actionlistener) for detals
rewrites each element of the list until it doesn't change and returns a new list iff there is at least one element of the list that changed during it's rewrite. otherwise the given list instance is returned unchanged.
read from a stream.
the number of term expansions to use.
sets a slop factor for phrase queries
gets the search analyzer for the given field, or the default if there is none present for the field todo: remove this by moving defaults into mappers themselves
this method fails if #freezecontext() is called before on this context. this is used to seal. this methods and all methods that call it should be final to ensure that setting the request as not cacheable and the freezing behaviour of this class cannot be bypassed. this is important so we can trust when this class says a request can be cached.
gets the search quote analyzer for the given field, or the default if there is none present for the field todo: remove this by moving defaults into mappers themselves
creates a query builder given a query provided as a bytes array
creates a query builder given a query provided as a string
creates a query builder given a query provided as a bytesreference
constructs a new regex query.
read from a stream.
query that returns spans from little that are contained in a spans from big.
read from a stream.
construct a span query matching spans from include which have no overlap with spans from exclude.
read from a stream.
here we could go overboard and use a pre-generated indexed random document for a given item, but for now we'd prefer to simply return the id as the content of the document and that for every field.
read from a stream.
the quote analyzer should overwrite any other forced analyzer in quoted parts of the query
validates that max_determinized_states can be parsed and lowers the allowed number of determinized states.
validates that max_determinized_states can be parsed and lowers the allowed number of determinized states.
test exception on missing `end` and `match` parameter in parser
a query that matches documents containing terms with a specified prefix.
read from a stream.
test (de)serialization on all previous released versions
test exceptions for three types of broken json, missing include exclude and both dist and prepost specified
test correct parsing of `dist` parameter, this should create builder with prepost set to same value
read from a stream.
add a span clause to the current list of clauses
read from a stream.
test that if we serialize and deserialize an object, further serialization leads to identical bytes representation. this is necessary to ensure because we use the serialized bytesreference of this builder as part of the cachekey in
implements the wildcard search query. supported wildcards are , which matches any character sequence (including the empty one), and ?, which matches any single character. note this query can be slow, as it needs to iterate over many terms. in order to prevent extremely slow wildcardqueries, a wildcard term should not start with one of the wildcards or
read from a stream.
read from a stream.
constructs a new base term query. in case value is assigned to a string, we internally convert it to a bytesref because in termquerybuilder and spantermquerybuilder string values are parsed to bytesref and we want internal representation of query to be equal regardless of whether it was created from xcontent or via java api.
read from a stream.
add a sub-query to this disjunction.
this helper method checks if the object passed in is a bytesref or charbuffer, if so it converts it to a utf8 string.
this helper method checks if the object passed in is a string or charbuffer, if so it converts it to a bytesref.
parses a query excluding the query element that wraps it
helper method to convert collection of querybuilder instances to lucene their querybuilder#toquery(queryshardcontext) method are not added to the resulting collection.
read from a stream.
read from a stream.
sets the distance from the center for this query.
sets the center point for this query.
construct new geodistancequerybuilder.
sets the center point for the query.
which type of geo distance calculation method to use.
read from a stream.
same as #convert(list) but on an iterable.
constructor used internally for serialization of both value termslookup variants
convert the internal list of values back to a user-friendly list. integers are kept as-is since the terms query does not make any difference between integers and longs, but bytesrefs are converted back to strings.
a filter for a field based on several terms matching on any of them.
convert the list in a way that optimizes storage in the case that all elements are either integers or strings bytesref use-cases that involve sending very large terms queries to elasticsearch. if the list does not only contain integers or strings, then a list is returned where all string charbuffers have been replaced with bytesrefs.
read from a stream.
sets the operator to use when using a boolean query. defaults to or.
constructs a new match query.
sets query to use in case no query terms are available, e.g. after analysis removed them. defaults to matchquery.zerotermsquery#none, but can be set to
sets the length of a length of common (non-fuzzy) prefix for fuzzy match queries
when using fuzzy or prefix type query, the number of term expansions to use.
a query that wraps another query and simply returns a constant score equal to the query boost for every document in the query.
read from a stream.
sets whether the query builder should ignore unmapped paths (and run a the path is unmapped.
query that matches spans queries defined in matchbuilder whose end position is less than or equal to end.
read from a stream.
in case of date field, we can adjust the fromto fields using a timezone
read from a stream.
in case of format field, we can parse the fromto fields using this time format
a query that matches documents within an range of terms.
test checks that we throw an unsupportedoperationexception if the query wrapped by spanmultitermquerybuilder does not generate a lucene multitermquery. this is currently the case for rangequerybuilder when the target field is mapped to a date.
read from a stream.
constructs a new fuzzy query.
create a new boostingquerybuilder
set the negative boost factor.
read from a stream.
adds ids to the query.
read from a stream.
defines which spatial strategy will be used for building the geo shape query. when not set, the strategy that will be used will be the one that is associated with the geo shape field in the mappings. the spatial strategy to use for building the geo shape query
sets the relation of query shape and indexed shape.
fetches the shape with the given id in the given type and index. getrequest containing index, type and id name or path of the field in the shape document where the shape itself is located
read from a stream.
creates a random function score query using only constructor params. the caller is responsible for randomizing fields set outside of the constructor.
create a random decay function setting all of its constructor parameters randomly. the caller is responsible for randomizing other fields.
called on a data node, converts this scorefunctionbuilder into its corresponding lucene function object.
parses bodies of the kind   "fieldname1" : "origin" : "somevalue", "scale" : "somevalue" , "multi_value_mode" : "min"  
read from a stream.
convenience constructor that converts its parameters into json to parse on the data nodes.
creates a function_score query that executes the provided filters and functions on documents that match a query.
score mode defines how results of individual score functions will be aggregated.
read from a stream.
boost mode defines how the combined result of score functions will influence the final score together with the sub query score.
read from a stream.
seed variant taking a string value.
read from a stream.
sets the new current nested level and pushes old current nested level down the stack returns that level.
this method deletes every file in this store that is not contained in the given source meta data or is a legacy checksum file. after the delete it pulls the latest metadata snapshot from the store and compares it to the given snapshot. if the snapshots are inconsistent an illegal state exception is thrown.
computes a strong hash value for small files. note that this method should only be used for files < 1mb
returns true iff the given location contains an index an the index can be successfully opened. this includes reading the segment infos and possible corruption markers.
tries to open an index for the given location. this includes reading the segment infos and possible corruption markers. if the index can not be opened, an exception is thrown
checks and returns the status of the existing index in this store.
deletes all corruption markers from this store.
returns a new metadatasnapshot for the given commit. if the given commit is null the latest commit point is used. note that this method requires the caller verify it has the right to access the store and no concurrent file changes are happening. if in doubt, you probably want to use one of the following: directory only be used if there is no started shard using this store. unexpected exception when opening the index reading the segments file.
repairs the index using the previous returned status from #checkindex(printstream).
keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe at the recovering time but they can suddenly become safe in the future. the following issues can happen if unsafe commits are kept oninit.  1. replica can use unsafe commit in peer-recovery. this happens when a replica with a safe commit c1(max_seqno=1) and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). if a new document(seqno=2) is added without flushing, the global checkpoint is advanced to 2; and the replica recovers again, it will use the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica.  2. min translog gen for recovery can go backwards in peer-recovery. this happens when are replica with a safe commit c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2). the replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. flushing a new commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. the recovery translog generation of a commit is calculated based on the current local checkpoint. the local checkpoint of c3 is 1 while the local checkpoint of c2 is 2.  3. commit without translog can be used in recovery. an old index, which was created before multiple-commits is introduced (v6.2), may not have a safe commit. if that index has a snapshotted commit without translog and an unsafe commit, the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.
marks this store as corrupted. this method writes a corrupted_$uuid file containing the given exception message. if a store contains a corrupted_$uuid file #ismarkedcorrupted() will return true.
reads a metadatasnapshot from the given index locations or returns an empty snapshot if it can't be read.
force bakes the given translog generation as recovery information in the lucene index. this is used when recovering from a snapshot or peer file based recovery where a new empty translog is created and the existing lucene index needs should be changed to use it.
returns the segments info for the given commit or for the latest commit if the given commit is null
marks an existing lucene index with a new history uuid. this is used to make sure no existing shard will recovery from this index using ops based recovery.
checks that the lucene index contains a history uuid marker. if not, a new one is generated and committed.
renames all the given files from the key of the map to the value of the map. all successfully renamed files are removed from the map in-place.
the returned indexoutput validates the files checksum.  note: checksums are calculated by default since version 4.8.0. this method only adds the verification against the checksum in the given metadata and does not add any significant overhead.
returns a diff between the two snapshots that can be used for recovery. the given snapshot is treated as the recovery target and this snapshot as the source. the returned diff will hold a list of files that are:  identical: they exist in both snapshots and they can be considered the same ie. they don't need to be recovered different: they exist in both snapshots but their they are not identical missing: files that exist in the source but not in the target  this method groups file into per-segment files and per-commit files. a file is treated as identical if and on if all files in it's group are identical. on a per-segment level files for a segment are treated as identical iff:  all files in this segment have the same checksum all files in this segment have the same length the segments .si files hashes are byte-identical note: this is a using a perfect hash function, the metadata transfers the .si file content as it's hash   the .si file contains a lot of diagnostics including a timestamp etc. in the future there might be unique segment identifiers in there hardening this method further.  the per-commit files handles very similar. a commit is composed of the segments_n files as well as generational files like deletes ( _x_y.del) or field-info ( _x_y.fnm) files. on a per-commit level files for a commit are treated as identical iff:  all files belonging to this commit have the same checksum all files belonging to this commit have the same length the segments file segments_n files hashes are byte-identical note: this is a using a perfect hash function, the metadata transfers the segments_n file content as it's hash   note: this diff will not contain the segments.gen file. this file is omitted on recovery.
creates an empty lucene index and a corresponding empty translog. any existing data will be deleted.
returns the segments file that this metadata snapshot represents or null if the snapshot is empty.
read from a stream.
return the cumulative size of all files in this directory.
read from a stream.
returns true iff the length and the checksums are the same. otherwise false
tests retry mechanism when indexing. if an exception occurs when indexing then the indexing request is tried again before finally failing. if auto generated ids are used this must not lead to duplicate ids see https:github.comelasticelasticsearchissues8788
tests corruption that happens on a single shard when no replicas are present. we make sure that the primary stays unassigned and all other replicas for the healthy shards happens
this test triggers a corrupt index exception during finalization size if an empty commit point is transferred during recovery we don't know the version of the segments_n file because it has no segments we can take it from. this simulates recoveries from old indices or even without checksums and makes sure if we fail during finalization we also check if the primary is ok. without the relevant checks this test fails with a red cluster
tests corruption that happens on the network layer and that the primary does not get affected by corruption that happens on the way to the replica. the file on disk stays uncorrupted
tests that restoring of a corrupted shard fails and we get a partial snapshot. todo once checksum verification on snapshotting is implemented this test needs to be fixed or split into several parts... we should also corrupt files on the actual snapshot and check that we don't restore the corrupted shard.
prunes the list of index files such that only the latest del generation files are contained.
this test verifies that if we corrupt a replica, we can still get to green, even though listing its store fails. note, we need to make sure that replicas are allocated on all data nodes, so that replica won't be sneaky and allocated on a node that doesn't have a corrupted replica.
tests that we can actually recover from a corruption on the primary given that we have replica shards around.
test that accept_z_value parameter correctly parses
test for geoshapefieldtype#setstrategyname(string) that checks that geoshapefieldtype#pointsonly() gets set as a side effect when using spatialstrategy.term
`index_options` was deprecated and is rejected as of 7.0
`index_options` was deprecated and is rejected as of 7.0
checks that the new field type is valid.
checks that the new field alias is valid. note that this method assumes that new concrete fields have already been processed, so that it can verify that an alias refers to an existing concrete field.
returns a list of the full names of a simple match regex like pattern against full name and index name.
return a new instance that contains the union of this instance and the field types from the provided mappers. if a field already exists, its field type will be updated to use the new type from the given field mapper. similarly if an alias already exists, it will be updated to reference the field type from the new mapper.
return a new context that will be used within a nested document.
return a new context that will have the provided path.
returns an array of values of the field specified as the method parameter. this method returns an empty array when there are no matching fields. it never returns null. if you want the actual numeric field instances back, use #getfields.
try to parse an externalvalue if any
return a new context that will be within multi-fields.
return a new context that will have the external value set.
return a new context that has the provided document as the current document.
return a new context that will be within a copy-to operation.
add fields so that they can later be fetched using #getbykey(object).
removes redundant root includes in objectmapper.nested trees to avoid duplicate fields on the root mapper when isincludeinroot is true for a node that is itself included into a parent node, for which either isincludeinroot is root
find a template. returns null if no template could be found.
returns the best nested objectmapper instances that is in the scope of the specified nested docid.
recursively update sub field types.
same as nestedobjectmappertests#testmultiplelevelsincluderoot1() but tests for the case where the transitive include_in_parent and redundant include_in_root happen on a chain of nodes that starts from a parent node that is not directly connected to root by a chain of include_in_parent, i.e. that has include_in_parent set to
checks that multiple levels of nested includes where a node is both directly and transitively included in root by include_in_root and a chain of include_in_parent does not lead to duplicate fields on the root document.
parses and indexes inputs parsing: acceptable format: "string" - interpreted as field value (input) "array" - each element can be one of "object" (see below) "object" - "input": string|array, "weight": string|int, "contexts": array|object indexing: if context mappings are defined, delegates to contextmappings#addfield(parsecontext.document, string, string, int, map) else adds inputs as a org.apache.lucene.search.suggest.document.suggestfield
completion prefix fuzzy query
note: prefixes longer than this will be truncated
acceptable inputs: "string" - interpreted as the field value (input) "object" - "input": string|array, "weight": string|int, "contexts": array|object
parses geopoint represented as a string and ignores malformed geopoints if needed
parses geopoint represented as an object or an array, ignores malformed geopoints if needed
build an update for the parent which will contain the given mapper and any intermediate fields.
creates an copy of the current field with given field name and boost
creates instances of the fields that the current field should be copied to
creates a mapping containing any dynamically added fields, or returns null if there were no dynamic mappings.
creates an update for intermediate object mappers that are not on the stack, but parents of newmapper.
removes mappers that exist on the stack, but are not part of the path of the current nameparts, returns the next unprocessed index from nameparts.
adds mappers from the end of the stack that exist as updates within those mappers. returns the next unprocessed index from nameparts.
adds a mapper as an update into the last mapper. if merge is true, the new mapper will be merged in with other child mappers of the last parent, otherwise it will be a new update.
factory method for range queries.
checks for any conflicts between this field type and other. if strict is true, all properties must be equal. otherwise, only properties which must never change in an index are checked.
checks this type is the same type as other. adds a conflict if they are different.
extract a term from a query created with #termquery by recursively removing boostquery wrappers.
sets the null value and initializes the string version
build a constant-scoring query that matches all values. the default implementation uses a are generated with #termquery.
an illegalargumentexception is needed in order to return an http error 400 when this error occurs in a request. see: org.elasticsearch.exceptionshelper#status
return a docvalueformat that can be used to display and parse values as returned by the fielddata api. the default implementation returns a docvalueformat#raw.
constructs a json path with an offset. the offset will result an offset number of path elements to not be included in #pathastext(string).
recursively update sub field types.
returns all the fields that match the given pattern. if the pattern is prefixed with a type then the fields will be returned with a type prefix.
returns the document mapper created, including a mapping update if the type has been dynamically created.
return the documentmapper for the given type. by using the special @value #default_mapping type, you can get a documentmapper for the default mapping.
return a term that uniquely identifies the document, or null if the type is not allowed.
update mapping by only merging the metadata that is different between received and stored entries
given a type (eg. long, string, ...), return an anonymous field mapper that can be used for search operations.
test that time zones are correctly parsed by the datefieldmapper. there is a known bug with joda 2.9.4 reported in https:github.comjodaorgjoda-timeissues373.
parse text field attributes. in addition to #parsefield common attributes this will parse analysis and term-vectors related settings.
parse common field attributes such as doc_values or store.
it is possible to search by token count.
count position increments in a token stream. package private for testing.
merge changes coming from mergewith in place.
parse using the provided parsecontext and return a mapping update if dynamic mappings modified the mappings, or null if mappings were not modified.
splits the provided mapper and its descendants into object, field, and field alias mappers.
this test checks that the multi-type validation is done before we do any other kind of validation on the mapping that's added, see https:github.comelasticelasticsearchissues29313
test that coerce parameter correctly parses
test that accept_z_value parameter correctly parses
test that ignore_malformed parameter correctly parses
test that orientation parameter correctly parses
with base64 ids, we decode and prepend an escape char in the cases that it could be mixed up with numeric or utf8 encoding. in the majority of cases (253256) the encoded id is exactly the binary form.
encode an id for storage in the index. this encoding is optimized for numeric and base64 ids, which are encoded in a much denser way than what utf8 would do.
with numeric ids, we just fold two consecutive chars in a single byte and use 0x0f as an end marker.
decode an indexed id back to its original form.
returns the parent objectmapper instance of the specified object mapper or null if there isn't any.
returns whether all parent objects fields are nested too.
build a mapping update with the provided sub mapping update.
removes this uid from the pending deletes map.
adds this uidversion to the pending adds map iff the map needs safe access.
try to prune tombstones whose timestamp is less than maxtimestamptoprune and seqno at most the maxseqnotoprune.
called when this index is closed.
exposes a translog associated with the given engine for testing purpose.
generate a new sequence number and return it. only works on internalengines
global stats on segments.
fail engine due to some error. the engine will also be closed. the underlying store is marked corrupted iff failure is caused by index corruption
flush the engine (committing segments to disk and truncating the translog) and close it.
returns whether a leaf reader comes from a merge (versus flush or addindexes).
deactivate throttling, which switches the lock to be an always-acquirable nooplock
returns 0 in the case where accountable is null, otherwise returns rambytesused()
creates a new org.elasticsearch.index.engine.engineconfig
commits the specified index writer.
reads the current stored history id from the iw commit data.
gets the commit data from indexwriter as a map.
resolves the current version of the document, returning null if not found
returns true if the indexing operation may have already be processed by this engine. note that it is ok to rarely return true even if this is not the case. however a `false` return value must always be correct.
closes the engine without acquiring the write lock. this should only be called while the write lock is hold or in a disaster condition ie. if the engine is failed.
reads the current stored translog id from the last commit data.
asserts that the doc in the index operation really doesn't exist
captures the most recent commit point #lastcommit or the most recent safe commit point #safecommit. index files of the capturing commit point won't be released until the commit reference is closed.
find the highest index position of a safe index commit whose max sequence number is not greater than the global checkpoint. index commits with different translog uuid will be filtered out as they don't belong to this engine.
checks if the deletion policy can release some index commits with the latest global checkpoint.
releases an index commit that acquired by #acquireindexcommit(boolean).
find a safe commit point from a list of existing commits based on the supplied global checkpoint. the max sequence number of a safe commit point should be at most the global checkpoint. if an index was created before v6.2, and we haven't retained a safe commit yet, this method will return the oldest commit.
end the recovery counter by decrementing the store's ref and the ongoing recovery counter
return a tuple representing the sequence id for the given get operation. the first value in the tuple is the sequence number, the second is the primary term.
tests that when the close method returns the engine is actually guaranteed to have cleaned up and that resources are closed
java docs
simulates what an upsert update api does
a sequence number generator that will generate a sequence number and if stall is set to true will wait on the barrier and the referenced latch before returning. if the local checkpoint should advance (because stall is false, then the value of number
random test that throws random exception and ensures all references are counted down released and resources are closed.
tests that age trumps size but recovery trumps both.
releases a generation that was acquired by #acquiretransloggen(long)
sets the translog generation of the last index commit.
acquires the basis generation for a new snapshot. any translog generation above, and including, the returned generation will not be deleted until the returned releasable is closed.
reads an operation at the given position into the given buffer.
create a snapshot of translog file channel.
ensures that the given location has be synced written to the underlying storage.
return stats
returns the minimum file generation referenced by the translog
creates a new translog instance. this method will create a new transaction log unless the given transloggeneration is the generation is not null, this method tries to open the given translog generation. the generation is treated as the last generation referenced from already committed data. this means all operations that have not yet been committed should be in the translog file referenced by this generation. the translog creation will fail if this generation can't be opened. deleted examined and stored in the header whenever a new generation is rolled. it's guaranteed from outside that a new generation is rolled when the term is increased. this guarantee allows to us to validate and reject operation whose term is higher than the primary term stored in the translog header.
trims translog for terms of files below belowterm and seq# above aboveseqno. effectively it moves max visible seq# checkpoint#trimmedaboveseqno therefore translogsnapshot skips those operations.
acquires a lock on the translog files, preventing them from being trimmed
returns the number of operations in the transaction files that contain operations with seq# above the given number.
writes the type and translog operation to the given stream
snapshots the current transaction log allowing to safely iterate over the snapshot. snapshots are fixed in time and will not be updated with future operations.
trims unreferenced translog generations by asking translogdeletionpolicy for the minimum required generation
returns the number of operations in the translog files at least the given generation
reads the type and the operation from the given stream. the operation must be written with
creates a new translog for the specified generation.
returns true if an fsync is required to ensure durability of the translogs operations or it's metadata.
the a location that will sort after the location returned by the last write but before any locations which can be returned by the next write.
returns the size in bytes of the translog files at least the given generation
recover all translog files found on disk
reads a list of operations written with #writeoperations(streamoutput, list)
closes the translog if the current translog writer experienced a tragic exception. note that in case this thread closes the translog it must not already be holding a read lock on the translog as it will acquire a write lock in the course of closing the translog
sync's the translog.
roll the current translog generation into a new generation. this does not commit the translog.
reads and returns the operation from the given location if the generation it references is still available. otherwise this method will return null.
ensures that all locations in the given stream have been synced written to the underlying storage. this method allows for internal optimization to minimize the amount of fsync operations if multiple locations must be synced.
adds an operation to the transaction log.
writes all operations in the given iterable to the given output stream including the size of the array use #readoperations(streaminput, string) to read it back.
returns the generation of the current transaction log.
extracts the translog generation from a file name.
returns true iff the given generation is the current generation of this translog
the last synced checkpoint for this translog.
gets the minimum generation that could contain any sequence number after the specified sequence number, or the current generation if there is no generation that could any such sequence number.
returns the current generation of this translog. this corresponds to the latest uncommitted translog generation
creates a new writer needed to solve and initialization problem while constructing an empty translog. with no readers and no current, a call to #getminfilegeneration() would not work.
sets the tragic exception or if the tragic exception is already set adds passed exception as suppressed exception
read the size of the op (i.e., number of bytes, including the op size) written at the given position
reads an operation at the given position and returns it. the buffer length is equal to the number of bytes reads.
given a file channel, opens a translogreader, taking care of checking and validating the file header.
reads an operation at the given position into the given buffer.
closes current reader and creates new one with new checkoint and same file channel
disables translog flushing for the specified index
creates a new point in time snapshot of the given snapshots. those snapshots are always iterated in-order.
marks this sequence number and returns true if it is seen before.
lists all existing commits in a given index path, then read the minimum translog generation that will be used in recoverfromtranslog.
corrupts random translog file (translog-n.tlog) from the given translog directory.
make sure that it's ok to close a translog snapshot multiple times
tests that closing views after the translog is fine and we can reopen the translog
tests that concurrent readers and writes maintain view and snapshot semantics
this test adds operations to the translog which might randomly throw an ioexception. the only thing this test verifies is that we can, after we hit an exception, open and recover the translog successfully and retrieve all successfully synced operations from the transaction log.
tests the situation where the node crashes after a translog gen was committed to lucene, but before the translog had the chance to clean up its files.
tests the situation where the node crashes after a translog gen was committed to lucene, but before the translog had the chance to clean up its files.
writes this header with the latest format into the file channel
read a translog header from the given path and file channel
syncs the translog up to at least the given offset unless already synced
returns true if there are buffered operations that have not been flushed and fsynced to disk or if the latest global checkpoint has not yet been fsynced
closes this writer and transfers its underlying file channel to a new immutable translogreader
add the given bytes to the translog with the specified sequence number; returns the location the bytes were written to.
show a warning about deleting files, asking for a confirmation if batchmode is false
return a set of all files in a given directory
write a translog containing the given translog uuid to the given location. returns the number of bytes written.
promotes the specific replica as the new primary
test request failures (failures before seq_no generation) are not added as a noop to translog
this test ensures the consistency between primary and replica when non-append-only (eg. index request with id or delete) operation of the same document is processed before the original append-only request on replicas. the append-only document can be exposed and deleted on the primary before it is added to replica. replicas should treat a late append-only request as a regular index request.
this test ensures the consistency between primary and replica with late and out of order delivery on the replica. an index operation on the primary is followed by a delete operation. the delete operation is delivered first and processed on the replica but the index is delayed with an interval that is even longer the gc deletes cycle. this makes sure that that replica still remembers the delete operation and correctly ignores the stale index operation.
test document failures (failures after seq_no generation) are added as noop operation to the translog for primary and replica shards
metric implementing precision@k. ratings equal or above this value will be considered relevant. controls how unlabeled documents in the search hits are treated. set to 'true', unlabeled documents are ignored and neither count as true or false positives. set to 'false', they are treated as false positives. controls the window size for the search results the metric takes into account
compute precisionatn based on provided relevant document ids.
joins hits with rated documents using the joint _index_id document key.
test that modifying the order of indexdocid to make sure it doesn't matter for parsing xcontent
assuming the result ranking is  rank | relevance | probr r | p | p probr r ------------------------------------------------------- 1 | 3 | 0.875 | 1 | 0.875 | 2 | na | na | 0.125 | na | 3 | 0 | 0 | 0.125 | 0 | 4 | 1 | 0.03125 | 0.125 | 0.00390625 |  err = sum of last column
assuming the result ranking is  rank | relevance | probr r | p | p probr r ------------------------------------------------------- 1 | 3 | 0.875 | 1 | 0.875 | 2 | 2 | 0.1875 | 0.125 | 0.0234375 | 3 | 0 | 0 | 0.078125 | 0 | 4 | 1 | 0.03125 | 0.078125 | 0.00244140625 |  err = sum of last column
test that the relevant rating threshold can be set to something larger than 1. e.g. we set it to 2 here and expect dics 0-2 to be not relevant, doc 3 and 4 to be relevant
test cases retrieves all six documents indexed above. the first part checks the prec@10 calculation where all unlabeled docs are treated as "unrelevant". we average prec@ metric across two search use cases, the first one that labels 4 out of the 6 documents as relevant, the second one with only one relevant document.
test that running a bad query (e.g. one that will target a non existing field) will produce an error in the response
this test assumes we are using the same ratings as in discountedcumulativegaintests#testdcgat(). see details in that test case for how the expected values are calculated
test that multiple indices work, setting indices options is possible and works as expected
test that the relevant rating threshold can be set to something larger than 1. e.g. we set it to 2 here and expect dics 0-2 to be not relevant, so first relevant doc has third ranking position, so rr should be 13
create searchhits for testing, starting from dociid 'from' up to docid 'to'. the search hits index also need to be provided
assuming the docs are ranked in the following order: rank | relevance | 2^(relevance) - 1 | log_2(rank + 1) | (2^(relevance) - 1) log_2(rank + 1) ------------------------------------------------------------------------------------------- 1 | 3 | 7.0 | 1.0 | 7.0 | 7.0 | 2 | 2 | 3.0 | 1.5849625007211563 | 1.8927892607143721 3 | 3 | 7.0 | 2.0 | 3.5 4 | 0 | 0.0 | 2.321928094887362 | 0.0 5 | 1 | 1.0 | 2.584962500721156 | 0.38685280723454163 6 | 2 | 3.0 | 2.807354922057604 | 1.0686215613240666 dcg = 13.84826362927298 (sum of last column)
this tests metric when some documents in the search result don't have a rating provided by the user. rank | relevance | 2^(relevance) - 1 | log_2(rank + 1) | (2^(relevance) - 1) log_2(rank + 1) ------------------------------------------------------------------------------------------- 1 | 3 | 7.0 | 1.0 | 7.0 2 | 2 | 3.0 | 1.5849625007211563 | 1.8927892607143721 3 | 3 | 7.0 | 2.0 | 3.5 4 | na | na | na | na 5 | 1 | 1.0 | 2.584962500721156 | 0.38685280723454163 6 | na | na | na | na dcg = 12.779642067948913 (sum of last column)
this tests that normalization works as expected when there are more rated documents than search hits because we restrict dcg to be calculated at the fourth position rank | relevance | 2^(relevance) - 1 | log_2(rank + 1) | (2^(relevance) - 1) log_2(rank + 1) ------------------------------------------------------------------------------------------- 1 | 3 | 7.0 | 1.0 | 7.0 2 | 2 | 3.0 | 1.5849625007211563 | 1.8927892607143721 3 | 3 | 7.0 | 2.0 | 3.5 4 | na | na | na | na ----------------------------------------------------------------- 5 | 1 | 1.0 | 2.584962500721156 | 0.38685280723454163 6 | na | na | na | na dcg = 12.392789260714371 (sum of last column until position 4)
test that metric returns 0.0 when there are no search results
compute reciprocalrank based on provided relevant document ids.
metric implementing mean reciprocal rank (https:en.wikipedia.orgwikimean_reciprocal_rank).
run a search action and call onresponse when a the response comes in, retrying if the action fails with an exception caused by rejected execution.
we can't send parsesearchrequest rest content that it doesn't support so we will have to remove the content that is valid in addition to what it supports from the content first. this is a temporary hack and should get better when searchrequest has full objectparser support then we can delegate and stuff.
read from a stream.
takes an action created by a bulkbyscrolltask and runs it with regard to whether the request is sliced or not. if the request is not sliced (i.e. the number of slices is 1), the worker action in the given runnable will be started on the local node. if the request is sliced (i.e. the number of slices is more than 1), then a subrequest will be created for each slice and sent. if slices are set as "auto", this class will resolve that to a specific number based on characteristics of the source indices. a request with "auto" slices may end up being sliced or unsliced.
slice a search request into times separate search requests slicing on field. note that the slices are shallow copies of this request so don't change them.
make sure that search failures get pushed back to the user as failures of the whole process. we do lose some information about how far along the process got, but its important that they see these failures.
build a task status with only some values. used for testing negative values.
blocks the named executor by getting its only thread running a task blocked on a cyclicbarrier and fills the queue with a noop task. so requests to use this queue should get esrejectedexecutionexceptions.
fetch the status for a task of type "action". fails if there aren't exactly one of that type of task running.
executes the cancellation test
read from a stream.
record a failure from a slice and respond to the listener if the request is finished.
get the combined statuses of slice subtasks, merged with the given list of statuses
record a response from a slice and respond to the listener if the request is finished.
returns the object that tracks the state of sliced subtasks. throws illegalstateexception if this task is not set to be a leader task.
read from a stream.
sets this task to be a leader task for slices sliced subtasks
returns the object that manages sending search requests. throws illegalstateexception if this task is not set to be a worker task.
build the status for this task given a snapshot of the information of running slices. this is only supported if the task is set as a leader for slice subtasks
sets this task to be a worker task that performs search requests
start terminating a request that finished non-catastrophically by refreshing the modified indices and then proceeding to
send a bulk request, handling retries.
start the action by firing the initial search request.
finish the request.
start the next scroll request. when the scroll returns
used to accept or ignore a search hit. ignored search hits will be excluded from the bulk request. it is also where we fail on invalid search hits, like when the document has no source but it's required.
process a scroll response.
processes bulk responses, accounting for failures.
prepare the bulk request. called on the generic thread pool after some preflight checks have been done one the searchresponse and any delay has been slept. uses the generic thread pool because reindex is rare enough not to need its own thread pool and because the thread may be blocked by the user script.
assert that two task statuses are equal after serialization.
maximum number of processed documents. defaults to -1 meaning process all documents.
append a short description of the search request to a stringbuilder. used to make tostring.
set the throttle for this request in sub-requests per second. float#positive_infinity means set no throttle and that is the default. throttling is done between batches, as we start the next scroll requests. that way we can increase the scroll's timeout to make sure that it contains any time that we might wait.
the number of slices this task should be divided into. defaults to 1 meaning the task isn't sliced into subtasks.
sets abortonversionconflict based on rest-friendly names.
constructor for actual use. request slicing
setup a clone of this request with the information needed to process a slice of it.
set the throttle to apply to all matching requests in sub-requests per second. float#positive_infinity means set no throttle. throttling is done between batches, as we start the next scroll requests. that way we can increase the scroll's timeout to make sure that it contains any time that we might wait.
execute a bulk retry test case. the total number of failures is random and the number of retries attempted is set to testrequest.getmaxretries and controled by the failwithrejection parameter.
mimicks search timeouts.
mimicks bulk indexing failures.
tests that we can cancel the request during its throttling delay. this can't use #canceltaskcase(consumer) because it needs to send the request un-canceled and cancel it at a specific time.
mimicks bulk rejections. these should be retried and eventually succeed.
tests that each scroll response is a batch and that the batch is launched properly.
mimicks script failures or general wrongness by implementers.
mimicks a threadpool rejecting execution of the task.
mimicks shard search failures usually caused by the data node serving the scroll request going down.
simulate a scroll response by setting the scroll id and firing the onscrollresponse method.
the default retry time matches what we say it is in the javadoc for the request.
test rethrottling.
overrides the parent's implementation is much more updatereindex oriented and so also copies things like timestampttl which we don't care for a deletion.
schedule preparebulkrequestrunnable to run after some delay. this is where throttling plugs into reindexing so the request can be rescheduled over and over again.
apply newrequestspersecond as the new rate limit for this task's search requests
perform a reindex with create optype which has "create" semantics.
perform a reindex with internal versioning which has "overwrite" semantics.
perform a reindex with external versioning which has "refresh" semantics.
build the restclient used for reindexing from remote clusters.
override the simple copy behavior to allow more fine grained control.
build the characterrunautomaton that represents the reindex-from-remote whitelist and make sure that it doesn't whitelist the world.
throws an actionrequestvalidationexception if the request tries to index back into the same index or into an index that points to two indexes. this cannot be done during request validation because the cluster state isn't available then. package private for testing.
furiously rethrottles a delayed request to make sure that we never run it twice.
yank a string array from a map. emulates xcontent's permissive string to string array conversions.
build a remoteinfo, defaulting values that we don't care about in this test to values that don't hurt anything.
sets common options of abstractbulkbyscrollrequest requests.
test for parsing _ttl, _timestamp, _routing, and _parent.
creates a hit source that doesn't make the remote request and instead returns data from some files. also requests are always returned synchronously rather than asynchronously.
test for parsing _ttl, _routing, and _parent. _timestamp isn't available.
versions of elasticsearch before 2.1.0 don't support sort:_doc and instead need to use search_type=scan. scan doesn't return documents the first iteration but reindex doesn't like that. so we jump start strait to the next iteration.
wrap the responseexception in an exception that'll preserve its status code if possible so we can send it back to the user. we might not have a constant for the status code so in that case we just use 500 instead. we also extract make sure to include the response body in the message so the user can figure out why the remote elasticsearch service threw the error back to us.
returns a docvaluebits representing all documents from pointvalues that have a value.
return a string representation of the provided values. that is typically used for scripts or for the `map` execution mode of terms aggs. note: this is slow!
return a numericdoublevalues that doesn't contain any value.
returns a docvaluebits representing all documents from docvalues that have a value.
returns a docvaluebits representing all documents from values that have a value.
return a string representation of the provided values. that is typically used for scripts or for the `map` execution mode of terms aggs. note: this is very slow!
returns a docvaluebits representing all documents from doublevalues that have a value.
given a sortednumericdoublevalues, return a to sortable long bits using
given a sortednumericdocvalues, return a sortednumericdoublevalues instance that will translate long values to doubles using
wrap the provided sortednumericdocvalues instance to cast all values to doubles.
returns a docvaluebits representing all documents from docvalues that have a value.
return a numericdocvalues instance that has a value for every document, returns the same value as values if there is a value for the current document and missing otherwise.
return a string representation of the provided values. that is typically used for scripts or for the `map` execution mode of terms aggs. note: this is very slow!
wrap the provided sortednumericdoublevalues instance to cast all values to longs.
return a string representation of the provided values. that is typically used for scripts or for the `map` execution mode of terms aggs. note: this is very slow!
return a numericdoublevalues instance that has a value for every document, returns the same value as values if there is a value for the current document and missing otherwise.
sets a org.elasticsearch.index.fielddata.indexfielddatacache.listener passed to each indexfielddata creation to capture oncache and onremoval events. setting a listener on this method will override any previously set listeners.
set the #size() and ensure that the #values array can store at least that many entries.
set the #size() and ensure that the #values array can store at least that many entries.
log a deprecation log, with the server's permissions, not the permissions of the script calling this method. we need to do this to prevent errors when rolling the log file.
refresh the backing array. package private so it can be called when longs loads dates.
fetch the first field value or 0 millis after epoch if there are no in.
make sure the #values array can store at least #count entries.
return the missing object value according to the reduced type of the comparator.
get a docidset that matches the inner documents.
returns numeric docvalues view of raw double bits
returns numeric docvalues view of raw float bits
proxy to the original next() call, but estimates the overhead of loading the next term.
flush the flushbuffer to the breaker, incrementing the total bytes and resetting the buffer.
helper: checks a fieldinfo and throws exception if its definitely not a latlondocvaluesfield
adjust the circuit breaker now that terms have been loaded, getting the actual used either from the parameter (if estimation worked for the entire set), or from the termsenum if it has been wrapped in a ramaccountingtermsenum.
determine whether the blocktreetermsreader.fieldreader can be used for estimating the field data, adding the estimate to the circuit breaker if it can, otherwise wrapping the terms in a ramaccountingtermsenum to be estimated on a per-term basis.
builds a bitset where each documents bit is that that has one or more ordinals associated with it. if every document has an ordinal associated with it this method returns null
builds an ordinals instance from the builders current state.
allocate a new slice and return its id.
associates the given document id with the current ordinal.
this method iterates all terms in the given termsenum and associates each terms ordinal with the terms documents. the caller must exhaust the returned bytesrefiterator which returns all values where the first returned value is associated with the ordinal 1 etc.
returns a shared longsref instance for the given doc id holding all ordinals associated with it.
build global ordinals for the provided indexreader.
the source of the document (as a string).
returns bytes reference, also un compress the source if needed.
returns getresult based on the specified org.elasticsearch.index.engine.engine.getresult argument. this method basically loads specified fields for the associated document in the enginegetresult. this method load the fields from the lucene index and not from transaction log and therefore isn't realtime.  note: call must release engine searcher associated with enginegetresult!
recovers a shard from it's local file system store. this method required pre-knowledge about if the shard should exist on disk ie. has been previously allocated or if the shard is a brand new allocation without pre-existing index files transaction logs. this has been ignored due to a concurrent modification of if the clusters state has changed due to async updates.
restores shard from snapshotrecoverysource associated with this shard in routing table
recovers the state of the shard from the store.
recovers an index from a given repository. this method restores a previously created index snapshot into an existing initializing shard. has been ignored due to a concurrent modification of if the clusters state has changed due to async updates.
recovers the state of the shard from the store.
tries to extract the shard id from a reader if possible, when its not possible, will return null.
tries to extract the shard id from a reader if possible, when its not possible, will return null.
promotes a replica to primary, incrementing it's term and starting it if needed
creates a new initializing shard. the shard will have its own unique data path. (ready to recover from another shard)
creates a new initializing shard.
creates a new initializing shard. the shard will will be put in its proper path under the supplied node id. (ready to recover from another shard)
takes an existing shard, closes it and starts a new initialing shard at the same location
creates a new initializing shard. the shard will will be put in its proper path under the current node id the shard is assigned to.
recovers a replica from the given primary
recover a shard from a snapshot using a given repository
recovers a replica from the given primary
recovers a replica from the give primary, allow the user to supply a custom recovery target. a typical usage of a custom recovery target is to assert things in the various stages of recovery. note: this method keeps the shard in indexshardstate#post_recovery and doesn't start it.
snapshot a shard using a given repository
creates a new initializing shard. the shard will have its own unique data path.
get the local knowledge of the global checkpoints for all in-sync allocation ids.
notifies the service to update the local checkpoint for the shard with the provided allocation id. see details.
add a listener for refreshes. false otherwise.
updates the global checkpoint on a replica shard after it has been updated by the primary.
marks the shard as recovering based on a recovery state, fails with exception is recovering is not allowed to be set.
schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. the operation will be executed asynchronously on the flush thread pool.
tests whether or not the translog generation should be rolled to a new generation. this test is based on the size of the current generation compared to the configured generation threshold size.
creates a new indexcommit snapshot from the currently running engine. all resources referenced by this commit won't be freed until the commit snapshot is closed.
executes the given flush request against the engine.
updates the known allocation ids and the local checkpoints for the corresponding allocations from a primary relocation source.
checks and removes translog files that no longer need to be retained. see
upgrades the shard to the current version of lucene and returns the minimum segment version
gets a store.metadatasnapshot for the current directory. this method is safe to call in all lifecycle of the index shard, without having to worry about the current state of the engine and concurrent flushes. mismatch or an unexpected exception when opening the index reading the segments file.
acquire a replica operation permit whenever the shard is ready for indexing (see name. the tracing will capture the supplied object's object#tostring() value. otherwise the object isn't used
update the local knowledge of the global checkpoint for the specified allocation id.
registers the given listener and invokes it once the shard is active again and all pending refresh translog location has been refreshed. if there is no pending refresh location registered the listener will be invoked immediately. true if the listener was registered to wait for a refresh.
completes the relocation. operations are blocked and current operations are drained before changing state to relocated. the provided
called by indexingmemorycontroller to check whether more than inactivetimens has passed since the last indexing operation, and notify listeners that we are now inactive so e.g. sync'd flush can happen.
perform the last stages of recovery once all translog operations are done. note that you should still call #postrecovery(string).
syncs the global checkpoint to the replicas if the global checkpoint on at least one replica is behind the global checkpoint on the primary.
called if recovery has to be restarted after network error delay
called when our shard is using too much heap and should move buffered indexeddeleted documents to disk.
called before starting to copy index files over
returns how many bytes we are currently moving from heap to disk
tests whether or not the engine should be flushed periodically. this test is based on the current size of the translog compared to the configured flush threshold size.
snapshots the most recent safe index commit from the currently running engine. all index files referenced by this index commit won't be freed until the commitsnapshot is closed.
returns number of heap bytes used by the indexing buffer for this shard, or 0 if the shard is closed
build refreshlisteners for this shard.
returns the current replication group for the shard.
acquire a primary operation permit whenever the shard is ready for indexing. if a permit is directly available, the provided actionlistener will be called on the calling thread. during relocation hand-off, permit acquisition can be delayed. the provided actionlistener will then be called using the provided executor. the tracing will capture the supplied object's object#tostring() value. otherwise the object isn't used
executes a scheduled refresh if necessary.
returns an operation that acquires a permit and synchronizes in the following manner:  waits on the barrier before acquiring a permit counts down the operationexecutinglatch when it acquires the permit waits on the operationlatch before releasing the permit counts down the operationcompletelatch after releasing the permit 
tests that the threadcontext is restored when a operation is executed after it has been delayed due to a block
test one can snapshot the store at various lifecycle stages
simulates a scenario that happens when we are async fetching snapshot metadata from gatewayservice and checking index concurrently. this should always be possible without any exception.
this test makes sure that people can use the shard routing entry + take an operation permit to check whether a shard was already promoted to a primary.
here we are simulating the scenario that happens when we do async shard fetching from gatewayserivce while we are finishing a recovery and concurrently clean files. this should always be possible without any exception. yet there was a bug where indexshard acquired the index writer lock before it called into the store that has it's own locking for metadata reads
index on the specified shard while introducing sequence number gaps.
check that the accounting breaker correctly matches the segments api for memory usage
attempts to add a listener at the same time as a refresh occurs by having a background thread force a refresh as fast as it can while adding listeners. this can catch the situation where a refresh happens right as the listener is being added such that the listener misses the refresh and has to catch the next one. if the listener wasn't able to properly catch the next one then this would fail.
uses a bunch of threads to index, wait for refresh, and non-realtime get documents to validate that they are visible after waiting regardless of what crazy sequence of events causes the refresh listener to fire.
the number of pending listeners.
fire some listeners. does nothing if the list of listeners is null.
add a listener for refreshes, calling it immediately if the location is already visible. if this runs out of listener slots then it forces a refresh and calls the listener immediately as well. false otherwise.
add a global checkpoint listener. if the global checkpoint is above the current global checkpoint known to the listener then the listener will be asynchronously notified on the executor used to construct this collection of global checkpoint listeners. if the shard is closed then the listener will be asynchronously notified on the executor used to construct this collection of global checkpoint listeners. the listener will only be notified of at most one event, either the global checkpoint is updated or the shard is closed. a listener must re-register after one of these events to receive subsequent events.
parse the string representation of this shardid back to an object. we lose index uuid information here, but since we use tostring in rest responses, this is the best we can do to reconstruct the object on the client side.
returns the stats, including type specific stats. if the types are null0 length, then nothing is returned for them. if they are set, then only types provided will be returned, or
this method tries to delete left-over shards where the index name has been reused but the uuid is different to allow the new shard to be allocated.
this method walks through the nodes shard paths to find the data and state path for the given shard. if multiple directories with a valid shard state exist the one with the highest version will be used. note: this method resolves custom data locations for the shard.
construct operation permits for the specified shards.
obtain the active operation count, or zero if all permits are held (even if there are outstanding operations in flight).
immediately delays operations and on another thread waits for in-flight operations to finish and then executes onblocked under the guarantee that no new operations are started. delayed operations are run after onblocked has executed. after operations are delayed and the blocking is forked to another thread, returns to the caller. if a failure occurs while blocking operations or executing onblocked then the onfailure handler will be invoked.
when the permit was acquired plus a stack traces that was captured when the permit was request.
if there are configured indexsearcherwrapper instances, the indexsearcher of the provided engine searcher gets wrapped and a new engine.searcher instances is returned, otherwise the provided engine.searcher is returned. this is invoked each time a engine.searcher is requested to do an operation. (for example search)
returns a copy of the current indexshardsnapshotstatus. this method is intended to be used when a coherent state of indexshardsnapshotstatus is needed.
returns information about a physical file with the given name
constructs a new instance of file info
checks if a file in a store is the same file
serializes file info into json
parses shard snapshot metadata
constructs new shard snapshot metadata from snapshot metadata
parses json that represents file info
serializes shard snapshot metadata info into json
returns base file name from part name
writes index file for the shard in the following format.   "files": [ "name": "__3", "physical_name": "_0.si", "length": 310, "checksum": "1tpsg3p", "written_by": "5.1.0", "meta_hash": "p9dsfxnmdwnlb......" , "name": "__2", "physical_name": "segments_2", "length": 150, "checksum": "11qjpz6", "written_by": "5.1.0", "meta_hash": "p9dsfwhzzwdtz......." , "name": "__1", "physical_name": "_0.cfe", "length": 363, "checksum": "er9r9g", "written_by": "5.1.0" , "name": "__0", "physical_name": "_0.cfs", "length": 3354, "checksum": "491liz", "written_by": "5.1.0" , "name": "__4", "physical_name": "segments_3", "length": 150, "checksum": "134567", "written_by": "5.1.0", "meta_hash": "p9dsfwhzzwdtz......." ], "snapshots": "snapshot_1": "files": ["__0", "__1", "__2", "__3"] , "snapshot_2": "files": ["__0", "__1", "__2", "__4"]  
return the number of tokens that the current document has in the considered field.
compute the part of the score that does not depend on the current document using the init_script.
parses the given settings and creates the appropriate distribution
parses the given settings and creates the appropriate basicmodel
parses the given settings and creates the appropriate lambda
parses the given settings and creates the appropriate normalization
parses the given settings and creates the appropriate aftereffect
convert a list of field names encoded with optional boosts to a map that associates the field name and its boost.
get a fieldmapper associated with a field name or null.
resolves the provided pattern or field name from the queryshardcontext and return a map of the expanded fields with their original boost. if false, only #allowed_query_mapper_types are accepted and other field types are discarded from the query. the original name of the field is kept if adding the suffix to the field name does not point to a valid field in the mapping.
resolve all the field names and patterns present in the provided map with the the original name of the field is kept if adding the suffix to the field name does not point to a valid field in the mapping.
expand a phrasequery to multiple fields that share the same analyzer. returns a disjunctionmaxquery with a disjunction for each expanded field.
rethrow the runtime exception, unless the lenient flag has been set, returns matchnodocsquery
creates a new parser with custom flags used to enabledisable certain features.
analyze the given string using its analyzer, constructing either a of termquerys and prefixquerys
rebuild a phrase query with a slop value
returns true if the given query might match nested documents.
returns true if a query on the given field might match parent documents or documents that are nested under a different path.
returns true if a query on the given field might match nested documents.
returns true if the given query might match parent documents or documents that are nested under a different path.
checks if graph analysis should be enabled for the field depending on the provided analyzer
returns the stats, including group specific stats. if the groups are null0 length, then nothing is returned for them. if they are set, then only groups provided will be returned, or
reads the sequence number stats from the commit data (maximum sequence number and local checkpoint).
compute the minimum of the given current minimum sequence number and the specified sequence number, accounting for the fact that the current minimum sequence number could be sequencenumbers#no_ops_performed or must not be sequencenumbers#unassigned_seq_no.
compute the maximum of the given current maximum sequence number and the specified sequence number, accounting for the fact that the current maximum sequence number could be sequencenumbers#no_ops_performed or must not be sequencenumbers#unassigned_seq_no.
called when the recovery process for a shard has opened the engine on the target shard. ensures that the right data structures have been set up locally to track local checkpoint information for the shard and that the shard is added to the replication group.
computes the global checkpoint based on the given local checkpoints. in case where there are entries preventing the computation to happen (for example due to blocking), it returns the fallback value.
returns the global checkpoint for the shard.
get the local knowledge of the global checkpoints for all in-sync allocation ids.
initiates a relocation handoff and returns the corresponding primary context.
notifies the service to update the local checkpoint for the shard with the provided allocation id. if the checkpoint is lower than the currently known one, this is a no-op. if the allocation id is not tracked, it is ignored.
scans through the currently known local checkpoint and updates the global checkpoint accordingly.
marks the shard with the provided allocation id as in-sync with the primary shard. this method will block until the local checkpoint on the specified shard advances above the current global checkpoint.
initialize the global checkpoint service. the specified global checkpoint should be set to the last known global checkpoint, or
notifies the tracker of the current allocation ids in the cluster state.
marks a relocation handoff attempt as successful. moves the tracker into replica mode.
initializes the global checkpoint tracker in primary mode (see #primarymode. called on primary activation or promotion.
activates the global checkpoint tracker in primary mode (see #primarymode. called on primary relocation target during primary relocation handoff.
class invariant that should hold before and after every invocation of public methods on this class. as java lacks implication as a logical operator, many of the invariants are written under the form (!a || b), they should be read as (a implies b) however.
if we do not update the global checkpoint in replicationtracker#markallocationidasinsync(string, long) after adding the allocation id to the in-sync set and removing it from pending, the local checkpoint update that freed the thread waiting for the local checkpoint to advance could miss updating the global checkpoint in a race if the waiting thread did not add the allocation id to the in-sync set and remove it from the pending set before the local checkpoint updating thread executed the global checkpoint update. this test fails without an additional call to replicationtracker#updateglobalcheckpointonprimary() after removing the allocation id from the pending set in replicationtracker#markallocationidasinsync(string, long) (even if a call is added after notifying all waiters in replicationtracker#updatelocalcheckpoint(string, long)).
resets the checkpoint to the specified value.
moves the checkpoint to the last consecutively processed sequence number. this method assumes that the sequence number following the current checkpoint is processed.
marks the processing of the provided sequence number as completed as updates the checkpoint if possible.
initialize the local checkpoint service. the maxseqno should be set to the last sequence number assigned, or or sequencenumbers#no_ops_performed.
returns a registered charfilterfactory provider by indexsettings or a registered charfilterfactory provider by predefined name or null if the charfilter was not registered
returns a registered tokenizerfactory provider by indexsettings or a registered tokenizerfactory provider by predefined name or null if the tokenizer was not registered
returns a settings by groupname from indexsettings or a default settings
returns a registered analyzer provider by name or null if the analyzer was not registered
returns a registered tokenfilterfactory provider by indexsettings or a registered tokenfilterfactory provider by predefined name or null if the tokenfilter was not registered
create a pre-configured token filter that may vary based on the lucene version.
create a pre-configured token filter that may not vary at all.
create a pre-configured token filter that may not vary at all.
if the reader can not be instantiated.
fetches a list of words from the specified settings file. the list should either be available at the key specified by settingsprefix or in a file specified by settingsprefix + _path. if the word list cannot be found at either key.
create a pre-configured token filter that may vary based on the lucene version.
create a pre-configured char filter that may not vary at all, provide access to the elasticsearch verison
create a pre-configured tokenizer that may not vary at all.
create a pre-configured tokenizer that may vary based on the lucene version.
tests that camelcase filter names and snake_case filter names don't collide.
this constructor only exists to expose analyzers defined in prebuiltanalyzers as prebuiltanalyzerproviderfactory.
transcodes the remaining parts of the string. the method operates on a sliding window, looking at 4 characters at a time: [i-1, i, i+1, i+2]. position
retrieves the nysiis code for a given string object.
encodes an object using the nysiis algorithm. this method is provided in order to satisfy the requirements of the encoder interface, and will throw an encoderexception if the supplied object is not of type corresponds to the given string.
deletes index templates, support wildcard notation. if no template name is passed to this method all templates are removed.
deletes the given indices from the tests cluster. if no index name is passed to this method all indices are removed.
this method should be executed before each test to reset the cluster to its initial state.
deletes repositories, supports wildcard notation.
removes all templates, except the templates defined in the exclude
start indexing
stop all background threads
start indexing in the background using a given number of threads. indexing will be paused after numofdocs docs has been indexed.
sort versions that have backwards compatibility guarantees from those that don't. doesn't actually check whether or not the versions are released, instead it relies on gradle to have already checked this which it does in :core:verifyversions. so long as the rules here match up with the rules in gradle then this should produce sensible results. guarantees in v1 and versions without the guranteees in v2
returns a random version between minversion and maxversion (inclusive).
get the released version before version#current.
returns the released version before the version#current where the minor version is less than the currents minor version.
get the released version before version.
returns the first future compatible version
returns the maximum version that is compatible with the given version.
returns the first future incompatible version
test that adding an additional object within each object of the otherwise correct query always triggers some kind of parse exception. some specific objects do not cause any exception as they can hold arbitrary content; they can be declared by overriding #getobjectsholdingarbitrarycontent().
call this method to check a valid json string representing the query under test against it's generated json. note: by the time of this writing (nov 2015) all queries are taken from the query dsl reference docs mirroring examples there. here's how the queries were generated:   take a reference documentation example.  stick it into the createparseablequeryjson method of the respective query test.  manually check that what the querybuilder generates equals the input json ignoring default options.  put the manual checks into the assertqueryparsedfromjson method.  now copy the generated json including default options into createparseablequeryjson  by now the roundtrip check for the json should be happy. 
checks the result of querybuilder#toquery(queryshardcontext) given the original querybuilder and queryshardcontext. verifies that named queries and boost are properly handled and delegates to
test that wraps the randomly generated query into an array as follows: "query_name" : [] this causes unexpected situations in parser code that may not be handled properly.
generic test that creates new query from the test query and checks both for equality and asserts equality on the two queries.
this test ensures that queries that need to be rewritten have dedicated tests. these queries must override this method accordingly.
traverses the json tree of the valid query provided as argument and mutates it one or more times by adding one object within each object encountered. for instance given the following valid term query: "term" : "field" : "value" : "foo" the following two mutations will be generated, and an exception is expected when trying to parse them: "term" : "newfield" : "field" : "value" : "foo" "term" : "field" : "newfield" : "value" : "foo" every mutation is then added to the list of results with a boolean flag indicating if a parsing exception is expected or not for the mutation. some specific objects do not cause any exception as they can hold arbitrary content; they are passed using the arbitrarymarkers parameter.
create a random value for either abstractquerytestcase#boolean_field_name, abstractquerytestcase#int_field_name,
test creates the query from the querybuilder under test and delegates the assertions being made on the result to the implementing subclass.
test that unknown field trigger parsingexception. to find the right position in the root query, we add a marker as `queryname` which all query builders support. the added bogus field after that should trigger the exception. queries that allow arbitrary field names at this level need to override this test.
serialize the given query builder and asserts that both are equal
tests that version#minimumcompatibilityversion() and versionutils#allreleasedversions() agree with the list of wire and index compatible versions we build in gradle.
set the static default settings to null to prevent a memory leak. the test framework also checks for memory leaks and computes the size, this can cause issues when running with the security manager as it tries to do reflection into protected sun packages.
create a new search context.
create a new index on the singleton node with the provided index settings.
ensures the cluster has a green state via the cluster health api. this method will also wait for relocations. it is useful to ensure that all action on the cluster have finished and all shards that were currently relocating are now allocated and started.
create a new index on the singleton node with the provided index settings.
compare two maps.
compare two values.
compare two lists.
compute numbers.
return an instance on an unmapped field.
stops a random node in the cluster that applies to the given filter or non if the non of the nodes applies to the filter.
restarts a node and calls the callback during restart.
restarts all nodes in a rolling restart fashion ie. only restarts on node a time.
returns a random node that applies to the given predicate. the predicate can filter nodes based on the nodes settings. if all nodes are filtered out this method will return null
returns a node client to a given node.
stops any of the current nodes but not the master node.
rebuilds a new node object using the current node settings and starts it
ensures that at least n data nodes are present in the cluster. if more nodes than n are present this method will not stop any of the running nodes.
ensures that at most n are up and running. if less nodes that n are running this method will not start any additional nodes.
returns a set of nodes that have at least one shard of the given index.
returns a node client to random node but not the master. this method will fail if no non-master client is available.
returns a node client to a data node in the cluster. note: use this with care tests should not rely on a certain nodes client.
stops a random data node in the cluster. returns true if a node was found to stop, false otherwise.
updates the min master nodes setting in the current running cluster.
ensure a cluster is formed with all published nodes, but do so by using the client of the specified node
returns a client to a coordinating only node
returns a node client to the current master node. note: use this with care tests should not rely on a certain nodes client.
stops the current master node forcefully
builds a new node the method will return the existing one
starts multiple nodes with the given settings and returns their names
returns a "smart" node client to a random node in the cluster
closes the current node if not already closed, builds a new node object using the current node settings and starts it
returns the name of the current master node in the cluster and executes the request via the node specified in the vianode parameter. if vianode isn't specified a random node will be picked to the send the request to.
returns an iterable to all instances for the given class >t< across all nodes in the cluster.
restarts all nodes in the cluster. it first stops all nodes and then restarts all the nodes again.
simulates sending diffs over the wire
asserts that changes are applied correctly, i.e. that applying diffs to localinstance produces that object equal but not the same as the remotechanges instance.
tests making random changes to an object, calculating diffs for these changes, sending this diffs over the wire and appling these diffs on the other side.
creates a new org.elasticsearch.test.nodeconfigurationsource for the security configuration.
corrupts a random file at a random position
fail the process if we didn't detect a particular violation. named to look like a junit assertion even though it isn't because it is similar enough.
returns a tuple that contains a randomized shardinfo value (left side) and its corresponding value (right side) after it has been printed out as a toxcontent and parsed back using a parsing method like shardinfo#fromxcontent(xcontentparser). a `withshardfailures` parameter indicates if the randomized shardinfo must or must not contain shard failures.
returns a tuple containing random stored field values and their corresponding expected values once printed out via org.elasticsearch.common.xcontent.toxcontent#toxcontent(xcontentbuilder, toxcontent.params) and parsed back via generates values based on what can get printed out. stored fields values are retrieved from lucene and converted via
returns a tuple that contains a randomized failure value (left side) and its corresponding value (right side) after it has been printed out as a toxcontent and parsed back using a parsing method like shardinfo.failure#fromxcontent(xcontentparser).
randomly adds fields, objects, or arrays to the provided builder. the maximum depth is 5.
returns a random source in a given xcontenttype containing a random number of fields, objects and array, with maximum depth 5. the minimum number of fields per object is provided as an argument.
this method takes the input xcontent data and adds a random field value, inner object or array into each json object. this can e.g. be used to test if parsers that handle the resulting xcontent can handle the augmented xcontent correctly, for example when testing lenient parsing. if the xcontent output contains objects that should be skipped of such treatment, an optional filtering this predicate should check the xcontent path that we want to insert to and return true if the path should be excluded. paths are string concatenating field names and array indices, so e.g. in:  "foo1 : "bar" : [ ... , ... , "baz" : insert here ]  "foo1.bar.2.baz" would point to the desired insert location. to exclude inserting into the "foo1" object we would user a predicate like  (path) -> path.endswith("foo1")  or if we don't want any random insertions in the "foo1" tree we could use  (path) -> path.contains("foo1") 
this utility method takes an xcontentparser and walks the xcontent structure to find all possible paths to where a new object or array starts. this can be used in tests that add random xcontent values to test parsing code for errors or to check their robustness against new fields. the path uses dot separated fieldnames and numbers for array indices, similar to what we do in the stack passed in should initially be empty, it gets pushed to by recursive calls as an example, the following json xcontent:  "foo" : "bar", "foo1" : [ 1, "foo2" : "baz" , 3, 4] "foo3" : "foo4" : "foo5": "buzz"  would return the following list:  "" (the empty string is the path to the root object) "foo1.1" "foo3 "foo3.foo4 
inserts keyvalue pairs into xcontent passed in as bytesreference and returns a new xcontentbuilder the paths argument uses dot separated fieldnames and numbers for array indices, similar to what we do in the keyvalue arguments can suppliers that either return fixed or random values.
perform common equality and hashcode checks on the input object from the input in one aspect. the output of this call is used to check that it is not equal() to the input object
verifies that all nodes that have the same version of the cluster state as master have same cluster state
waits until mappings for the provided fields exist on all nodes. note, this waits for the current started shards and checks for concrete mappings.
returns a settings object used in #createindex(string...) and #preparecreate(string) and friends. this method can be overwritten by subclasses to set defaults for the indices that are created by the test. by default it returns a settings object that sets a random number of shards. number of shards and replicas can be controlled through specific methods.
creates a randomized index template. this template is used to pass in randomized settings on a per index basis. allows to enabledisable the randomization for number of shards and replicas
returns path to a random directory that can be used to create a temporary file system repo
asserts that all shards are allocated on nodes matching the given node pattern.
indexes the given indexrequestbuilder instances randomly. it shuffles the given builders and either indexes them in a blocking or async fashion. this is very useful to catch problems that relate to internal document ids or index segment creations. some features might have bug when a given document is the first or the last in a segment or if only one document is in a segment etc. this method prevents issues like this by randomizing the index layout. all documents are indexed. this is useful to produce deleted documents on the server side.
waits until at least a give number of document is visible for searchers this saves on unneeded searches.
flush some or all indices in the cluster.
waits for the given mapping type to exists on the master node.
return the mock plugins the cluster should use
ensures the result counts are as expected, and logs the results if different
creates one or more indices and asserts that the indices are acknowledged. if one of the indices already exists this method will fail and wipe all the indices created so far.
this method is used to obtain settings for the nth node in the cluster. nodes in this cluster are associated with an ordinal number such that nodes can be started with specific configurations. this method might be called multiple times with the same ordinal and is expected to return the same value for each invocation. in other words subclasses must ensure this method is idempotent.
asserts that all segments are sorted with the provided sort.
waits for all relocating shards to become active and the cluster has reached the given health status using the cluster health api.
returns path to a random directory that can be used to create a temporary file system repo
maybe refresh, force merge, or flush then always make sure there aren't too many in flight async operations.
remove any customs except for customs that we know all clients understand.
returns the transport client ratio from the class level annotation or via return a random ratio in the interval [0..1].
restricts the given index to be allocated on n nodes using the allocation deciders. yet if the shards can't be allocated on any other node shards for this index will remain allocated on more than n nodes.
returns the client ratio configured via
creates the indices provided as argument, randomly associating them with aliases, indexes one dummy document per index and refreshes the new indices
asserts that there are no files in the specified path
returns the bytes that represent the xcontent output of the provided toxcontent object, using the provided by the toxcontent#isfragment() method returns. shuffles the keys to make sure that parsing never relies on keys ordering.
creates an indicesmodule for testing with the given mappers and metadata mappers.
returns a random number of temporary paths.
assert that two objects are equals, calling toxcontent#toxcontent(xcontentbuilder, toxcontent.params) to print out their differences if they aren't equal.
runs the code block for the provided interval, waiting for no assertions to trip.
helper method to create a byte array of a given length populated with random byte values
randomly shuffles the fields inside objects parsed using the xcontentparser passed in. recursively goes through inner objects and also shuffles them. exceptions for this recursive shuffling behavior can be made by passing in the names of fields which internally should stay untouched.
builds a set of unique items. usually you'll get the requested count but you might get less than that number if the supplier returns lots of repeats. make sure that the items properly implement equals and hashcode.
helper to get a random value in a certain range that's different from the input
returns a java.nio.file.path pointing to the class path relative resource given as the first argument. in contrast to getclass().getresource(...).getfile() this method will not return url encoded paths if the parent path contains spaces or other non-standard characters.
returns size random values
creates an testanalysis with all the default analyzers configured.
reset the deprecation logger by removing the current thread context, and setting a new thread context if setnewthreadcontext is set to true and otherwise clearing the current thread context.
helper to randomly perform on consumer with value
helper method to return the first 20 chars of a request's body
closes down the webserver. also tries to stop all the currently sleeping requests first by counting down their respective latches.
creates a mockrequest from an incoming http request, that can later be checked in your test assertions
adds a response to the response queue that is used when a request comes in note: every response is only processed once
starts the webserver and binds it to an arbitrary ephemeral port the webserver will be able to serve requests once this method returns
sleep the specified amount of time, if the time value is not null
adds a new header to this headers object
returns the closeaction to execute on the actual engine. note this method changes the state on the first call and treats subsequent calls as if the engine passed is already closed.
obtain the logging levels from the test logging annotation.
reset the logging levels to the state provided by the map.
obtain the logger with the given name.
applies the test logging annotation and returns the existing logging levels.
append a single vm option.
creates a random shape useful for randomized testing, note: exercise caution when using this to build random geometrycollections as creating a large random number of random shapes can result in massive resource consumption see: geoshapequerytests#testshapefilterwithrandomgeocollection the following options are included
assert that an index template is missing
assert that an index template exists
asserts that the provided bytesreferences created through the comparison is done by parsing both into a map and comparing those two, so that keys ordering doesn't matter. also binary values (byte[]) are properly compared through arrays comparisons.
compares two maps recursively, using arrays comparisons for byte[] through arrays.equals(byte[], byte[])
checks that all shard requests of a replicated broadcast request failed due to a cluster block
compares two lists recursively, but using arrays comparisons for byte[] through arrays.equals(byte[], byte[])
run future.actionget() and check that it throws an exception of the right type, optionally checking the exception's rest status
executes the request and fails if the request has not been blocked by a specific clusterblock.
remove any cluster settings.
used to obtain settings for the rest client that is used to send rest requests.
checks that the specific index is green. we force a selection of an index as the tests share a cluster and often leave indices in an non green state
convert the entity from a response into a map of maps.
is this template one that is automatically created by xpack?
waits until all shard initialization is completed. this is a handy alternative to ensuregreen as it relates to all shards in the cluster and doesn't require to know how many nodesreplica there are.
waits for the cluster state updates to have been processed, so that no cluster state updates are still in-progress when the next test starts.
does the cluster being tested have xpack installed?
logs a message if there are still running tasks. the reasoning is that any tasks still running are state the is trying to bleed into other tests.
wipe fs snapshots we created one by one and all repositories so that the next test can create the repositories fresh and they'll start empty. there isn't an api to delete all snapshots. there is an api to delete all snapshot repositories but that leaves all of the snapshots intact in the repository.
tells whether a particular key needs to be looked up in the stash based on its name. returns true if the string representation of the key starts with "$", false otherwise the stash contains fields eventually extracted from previous responses that can be reused as arguments for following requests (e.g. scroll_id)
lookup a value from the stash adding support for a special key ( $_path) which returns a string that is the location in the path of the of the object currently being unstashed. this is useful during documentation testing.
allows to saved a specific field in the stash as key-value pair
retrieves a value from the current stash. the stash contains fields eventually extracted from previous responses that can be reused as arguments for following requests (e.g. scroll_id)
constructs a new blacklistedpathpatternmatcher instance from the provided suffix pattern.
create a new xcontentbuilder from the xcontent object underlying this objectpath. this only works for objectpath instances created from an xcontent object, not from nested substructures. we throw an unsupportedoperationexception in those cases.
calls an elasticsearch api with the parameters and request body provided as arguments. saves the obtained response in the execution context.
clears the last obtained response and the stashed fields
calls an api with the provided parameters and body
returns the body as a string
returns the body properly parsed depending on the content type. might be a string or a json object parsed as a map.
get a list of all of the values of all warning headers returned in the response.
parses the response body and extracts a specific value from it (identified by the provided path)
sniff the cluster for host metadata and return a
create parameters for this parameterized test.
execute an executablesection, careful to log its place of origin on failure.
add a single suite file to the set of suites.
tells whether all the features provided as argument are supported
parses the complete set of rest spec available under the provided directories
finds the matching rest paths out of the available ones with the current api (based on rest spec). the best path is the one that has exactly the same number of placeholders to replace (e.g. indextypeid when the path params are exactly index, type and id).
returns the supported http methods given the rest parameters provided
finds the best matching rest path given the current parameters and replaces placeholders with their corresponding values received as arguments
parse a teardownsection if the next field is skip, otherwise returns teardownsection#empty.
check that the response contains only the warning headers that we expect.
parse a skipsection if the next field is skip, otherwise returns skipsection#empty.
parse a setupsection if the next field is skip, otherwise returns setupsection#empty.
opens a httpserver and start listening on a random port.
returns an error message that has the form of stack traces emitted by throwable#printstacktrace
checks that a gc disruption never blocks threads while they are doing something "unsafe" but does keep retrying until all threads can be safely paused
ensures that all nodes in the cluster are connected to each other. some network disruptions may leave nodes that are not the master disconnected from each other. handy to be able to ensure this happens faster
applies action to all disrupted links between two sets of nodes.
delays requests by a random but fixed time value between delaymin and delaymax.
resolves all threads belonging to given node and suspends them if their current stack trace is "safe". threads are added to nodethreads if suspended. returns true if some live threads were found. the caller is expected to call this method until no more "live" are found.
simulate a remote error for the given requestid, will be wrapped by a remotetransportexception request
returns all requests captured so far. this method does clear the captured requests list. if you do not want the captured requests list cleared, use #capturedrequests().
returns all requests captured so far, grouped by target node. this method does clear the captured request list. if you do not want the captured requests list cleared, use
simulate a local error for the given requestid, will be wrapped by a sendrequesttransportexception request
build the service. updates for #trace_log_exclude_setting and #trace_log_include_setting.
adds a rule that will cause ignores each send request, simulating an unresponsive node and failing to connect once the rule was added.
adds a rule that will cause every send request to fail, and each new connect since the rule is added to fail as well.
adds a rule that will cause ignores each send request, simulating an unresponsive node and failing to connect once the rule was added.
adds a random non existing field to the provided document and associates it with the provided value. the field will be added at a random position within the document, not necessarily at the top level using a leaf field name.
returns a random field name. can be a leaf field name or the path to refer to a field name using the dot notation.
generates a document that holds random metadata and the document provided as a map argument
checks whether the provided field name can be safely added to the provided document. when the provided field name holds the path using the dot notation, we have to make sure that each node of the tree either doesn't exist or is a map, otherwise new fields cannot be added.
returns a randomly selected existing field name out of the fields that are contained in the document provided as an argument.
helper method to assert the equivalence between two ingestdocuments.
returns the value contained in the document for the provided path as a byte array. if the path value is a string, a base64 decode operation will happen. if the path value is a byte array, it is just returned or if the field that is found at the provided path is not of the expected type.
does the same thing as #extractmetadata but does not mutate the map.
removes the field identified by the provided path.
checks whether the document contains a value for the provided path
one time operation that extracts the metadata fields from the ingest document and returns them. metadata fields that used to be accessible as ordinary top level fields will be removed as part of this call.
returns the value contained in the document for the provided path or if the field that is found at the provided path is not of the expected type.
read from a stream.
read from a stream.
deletes the pipeline specified by id in the request.
stores the specified pipeline definition in the request.
returns and removes the specified property from the specified configuration map. if the property value isn't of type string or int a elasticsearchparseexception is thrown. if the property is missing and no default value has been specified a elasticsearchparseexception is thrown
returns and removes the specified property of type list from the specified configuration map. if the property value isn't of type list an elasticsearchparseexception is thrown. if the property is missing an elasticsearchparseexception is thrown
returns and removes the specified property of type list from the specified configuration map. if the property value isn't of type list an elasticsearchparseexception is thrown.
returns and removes the specified property of type map from the specified configuration map. if the property value isn't of type map an elasticsearchparseexception is thrown. if the property is missing an elasticsearchparseexception is thrown
returns and removes the specified optional property from the specified configuration map. if the property value isn't of type string a elasticsearchparseexception is thrown.
returns and removes the specified property from the specified configuration map. if the property value isn't of type int a elasticsearchparseexception is thrown. if the property is missing an elasticsearchparseexception is thrown
returns and removes the specified property from the specified configuration map. if the property value isn't of type string a elasticsearchparseexception is thrown. if the property is missing and no default value has been specified a elasticsearchparseexception is thrown
returns and removes the specified property as an object from the specified configuration map.
returns and removes the specified property of type map from the specified configuration map. if the property value isn't of type map an elasticsearchparseexception is thrown.
returns and removes the specified property from the specified configuration map. if the property value isn't of type string or int a elasticsearchparseexception is thrown.
read from a stream.
parses with tika, throwing any exception hit while parsing the document
don't silently do dns lookups or anything trappy on bogus data
executes the script with the ingest document in context.
to maintain compatibility with logstash-filter-useragent
this test tries to simulate load while creating an index and indexing documents while the index is being created.
this method sets the context for a server socket channel. the context is called when a new channel is accepted, an exception occurs, or it is time to close the channel.
this method will attempt to complete the connection process for this channel. it should be called for new channels or for a channel that has produced a op_connect event. if this method returns true then the connection is complete and the channel is ready for reads and writes. if it returns false, the channel is not yet connected and this method should be called again when a op_connect event is received.
this method is called after events (read, write, connect) have been handled for a channel.
this method is called when a niochannel is being registered with the selector. it should only be called once per channel.
this method is called when a niosocketchannel has just been accepted or if it has receive an op_connect event.
this method handles the closing of an niochannel
this will create an niogroup with dedicated acceptors. all server channels will be handled by a group of selectors dedicated to accepting channels. these accepted channels will be handed off the non-server selectors.
returns a boolean indicating if the operation was fully flushed.
this method will return an array of bytebuffer representing the bytes from the index passed through the end of this buffer. the buffers will be duplicates of the internal buffers, so any modifications to the markers bytebuffer#position(), bytebuffer#limit(), etc will not modify the this class.
this method will release bytes from the head of this buffer. if you release bytes past the current index the index is truncated to zero.
this method will return an array of bytebuffer representing the bytes from the beginning of this buffer up through the index argument that was passed. the buffers will be duplicates of the internal buffers, so any modifications to the markers bytebuffer#position(),
this method will return an array of page representing the bytes from the beginning of this buffer up through the index argument that was passed. the pages and buffers will be duplicates of the internal components, so any modifications to the markers bytebuffer#position(), retain the underlying pages, so the pages returned by this method must be closed.
this method cleans up any context resources that need to be released when a channel is closed. it should only be called by the selector thread.
executes a failed listener with consistent exception handling. this can only be called from current selector thread.
queues a write operation directly in a channel's buffer. if this channel does not have pending writes already, the channel will be flushed. channel buffers are only safe to be accessed by the selector thread. as a result, this method should only be called by the selector thread. if this channel does not have pending writes already, the channel will be flushed.
executes a success listener with consistent exception handling. this can only be called from current selector thread.
starts this selector. the selector will run until #close() is called.
queues a write operation to be handled by the event loop. this can be called by any thread and is the api available for non-selector threads to schedule writes.
this is a convenience method to be called after some object (normally channels) are enqueued with this selector. this method will check if the selector is still open. if it is open, normal operation can proceed. if the selector is closed, then we attempt to remove the object from the queue. if the removal succeeds then we throw an illegalstateexception indicating that normal operation failed. if the object cannot be removed from the queue, then the object has already been handled by the selector and operation can proceed normally. if this method is called from the selector thread, we will not allow the queuing to occur as the selector thread can manipulate its queues internally even if it is no longer open.
schedules a niochannel to be registered with this selector. the channel will by queued and eventually registered next time through the event loop.
indexes a document in index with docid then concurrently updates the same document nupdates times
gets attributes that are supported by all filesystems
gets the owner of a file in a way that should be supported by all filesystems that have a concept of file owner
gets attributes that are supported by posix filesystems
schedules a one-shot command to run after a given delay. the command is not run in the context of the calling thread. to preserve the context of the calling thread you may call threadpool.getthreadcontext().preservecontext on the runnable before passing it to this method. meaning of the scheduledfuture returned by this method. in that case the scheduledfuture will complete only when the command completes. the task is canceled before it was added to its target thread pool. once the task has been added to its target thread pool the scheduledfuture will cannot interact with it.
returns true if the given service was terminated successfully. if the termination timed out, the service is null this method will return false.
get the executorservice with the given name. this executor service's  warning: this executorservice might not throw rejectedexecutionexception if you submit a task while it shutdown. it will instead silently queue it and not run it.
returns true if the given pool was terminated successfully. if the termination timed out, the service is null this method will return false.
construct a fixed executor builder.
creates a new rescheduling runnable and schedules the first execution to occur after the interval specified
returns an implementation that checks for each fixed interval if there are threads that have invoked #register() and not #unregister() and have been in this state for longer than the specified max execution interval and then interrupts these threads.
checks whether patterns reference each other in a circular manner and if so fail with an exception in a pattern, anything between % and  or : is considered a reference to another named pattern. this method will navigate to all these named patterns and check for a circular reference.
checks whether a specific text matches the defined grok expression.
matches and returns any named captures within a compiled grok expression that matched within the provided text.
converts a grok expression into a named regex expression
read from a stream.
return an extraction for the conjunction of result1 and result2 by picking up clauses that look most restrictive and making it unverified if the other clause is not null and doesn't match all documents. this is used by 6.0.0 indices which didn't use the terms_set query.
extracts terms and ranges from the provided query. these terms and ranges are stored with the percolator query and used by the percolate query's candidate query as fields to be query by. the candidate query holds the terms from the document to be percolated and allows to the percolate query to ignore percolator queries that we know would otherwise never match.  when extracting the terms for the specified query, we can also determine if the percolator query is always going to match. for example if a percolator query just contains a term query or a disjunction query then when the candidate query matches with that, we know the entire percolator query always matches. this allows the percolate query to skip the expensive memory index verification step that it would otherwise have to execute (for example when a percolator query contains a phrase query or a conjunction query).  the query analyzer doesn't always extract all terms from the specified query. for example from a boolean query with no should clauses or phrase queries only the longest term are selected, since that those terms are likely to be the rarest. boolean query's must_not clauses are always ignored.  sometimes the query analyzer can't always extract terms or ranges from a sub query, if that happens then query analysis is stopped and an unsupportedqueryexception is thrown. so that the caller can mark this query in such a way that the percolatorquery always verifies if this query with the memoryindex.
fails if a percolator contains an unsupported query. the following queries are not supported: 1) a has_child query 2) a has_parent query
return settings that could be used to start a node that has the given zipped home directory.
read from a stream.
sets whether the query builder should ignore unmapped types (and run a the type is unmapped.
test (de)serialization on all previous released versions
read from a stream.
sets whether the query builder should ignore unmapped types (and run a the type is unmapped.
defines the minimum number of children that are required to match for the parent to be considered a match and the maximum number of children that are required to match for the parent to be considered a match.
read from a stream.
test (de)serialization on all previous released versions
returns the parent id field mapper associated with a parent name if isparent is true and a child name otherwise.
returns the parentjoinfieldmapper associated with the service or null if there is no parent-join field in this mapping.
the name of this aggregation the type of children documents
adds a new synthetic method to be written. it must be analyzed!
adds a new constant initializer to be written
headers to be added to the scriptexception for structured rendering.
test that the scriptstack looks good. by implication this tests that we build proper "line numbers" in stack trace. these line numbers are really 1 based character numbers.
constructor.
generates a stateful factory class that will return script instances. acts as a middle man between the scriptcontext#factoryclazz and the scriptcontext#instanceclazz when used so that the stateless factory can be used for caching and the stateful factory can act as a cache for new script instances. uses the newinstance method from a scriptcontext#statefulfactoryclazz to define the factory method to create new instances of the scriptcontext#instanceclazz.
generates a factory class that will return script instances or stateful factories. uses the newinstance method from a scriptcontext#factoryclazz to define the factory method to create new instances of the scriptcontext#instanceclazz or uses the newfactory method to create new factories of the scriptcontext#statefulfactoryclazz. on whether a scriptcontext#statefulfactoryclazz is specified.
creates an constantcallsite
links the delegate method to the returned callsite. the linked delegate method will use converted types from the interface method. using invokedynamic to make the delegate method call allows of either a lot more code or requiring many classes to be looked up at link-time.
generates a lambda class for a lambda functionmethod reference within a painless script. variables with the prefix interface are considered to represent values for code generated for the lambda class. variables with the prefix delegate are considered to represent values for code generated within the painless script. the interface method delegates (calls) to the delegate method. captured types are based on the parameters for this method (static, virtual, interface, or constructor) the captured types if the value is '1' if the delegate is an interface and '0' otherwise; note this is an int because the bootstrap method cannot convert constants to boolean that implements the expected functional interface
validates some conversions at link time. currently, only ensures that the lambda method with a return value cannot delegate to a delegate method with no return type.
defines the class for the lambda class using the same compiler.loader that originally defined the class for the painless script.
generates a factory method to delegate to constructors.
creates the classwriter to be used for the lambda class generation.
generates member fields for captured variables based on the parameters for the factory method. for generating method arguments later on
generates the interface method that will delegate (call) to the delegate method with invokedynamic using the #delegatebootstrap type converter.
creates an constantcallsite that will return the same instance of the generated lambda class every time this linked factory method is called.
generates a constructor that will take in captured arguments if any and store them in their respective member fields.
creates a scriptimpl for the a previously compiled painless script.
writes a dynamic binary instruction: returntype, lhs, and rhs can be different
encodes the offset into the line number table as offset + 1.  this is invoked before instructions that can hit exceptions.
marks a new statement boundary.  this is invoked for each statement boundary (leaf s nodes).
starts a new string concat.
writes a dynamic call for a def method.
writes a static binary instruction
will check to see if the class has already been loaded when the painlesslookup was initially created. allows for whitelisted classes to be loaded from other modulesplugins without a direct relationship to the module'splugin's classloader.
runs the two-pass compiler to generate a painless script.
runs the two-pass compiler to generate a painless script. (used by the debugger.)
called when a new type is encountered or if cached type does not match. in that case we revert to a generic, but slower operator handling.
invokedynamic bootstrap method  in addition to ordinary parameters, we also take some parameters defined at the call site:   initialdepth: initial call site depth. this is used to exercise megamorphic fallback.  flavor: type of dynamic call it is (and which part of whitelist to look at).  args: flavor-specific args.  and we take the painlesslookup used to compile the script for whitelist checking.  see https:docs.oracle.comjavasespecsjvmsse7htmljvms-6.html#jvms-6.5.invokedynamic
creates the methodhandle for the megamorphic call site using classvalue and methodhandles#exactinvoker(methodtype):
called when a new type is encountered (or, when we have encountered more than max_depth types at this call site and given up on caching using this fallback and we switch to a megamorphic cache using classvalue).
does a slow lookup for the operator
does a slow lookup against the whitelist.
creates a new lambda scope inside the current scope  this is just like #newfunctionscope, except the captured parameters are made read-only.
creates a new main method scope
creates a new function scope inside the current scope
creates a new program scope: the list of methods. it is the parent for all methods
returns the top-level program scope.
creates a new variable. throws iae if the variable has already been defined (even in a parent) or reserved.
looks up a variable at this scope only. returns null if the variable does not exist.
defines a variable at this scope internally.
checks if a variable exists or not, in this scope or any parents.
accesses a variable. this will throw iae if the variable does not exist
create a new locals with specified return type
creates a new local variable scope (e.g. loop) inside the current scope
looks up handle for a dynamic field getter (field load)  a dynamic field load for variable x of type def looks like:  the following field loads are allowed:  whitelisted field from receiver's class or any superclasses. whitelisted method named getfield() from receiver's classsuperclassesinterfaces. whitelisted method named isfield() from receiver's classsuperclassesinterfaces. the length field of an array. the value corresponding to a map key named field when the receiver is a map. the value in a list at element field (integer) when the receiver is a list.   this method traverses recieverclass's class hierarchy (including interfaces) until it finds a matching whitelisted getter. if one is not found, it throws an exception. otherwise it returns a handle to the matching getter. 
returns a method handle to do iteration (for enhanced for loop)
returns an implementation of interfaceclass that calls receiverclass.name  this is just like lambdametafactory, only with a dynamic type. the interface type is known, so we simply need to lookup the matching implementation method based on receiver type.
returns a method handle to normalize the index into an array. this is what makes lists and arrays stored in def support negative offsets. to use with array loads and array stores
returns a method handle to do an array store. and the value to set as 3rd argument. return value is undefined and should be ignored.
returns a method handle to do an array load. it returns the loaded value.
returns a method handle to an implementation of clazz, given method reference signature.
looks up handle for a dynamic field setter (field store)  a dynamic field store for variable x of type def looks like:  the following field stores are allowed:  whitelisted field from receiver's class or any superclasses. whitelisted method named setfield() from receiver's classsuperclassesinterfaces. the value corresponding to a map key named field when the receiver is a map. the value in a list at element field (integer) when the receiver is a list.   this method traverses recieverclass's class hierarchy (including interfaces) until it finds a matching whitelisted setter. if one is not found, it throws an exception. otherwise it returns a handle to the matching setter. 
looks up handle for a dynamic method call, with lambda replacement  a dynamic method call for variable x of type def looks like:  this method traverses recieverclass's class hierarchy (including interfaces) until it finds a matching whitelisted method. if one is not found, it throws an exception. otherwise it returns a handle to the matching method. 
returns an array length getter methodhandle for the given array type
compiles to bytecode, and returns debugging output
test loads and stores with update script equivalent
test that we revert to the megamorphic classvalue cache and that it works as expected
calls tostring() on integers, twice
checks a specific exception class is thrown (boxed inside scriptexception) and returns it.
compiles and returns the result of script with access to vars and compile-time parameters
asserts that the script_stack looks right.
compiles and returns the result of script with access to vars
script contexts used to build the script engine. override to customize which script contexts are available.
returns an appropriate method handle for a binary operator, based on promotion of the lhs and rhs arguments
looks up generic method, with a dynamic cast to the receiver's type. (compound assignment)
slowly returns a number for o. just for supporting dynamiccast
looks up generic method, with a dynamic cast to the specified type. (explicit assignment)
slow dynamic cast: casts value to an instance of clazz based upon inspection. if lhs is null, no cast takes place.
forces a cast to class a for target (only if types differ)
binary promotion.
returns an appropriate method handle for a unary or shift operator, based only on the receiver (lhs)
unary promotion. all objects are promoted to object.
creates a new functionref which will resolve type::call from the whitelist.
emit an external link to javadoc for a painlessfield.
document a method.
document a constructor.
emit an external link to javadoc for a painlessmethod.
anchor text for a painlessconstructor.
emit an external link to javadoc for a painlessmethod.
emit a painlessclass. if the painlessclass is primitive or def this just emits the name of the struct. otherwise this emits an internal link with the name.
pick the javadoc root for a class.
emit a class. if the type is primitive or an array of primitives this just emits the name of the type. otherwise this emits an internal link with the text.
anchor text for a painlessfield.
anchor text for a painlessmethod.
returns true for methods that are part of the runtime
adds stack trace and other useful information to exceptions thrown from a painless script.
computes the file name (mostly important for stacktraces)
augments an exception with this location's information.
converts a canonical type name to a type based on the terminology specified as part of the documentation for safely convert a canonical class name to a class as well.
converts a type to a canonical type name based on the terminology specified as part of the documentation for to a canonical class name as well.
converts a type to a java type based on the terminology specified as part of painlesslookuputility where if a type is a def class or def array, the returned type will be the equivalent object class or object array. otherwise, this behaves as an identity function.
converts a java type to a type based on the terminology specified as part of painlesslookuputility where if a type is an object class or object array, the returned type will be the equivalent def class or def array. otherwise, this behaves as an identity function.
converts a list of types to a list of canonical type names as a string based on the terminology specified as part of the documentation for painlesslookuputility. since classes are a subset of types, this method will safely convert a list of classes or a mixed list of classes and types to a list of canonical type names as a string as well.
create a cast where the original type will be boxed, and then the cast will be performed.
create a cast where the target type will be unboxed, and then the cast will be performed.
create a cast where the original type will be unboxed, and then the cast will be performed.
create a cast where the target type will be boxed, and then the cast will be performed.
build #tostring() for a node that optionally ends in (args some arguments here). usually function calls.
build a #tostring() for some expressions. usually best to use #singlelinetostring(object...) or
zip two (potentially uneven) lists together into for #tostring().
handles writing byte code for variablemethod chains for all given possibilities including string concatenation, compound assignment, regular assignment, and simple reads. includes proper duplication for chained assignments and assignments that are also read from.
inserts ecast nodes into the tree for implicit casts. also replaces nodes with the constant variable set to a non-null value with econstant.
writes the function to given classvisitor.
writes the opcodes to flip a negative array index (meaning slots from the end of the array) into a 0-based one (meaning slots from the start of the array).
sums the result of an iterable
counts the number of occurrences which satisfy the given predicate from inside this iterable.
iterates through an iterable type, passing each item and the item's index (a counter starting at zero) to the given consumer.
replace the first match. similar to matcher#replacefirst(string) but allows you to customize the replacement based on the match.
iterates through this collection transforming each entry into a new value using the function, adding the values to the specified collection.
iterates through the collection calling the given function for each item but stopping once the first non-null result is found and returning that result. if all results are null, defaultresult is returned.
finds the first value matching the predicate, or returns null.
finds all values matching the predicate, returns as a list
iterates through the map transforming items using the supplied function and collecting any non-null results.
sorts all map members into groups determined by the supplied mapping function.
used to determine if the given predicate is valid (i.e. returns true for all items in this map).
sorts all iterable members into groups determined by the supplied mapping function.
splits all items into two collections based on the predicate. the first list contains all items which match the closure expression. the second list all those that don't.
converts this iterable to a collection. returns the original iterable if it is already a collection.
iterates through this map transforming each entry into a new value using the function, adding the values to the specified collection.
iterates over the contents of an iterable, and checks whether a predicate is valid for at least one element.
sums the result of applying a function to each item of an iterable.
finds the first entry matching the predicate, or returns null.
counts the number of occurrences which satisfy the given predicate from inside this map
concatenates the tostring() representation of each item in this iterable, with the given string as a separator between each item.
iterates through this map transforming each entry into a new value using the function, returning a list of transformed values.
converts this iterable to a list. returns the original iterable if it is already a list.
replace all matches. similar to matcher#replaceall(string) but allows you to customize the replacement based on the match.
used to determine if the given predicate is valid (i.e. returns true for all items in this iterable).
iterates through the iterable transforming items using the supplied function and collecting any non-null results.
iterates through the map calling the given function for each item but stopping once the first non-null result is found and returning that result. if all results are null, defaultresult is returned.
iterates through this collection transforming each entry into a new value using the function, returning a list of transformed values.
finds all values matching the predicate, returns as a map.
loads and creates a whitelist from one to many text files. the file paths are passed in as an array of is the path of a single text file. the class's classloader will be used to lookup the java reflection objects for each individual class, constructor, method, and field specified as part of the whitelist in the text file. a single pass is made through each file to collect all the information about each class, constructor, method, and field. most validation will be done at a later point after all whitelists have been gathered and their merging takes place. a painless type name is one of the following:   def - the painless dynamic type which is automatically included without a need to be whitelisted.   fully-qualified java type name - any whitelisted java class will have the equivalent name as a painless type name with the exception that any dollar symbols used as part of inner classes will be replaced with dot symbols.   short java type name - the text after the final dot symbol of any specified java class. a short type java name may be excluded by using the 'no_import' token during painless class parsing as described later.   the following can be parsed from each whitelist text file:   blank lines will be ignored by the parser.   comments may be created starting with a pound '#' symbol and end with a newline. these will be ignored by the parser.   primitive types may be specified starting with 'class' and followed by the java type name, an opening bracket, a newline, a closing bracket, and a final newline.   complex types may be specified starting with 'class' and followed the fully-qualified java class name, optionally followed by an 'no_import' token, an opening bracket, a newline, constructormethodfield specifications, a closing bracket, and a final newline. within a complex type the following may be parsed:   a constructor may be specified starting with an opening parenthesis, followed by a comma-delimited list of painless type names corresponding to the typeclass names for the equivalent java parameter types (these must be whitelisted as well), a closing parenthesis, and a newline.   a method may be specified starting with a painless type name for the return type, followed by the java name of the method (which will also be the painless name for the method), an opening parenthesis, a comma-delimited list of painless type names corresponding to the typeclass names for the equivalent java parameter types (these must be whitelisted as well), a closing parenthesis, and a newline.   an augmented method may be specified starting with a painless type name for the return type, followed by the fully qualified java name of the class the augmented method is part of (this class does not need to be whitelisted), the java name of the method (which will also be the painless name for the method), an opening parenthesis, a comma-delimited list of painless type names corresponding to the typeclass names for the equivalent java parameter types (these must be whitelisted as well), a closing parenthesis, and a newline.  a field may be specified starting with a painless type name for the equivalent java type of the field, followed by the java name of the field (which all be the painless name for the field), and a newline.    note there must be a one-to-one correspondence of painless type names to java typeclass names. if the same painless type is defined across multiple files and the java class is the same, all specified constructors, methods, and fields will be merged into a single painless type. the painless dynamic type, 'def', used as part of constructor, method, and field definitions will be appropriately parsed and handled. painless complex types must be specified with the fully-qualified java class name. method argument types, method return types, and field types must be specified with painless type names (def, fully-qualified, or short) as described earlier. the following example is used to create a single whitelist text file: # primitive types class int -> int # complex types class my.package.example no_import # constructors () (int) (def, def) (example, def) # method example add(int, def) int add(example, example) void example() # augmented example some.other.class sub(example, int, def) # fields int value0 int value1 def value2
creates a new instance of this consumer with the provided buffer limit
asynchronously put a watch into the cluster see  href="https:www.elastic.coguideenelasticsearchreferencecurrentwatcher-api-put-watch.html"> the docs for more.
create a random number of headers. generated header names will either be the basename plus its index, or exactly the provided basename so that the we test also support for multiple headers with same key and different values.
opens a machine learning job asynchronously, notifies listener on completion. when you open a new job, it starts with an empty model. when you open an existing job, the most recent model state is automatically loaded. the job is ready to resume its analysis from where it left off, once new data is received.  for additional info see  href="https:www.elastic.coguideenelasticsearchreferencecurrentml-open-job.html"> 
deletes the given machine learning job asynchronously and notifies the listener on completion  for additional info see  href="http:www.elastic.coguideenelasticsearchreferencecurrentml-delete-job.html">ml delete job documentation 
deletes the given machine learning job  for additional info see  href="http:www.elastic.coguideenelasticsearchreferencecurrentml-delete-job.html">ml delete job documentation 
opens a machine learning job. when you open a new job, it starts with an empty model. when you open an existing job, the most recent model state is automatically loaded. the job is ready to resume its analysis from where it left off, once new data is received.  for additional info see  href="https:www.elastic.coguideenelasticsearchreferencecurrentml-open-job.html"> 
creates a new machine learning job asynchronously and notifies listener on completion  for additional info see  href="https:www.elastic.coguideenelasticsearchreferencecurrentml-put-job.html">ml put job documentation
creates a new machine learning job  for additional info see  href="https:www.elastic.coguideenelasticsearchreferencecurrentml-put-job.html">ml put job documentation
parse the get aliases response
asynchronously deletes a snapshot. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
gets the status of requested snapshots. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
asynchronously deletes a snapshot repository. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
asynchronously creates a snapshot repository. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
creates a snapshot.  see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
asynchronously restores a snapshot. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
deletes a snapshot repository. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
asynchronously gets a list of snapshot repositories. if the list of repositories is empty or it contains a single element "_all", all registered repositories are returned. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
verifies a snapshot repository. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
asynchronously gets the status of requested snapshots. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
gets a list of snapshot repositories. if the list of repositories is empty or it contains a single element "_all", all registered repositories are returned. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
asynchronously verifies a snapshot repository. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
asynchronously creates a snapshot.  see  href="https:www.elastic.coguideenelasticsearchreferencecurrentmodules-snapshots.html"> snapshot and restore api on elastic.co
asynchronously simulate a pipeline on a set of documents provided in the request  see  href="https:www.elastic.coguideenelasticsearchreferencemastersimulate-pipeline-api.html"> simulate pipeline api on elastic.co
checks the given rest client has the provided default headers.
converts an entire response into a json string this is useful for responses that we don't parse on the client side, but instead work as string such as in case of the license json
add a query string parameter. the parameter is sent as name rather than name=value already been set
asynchronously fetch information about x-pack from the cluster. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentinfo-api.html"> the docs for more.
executes the provided request using either the sync method or its async variant, both provided as functions
test cases retrieves all six documents indexed above and checks the prec@10 calculation where all unlabeled documents are treated as not relevant.
test case checks that the default metrics are registered and usable
sets the maximum timeout (in milliseconds) to honour in case of multiple retries of the same request.
creates a new restclient based on the provided configuration.
sets the path's prefix for every request used by the http client.  for example, if this is set to "mypath", then any client request will become "mypath" + endpoint.  in essence, every request's endpoint is prefixed by this pathprefix. the path prefix is useful for when elasticsearch is behind a proxy that provides a base path or a proxy that requires all paths to start with ''; it is not intended for other purposes and it should not be supplied in other scenarios.
creates a new builder instance and sets the hosts that the client will send requests to.
test host selector against a real server and test what happens after calling
returns the value of the first header with a specified name of this message. if there is more than one matching header in the message the first element is returned. if there is no matching header in the message null is returned.
mocks the synchronous request execution like if it was executed by elasticsearch.
the resthighlevelclient must declare the following execution methods using the protected modifier so that they can be used by subclasses to implement custom logic.
mocks the asynchronous request execution by calling the #mockperformrequest(request) method.
creates a resthighlevelclient given the low level restclient that it should use to perform requests and a list of entries that allow to parse custom response sections added to elasticsearch through plugins. this constructor can be called by subclasses in case an externally created low-level rest client needs to be provided. the consumer argument allows to control what needs to be done when the #close() method is called. also subclasses can provide parsers for custom response sections added to elasticsearch through plugins.
converts a responseexception obtained from the low level rest client into an elasticsearchexception. if a response body was returned, tries to parse it as an error returned from elasticsearch. if no response body was returned or anything goes wrong while parsing the error, returns a new elasticsearchstatusexception that wraps the original responseexception. the potential exception obtained while parsing is added to the returned exception as a suppressed exception. this method is guaranteed to not throw any exception eventually thrown while parsing.
loads and returns the namedxcontentregistry.entry parsers provided by plugins.
asserts that the provided exception contains the method that called this somewhere on its stack. this is normally the case for synchronous calls but restclient performs synchronous calls by performing asynchronous calls and blocking the current thread until the call returns so it has to take special care to make sure that the caller shows up in the exception. we use this assertion to make sure that we don't break that "special care".
ensure that the indexrequest's content type is supported by the bulk api and that it conforms to the current bulkrequest's content type (if it's known at the time of this method get called).
assert that restclient#selectnodes fails on the provided arguments.
add the provided header to the request.
assert that the actual headers are the expected ones given the original default and request headers. some headers can be ignored, for instance in case the http client is adding its own automatically. will be part of the actual ones
end to end test for request and response body. exercises the mock http client ability to send back whatever body it has received.
verifies the content of the httprequest that's internally created and passed through to the http client
end to end test for request and response headers. exercises the mock http client ability to send back whatever headers it has received.
end to end test for ok status codes
end to end test for error status codes: they should cause an exception to be thrown, apart from 404 with head requests
asynchronously updates cluster wide specific settings using the cluster update settings api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentcluster-update-settings.html"> cluster update settings api on elastic.co
get the cluster wide settings using the cluster get settings api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentcluster-get-settings.html"> cluster get settings api on elastic.co
updates cluster wide specific settings using the cluster update settings api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentcluster-update-settings.html"> cluster update settings api on elastic.co
asynchronously get the cluster wide settings using the cluster get settings api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentcluster-get-settings.html"> cluster get settings api on elastic.co
build the initial dead state of a host. useful when a working host stops functioning and needs to be marked dead after its first failure. in such case the host will be retried after a minute or so.
puts an index template using the index templates api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-templates.html"> index templates api on elastic.co
asynchronously retrieve the settings of one or more indices. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-get-settings.html"> indices get settings api on elastic.co
checks if the index (indices) exists or not. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-exists.html"> indices exists api on elastic.co
asynchronously updates the mappings on an index using the put mapping api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-put-mapping.html"> put mapping api on elastic.co
asynchronously updates aliases using the index aliases api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-aliases.html"> index aliases api on elastic.co
retrieves the field mappings on an index or indices using the get field mapping api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-get-field-mapping.html"> get field mapping api on elastic.co
asynchronously gets index templates using the index templates api see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-templates.html"> index templates api on elastic.co
asynchronously retrieves the mappings on an index on indices using the get mapping api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-get-mapping.html"> get mapping api on elastic.co
asynchronously validate a potentially expensive query without executing it.  see  href="https:www.elastic.coguideenelasticsearchreferencecurrentsearch-validate.html"> validate query api on elastic.co
asynchronously initiate a synced flush manually using the synced flush api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-synced-flush.html"> synced flush api on elastic.co
asynchronously checks if the index (indices) exists or not. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-exists.html"> indices exists api on elastic.co
gets index templates using the index templates api see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-templates.html"> index templates api on elastic.co
asynchronously retrieves the field mappings on an index on indices using the get field mapping api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-get-field-mapping.html"> get field mapping api on elastic.co
asynchronously puts an index template using the index templates api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-templates.html"> index templates api on elastic.co
asynchronously calls the analyze api see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-analyze.html">analyze api on elastic.co
retrieve information about one or more indexes see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-get-index.html"> indices get index api on elastic.co
asynchronously clears the cache of one or more indices using the clear cache api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-clearcache.html"> clear cache api on elastic.co
clears the cache of one or more indices using the clear cache api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-clearcache.html"> clear cache api on elastic.co
asynchronously updates specific index level settings using the update indices settings api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrentindices-update-settings.html"> update indices settings api on elastic.co
cancel one or more cluster tasks using the task management api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrenttasks.html"> task management api on elastic.co
asynchronously cancel one or more cluster tasks using the task management api. see  href="https:www.elastic.coguideenelasticsearchreferencecurrenttasks.html"> task management api on elastic.co
this test verifies that we don't change the default value for the connection request timeout as that causes problems. see https:github.comelasticelasticsearchissues24069
build a mutable set containing all the node#gethost() hosts in use by the test.
sends a request to the elasticsearch cluster that the client points to. doesn't wait for the response, instead the provided responselistener will be notified upon completion or failure. shortcut to
returns a non-empty iterator of nodes to be used for a request that match the nodeselector.  if there are no living nodes that match the nodeselector this will return the dead node that matches the nodeselector that is closest to being revived.
sends a request to the elasticsearch cluster that the client points to. blocks until the request is completed and returns its response or fails by throwing an exception. selects a host out of the provided ones in a round-robin fashion. failing hosts are marked dead and retried after a certain amount of time (minimum 1 minute, maximum 30 minutes), depending on how many times they previously failed (the more failures, the later they will be retried). in case of failures all of the alive nodes (or dead nodes that deserve a retry) are retried until one responds or none of them does, in which case an ioexception will be thrown. this method works by performing an asynchronous call and waiting for the result. if the asynchronous call throws an exception we wrap it and rethrow it so that the stack trace attached to the exception contains the call site. while we attempt to preserve the original exception this isn't always possible and likely haven't covered all of the cases. you can get the original exception from connection on the client side.
select nodes to try and sorts them so that the first one will be tried initially, then the following ones if the previous attempt failed and so on. package private for testing.
add all parameters from a map to a request. this only exists to support methods that exist for backwards compatibility.
called after each successful request call. receives as an argument the host that was used for the successful request.
sends a request to the elasticsearch cluster that the client points to and waits for the corresponding response to be returned. shortcut to #performrequest(string, string, map, httpentity, header...) but without request body.
sends a request to the elasticsearch cluster that the client points to and waits for the corresponding response to be returned. shortcut to #performrequest(string, string, map, httpentity, header...) but without parameters and request body.
sends a request to the elasticsearch cluster that the client points to. doesn't wait for the response, instead the provided responselistener will be notified upon completion or failure. shortcut to
called after each failed attempt. receives as an argument the host that was used for the failed attempt.
sends a request to the elasticsearch cluster that the client points to. the request is executed asynchronously and the provided failure. selects a host out of the provided ones in a round-robin fashion. failing hosts are marked dead and retried after a certain amount of time (minimum 1 minute, maximum 30 minutes), depending on how many times they previously failed (the more failures, the later they will be retried). in case of failures all of the alive nodes (or dead nodes that deserve a retry) are retried until one responds or none of them does, in which case an ioexception will be thrown. request is completed or fails
sends a request to the elasticsearch cluster that the client points to and waits for the corresponding response to be returned. shortcut to #performrequest(string, string, map, httpentity, httpasyncresponseconsumerfactory, header...) which doesn't require specifying an httpasyncresponseconsumerfactory instance,
sends a request to the elasticsearch cluster that the client points to. the request is executed asynchronously and the provided responselistener gets notified upon request completion or failure. selects a host out of the provided ones in a round-robin fashion. failing hosts are marked dead and retried after a certain amount of time (minimum 1 minute, maximum 30 minutes), depending on how many times they previously failed (the more failures, the later they will be retried). in case of failures all of the alive nodes (or dead nodes that deserve a retry) are retried until one responds or none of them does, in which case an ioexception will be thrown. connection on the client side.
replaces the nodes with which the client communicates.
sends a request to the elasticsearch cluster that the client points to. doesn't wait for the response, instead the provided responselistener will be notified upon completion or failure. shortcut to #performrequestasync(string, string, map, httpentity, httpasyncresponseconsumerfactory, responselistener, header...) which doesn't require specifying an httpasyncresponseconsumerfactory instance,
add all headers from the provided varargs argument to a request. this only exists to support methods that exist for backwards compatibility.
sends a request to the elasticsearch cluster that the client points to. blocks until the request is completed and returns its response or fails by throwing an exception. selects a host out of the provided ones in a round-robin fashion. failing hosts are marked dead and retried after a certain amount of time (minimum 1 minute, maximum 30 minutes), depending on how many times they previously failed (the more failures, the later they will be retried). in case of failures all of the alive nodes (or dead nodes that deserve a retry) are retried until one responds or none of them does, in which case an ioexception will be thrown. this method works by performing an asynchronous call and waiting for the result. if the asynchronous call throws an exception we wrap it and rethrow it so that the stack trace attached to the exception contains the call site. while we attempt to preserve the original exception this isn't always possible and likely haven't covered all of the cases. you can get the original exception from
waits (up to a timeout) for some result of the request: either a response, or an exception.
tests sending a bunch of async requests works well (e.g. no timeoutexception from the leased pool) see https:github.comelasticelasticsearchissues24069
end to end test for headers. we test it explicitly against a real http client as there are different ways to setadd headers to the org.apache.http.client.httpclient. exercises the test http server ability to send back whatever headers it received.
verify that credentials continue to be sent even if a 401 (unauthorized) response is received
creates curl output for given request
logs a request that yielded a response
creates curl output for given response
logs a request that failed
randomize the fetchsourcecontext request parameters.
create a node with metadata. all parameters except need to decide what to do in their absence.
get the transportaction for an action, throwing exceptions if the action isn't available.
this test method is used to generate the put mapping java indices api documentation at "docsjava-apiadminindicesput-mapping.asciidoc" so the documentation gets tested so that it compiles and runs without throwing errors at runtime.
this is the single execution point of all clients.
establishes the node connections. if validateinhandshake is set to true, the connection will fail if node returned in the handshake response is different than the discovery node.
closes the client.
test that when plugins are provided that want to register
sets the sniffer instance used to perform sniffing
sets this task to be skipped. returns true if the task will be skipped, false if the task has already started.
cancels this task. returns true if the task has been successfully cancelled, meaning it won't be executed or if it is its execution won't have any effect. returns false if the task cannot be cancelled (possibly it was already cancelled or already completed).
schedule sniffing to run as soon as possible if it isn't already running. once such sniffing round runs it will also schedule a new round after sniffafterfailuredelay ms.
sets the delay of a sniff execution scheduled after a failure (in milliseconds)
creates the sniffer based on the provided configuration.
sets the interval between consecutive ordinary sniff executions in milliseconds. will be honoured when sniffonfailure is disabled or when there are no failures between consecutive sniff executions.
calls the elasticsearch nodes info api, parses the response and returns all the found http hosts
returns defaultvalue if the attribute didn't come back, either of those, or throws an ioexception if the attribute came back in a strange way.
creates a new instance of the elasticsearch sniffer. it will use the provided restclient to fetch the hosts through the nodes info api, the provided sniff request timeout value and scheme. that is also provided to sniffer#builder(restclient), so that the hosts are set to the same client that was used to sniff them. to elasticsearch. allows to halt the request without any failure, as only the nodes that have responded within this timeout will be returned.
tests the sniffer#sniff() method in isolation. verifies that it uses the nodessniffer implementation to retrieve nodes and set them (when not empty) to the provided restclient instance.
test behaviour when a bunch of onfailure sniffing rounds are triggered in parallel. each run will always schedule a subsequent afterfailure round. also, for each onfailure round that starts, the net scheduled round (either afterfailure or ordinary) gets cancelled.
test that sniffer#close() shuts down the underlying scheduler, and that such calls are idempotent. also verifies that the next scheduled round gets cancelled.
test multiple sniffing rounds by mocking the scheduler as well as the nodessniffer. simulates the ordinary behaviour of sniffer when sniffing on failure is not enabled. the countingnodessniffer doesn't make any network connection but may throw exception or return no nodes, which makes it possible to verify that errors are properly handled and don't affect subsequent runs and their scheduling. the scheduler implementation submits rather than scheduling tasks, meaning that it doesn't respect the requested sniff delays while allowing to assert that the requested delays for each requested run and the following one are the expected values.
here, we verify that the order that we add fields to a document counts, and not the lexi order of the field. this means that heavily accessed fields that use field selector should be added first (with load and break).
spawns the native controllers for each module.
attempt to spawn the controller daemon for a given module. the spawned process will remain connected to this jvm via its stdin, stdout, and stderr streams, but the references to these streams are not available to code outside this package.
returns first cause from a guice error (it can have multiple).
this method is invoked by elasticsearch#main(string[]) to startup elasticsearch.
creates a new instance
initialize native resources
add access to path (and all files underneath it); this also creates the directory if it does not exist.
add access to single file path
executes the bootstrap checks if the node has the transport protocol bound to a non-loopback interface. if the system property the transport protocol is bound to a non-loopback interface.
tests if the checks should be enforced.
executes the provided checks and fails the node if enforcelimits is true, otherwise logs warnings. if the system property es.enforce.bootstrap.checks is set to true then the bootstrap checks will be enforced regardless of whether or not the transport protocol is bound to a non-loopback interface.
main entry point for starting elasticsearch
checks that this path has no permissions
test generated permissions
checks exact file permissions, meaning those and only those for that path.
when a configured dir is a symlink, test that permissions work on link target
test generated permissions for all configured paths
adds a console ctrl handler.
try to install our custom rule profile into sandbox_init() to block execution
attempt to drop the capability to execute for the process.  this is best effort and os and architecture dependent. it may throw any throwable.
try to install our bpf filters via seccomp() or prctl() to block execution
we don't know which codesources belong to which plugin, so just remove the permission from key codebases like core, test-framework, etc. this way tests fail if accesscontroller blocks are missing.
return parsed classpath, but with symlinks resolved to destination files for matching this is for matching the torealpath() in the code where we have a proper plugin structure
add the codebase url of the given classname to the codebases map, if the class exists.
returns a read-only view of all system properties
sets properties (codebase urls) for policy files. we look for matching plugins and set urls to fit
return a map from codebase name to codebase url of jar codebases used by es core.
simple checks that everything is ok
adds access to all configurable paths.
reads and returns the specified policyfile.  jar files listed in codebases location will be provided to the policy file via a system property of the short name: e.g. $codebase.joda-convert-1.2.jar would map to full url.
add dynamic socketpermission based on transport settings. this method will first check if there is a port range specified in the transport profile specified by profilesettings and will fall back to settings.
adds access to classpath jarsclasses for jar hell scan, etc
add dynamic socketpermission for the specified port range.
initializes securitymanager for the environment can only happen once!
returns dynamic permissions to configured paths and bind ports
ensures configured directory path exists.
retrieves the short path form of the specified path.
retrieves the short path form of the specified path.
test restricting privileges to no permissions actually works
simplest case: a module with no controller daemon.
test with null location  its unclear whenif this happens, see https:bugs.openjdk.java.netbrowsejdk-8129972
test policy with null codesource.  this can happen when restricting privileges with doprivileged, even though protectiondomain's ctor javadocs might make you think that the policy won't be consulted.
inspect manifest for sure incompatibilities
checks that the java specification version targetversion required by resource is compatible with the current installation.
checks the set of urls for duplicate classes
checks the current classpath for duplicate classes
parses the classpath into a set of urls. for testing.
the "uwe schindler" algorithm.
return the cors setting as an array of origins. should never pass null, but we check for it anyway.
determine if cors setting is a regex
decodes a bit of an url encoded by a browser.  the string is expected to be encoded as per rfc 3986, section 2. this is the encoding used by javascript functions encodeuri and encodeuricomponent, but not escape. for example in this encoding,  (in unicode u+00e9 or in utf-8  this is essentially equivalent to calling  java.net.urldecoder urldecoder. java.net.urldecoder#decode(string, string) except that it's over 2x faster and generates less garbage for the gc. actually this function doesn't allocate any memory if there's nothing to decode, the argument itself is returned. be standardcharsets#utf_8. if the string to decode is null, returns an empty string. escape sequence.
get the content of the request or the contents of the source param or throw an exception if both are missing. prefer #contentorsourceparamparser() or #withcontentorsourceparamparserornull(checkedconsumer) if you need a parser.
creates a new rest request. the path is not decoded so this constructor will not throw a
if there is any content then call applyparser with the parser, otherwise do nothing.
returns a list of parameters that have not yet been consumed. this method returns a copy, callers are free to modify the returned list.
get the value of the header or null if not found. this method only retrieves the first header value if multiple values are sent. use of #getallheadervalues(string) should be preferred
call a consumer with the parser for the contents of this request if it has contents, otherwise with a parser for the source parameter if there is one, otherwise with null. use #contentorsourceparamparser() if you should throw an exception back to the user when there isn't request content.
get all values for the header or null if the header was not found
creates a new rest request. this method will throw badparameterexception if the path cannot be decoded
add a custom header.
registers a rest handler to be executed when the provided method and path match the request.
registers a rest handler to be executed when the provided method and path match the request, or when provided with deprecatedmethod and deprecatedpath. expected usage:  remove deprecation in next major release controller.registerwithdeprecatedhandler(post, "_forcemerge", this, post, "_optimize", deprecationlogger); controller.registerwithdeprecatedhandler(post, "index_forcemerge", this, post, "index_optimize", deprecationlogger);   the registered rest handler ( method with path) is a normal rest handler that is not deprecated and it is replacing the deprecated rest handler ( deprecatedmethod with deprecatedpath) that is using the same  deprecated rest handlers without a direct replacement should be deprecated directly using #registerasdeprecatedhandler and a specific message.
checks the request parameters against enabled settings for error trace support
dispatch the request, if possible, returning true if a response was sent or false otherwise.
handle requests to a valid rest endpoint using an unsupported http method. a 405 http response code is returned, and the response 'allow' header includes a list of valid http methods for the endpoint (see  href="https:tools.ietf.orghtmlrfc2616#section-10.4.6">http1.1 - 10.4.6 - 405 method not allowed).
handle a requests with no candidate handlers (return a 400 bad request error).
if a request contains content, this method will return true if the content-type header is present, matches an
get the valid set of http methods for a rest request.
registers a rest handler to be executed when one of the provided methods and path match the request.
handle http options requests to a valid rest endpoint. a 200 http response code is returned, and the response 'allow' header includes a list of valid http methods for the endpoint (see  href="https:tools.ietf.orghtmlrfc2616#section-9.2">http1.1 - 9.2 - options).
construct a channel for handling the request.
a channel level bytes output that can be reused. the bytes output is lazily instantiated by a call to #newbytesoutput(). once the stream is created, it gets reset on each call to this method.
creates a new xcontentbuilder for a response to be sent using this channel. the builder's type is determined by the following logic. if the request has a format parameter that will be used to attempt to map to an xcontenttype. if there is no format parameter, the http accept header is checked to see if it can be matched to a xcontenttype. if this first attempt to map fails, the request content type will be used if the value is not null; if the value is null the output format falls back to json.
add an additional method and handler for an existing path. note that methodhandlers does not allow replacing the handler for an already existing method.
for requests to a valid rest endpoint using an unsupported http method, verify that a 405 http response code is returned, and that the response 'allow' header includes a list of valid http methods for the endpoint (see  href="https:tools.ietf.orghtmlrfc2616#section-10.4.6">http1.1 - 10.4.6 - 405 method not allowed).
create a character array for characters [ from, to].
 usage is logged via the deprecationlogger so that the actual response can be notified of deprecation as well.
this does a very basic pass at validating that a header's value contains only expected characters according to rfc-5987, and those that it references.  https:tools.ietf.orghtmlrfc5987  this is only expected to be used for assertions. the idea is that only readable us-ascii characters are expected; the rest must be encoded with percent encoding, which makes checking for a valid character range very simple.
throw an exception if the value is not a #validheadervalue(string) valid header.
parses a top level query including the query element that wraps it
create the xcontent header for any basenodesresponse.
automatically transform the toxcontent-compatible, nodes-level response into a a bytesrestresponse.  this looks like:  "_nodes" : ... , "cluster_name" : "...", ... 
create the xcontent header for any basenodesresponse. this looks like:  "_nodes" : "total" : 3, "successful" : 1, "failed" : 2, "failures" : [ ... , ... ]  prefer the overload that properly invokes this method to calling this directly.
buildrecoverytable will build a table of recovery information suitable for displaying at the command line.
extracts all the required fields from the restrequest 'h' parameter. in order to support wildcards like 'bulk.' this needs potentially parse all the configured headers and its aliases and needs to ensure that everything is only added once to the returned headers, even if 'h=bulk..bulk.' is specified or some headers are contained twice due to matching aliases
helper method to find out if the only included fieldmapping metadata is typed null, which means that type and index exist, but the field did not
standard listener for extensions of listtasksresponse that supports group_by=nodes.
parses a multi-line restrequest body, instantiating a searchrequest for each line and applying the given consumer.
parses a restrequest body and returns a multisearchrequest
parses the rest request on top of the searchrequest, preserving values that are not overridden by the rest request. parameter
parses the rest request on top of the searchsourcebuilder, preserving values that are not overridden by the rest request.
read from a stream.
read a taskid from a stream. taskid has this rather than the usual constructor that takes a
stores the task failure
marks task as cancelled.  returns true if cancellation was successful, false otherwise.
bans all tasks with the specified parent task from execution, cancels all tasks that are currently executing.  this method is called when a parent task that has children is cancelled.
cancels a task  returns true if cancellation was started successful, null otherwise. after starting cancellation on the parent task, the task manager tries to cancel all children tasks of the current task. once cancellation of the children tasks is done, the listener is triggered.
stores the task result
returns a cancellable task with given id, or null if the task is not found.
returns a task with given id, or null if the task is not found.
marks task as finished.
returns the list of currently running tasks on the node
returns the list of currently running tasks on the node that can be cancelled
unregister the task
blocks the calling thread, waiting for the task to vanish from the taskmanager.
registers a task without parent task
read from a stream.
tests recovery of an index with or without a translog and the statistics we gather about that.
tests that a single document survives. super basic smoke test.
tests snapshotrestore by creating a snapshot and restoring it. it takes a snapshot on the old cluster and restores it on the old cluster as a sanity check and on the new cluster as an upgrade test. it also takes a snapshot on the new cluster and restores that on the new cluster as a test that the repository is ok with containing snapshot from both the old and new versions. all of the snapshots include an index, a template, and some routing configuration.
search on an alias that contains illegal characters that would prevent it from being created after 5.1.0. it should still be search-able though.
wait for an index to have green health, waiting longer than
test creating a trial license and using it. this is interesting because our other tests test cover starting a new cluster with the default distribution and enabling the trial license but this test is the only one that can upgrade from the oss distribution to the default distribution with xpack and the create a trial license. we don't do a lot with the trial license because for the most part those things are tested elsewhere, off in xpack. but we do use the trial license a little bit to make sure that it works.
test a basic feature (sql) which doesn't require any trial license. note that the test methods on this class can run in any order so we might have already installed a trial license.
has the master been upgraded to the new version?
this test verifies that as a cluster is upgraded incrementally, new documents eventually switch over to the "new" form of id (128 bit murmur3 ids). rollup ids are essentially the hashed concatenation of keys returned by the composite aggregation, so the field values that are being indexed (timestamp, value, etc) directly affect the id that is generated. we don't know which node will get the rollup task to start, so we don't know when it will migrate. the first doc is guaranteed to be the "old" style since all nodes are un-upgraded. the second and third phase will have a mixed cluster, and the rollup task may or may not migrate. in those phases we have two options (old and new) for the document added in the phase. the last phase is guaranteed to be new as it's a fully upgraded cluster.
constructor - parses the string key into it's name and modifier(s)
add the keyvalue that was found as result of the parsing
gets all the current matches. pass the results of this to isvalid to determine if a fully successful match has occured.
shared specification between beats, logstash, and ingest node
entry point to dissect a string into it's parts.
copy the contents of the given inputstream to the given outputstream. closes both streams when done.
deletes one or more files or directories (and everything underneath it).
closes all given closeables. if a non-null exception is passed in, or closing a stream causes an exception, throws the exception with other runtimeexception or
closes all given closeables, suppressing all thrown exceptions.
generates a random monitoringdoc.node
generates a random monitoringdoc with a given xcontenttype, bytesreference source,
generates a random monitoringdoc with a given xcontenttype, bytesreference source and monitoredsystem
returns the names of indices monitoring collects data from.
indicates if the current collector is allowed to collect data
creates a monitoringdoc.node from a discoverynode and a timestamp, copying over the required information.
returns the value of the collection timeout configured for the current collector.
compute an id that has the format: state_uuid:node_id || '_na':index:shard:'p' || 'r'
create a simple hash value that can be used to determine if the nodes listing has changed since the last report.
asserts that the monitoring document (provided as a map) contains the common information that all monitoring documents must have
assert that a indicesstatsmonitoringdoc contains the expected information
disable the monitoring service and the local exporter.
monitoring bulk api test: this test uses the monitoring bulk api to index document as an external application like kibana would do. it then ensure that the documents were correctly indexed and have the expected information.
monitoring service test: this test waits for the monitoring service to collect monitoring documents and then checks that all expected documents have been indexed with the expected information.
assert that a shardmonitoringdoc contains the expected information
assert that a clusterstatsmonitoringdoc contains the expected information
returns the searchhit content as a map object.
assert that a indexrecoverymonitoringdoc contains the expected information
executes the given runnable once the monitoring exporters are ready and functional. ensure that the exporters and the monitoring service are shut down after the runnable has been executed.
assert that a indexstatsmonitoringdoc contains the expected information
asserts that the source_node information (provided as a map) of a monitoring document correspond to the current local node information
assert that a nodestatsmonitoringdoc contains the expected information
test that monitoringdoc rendered using toxcontent#toxcontent(xcontentbuilder, toxcontent.params) contain a common set of fields.
asserts that the xcontent filters of a filteredmonitoringdoc contains the common filters and the expected custom ones.
flush the exporting bulk
add documents to the exporting bulk
close the exporting bulk
return all the settings of all the exporters, no matter if http or local
exports a collection of monitoring documents using the configured exporters
create a unique watch id and load the watchid resource by replacing variables, such as the cluster's uuid.
get any blacklisted cluster alerts by their id.
this test creates n threads that export a random number of document using a exporters instance.
perform publishablehttpresource#docheck(restclient) docheck against the resource that throws an exception and assert that it returns error.
perform publishablehttpresource#docheck(restclient) docheck against the resource that throws an exception and assert that it returns errpr when performing a delete rather than the more common get.
perform publishablehttpresource#dopublish(restclient) dopublish against the resource that throws an exception and assert that it returns false.
sets the httpasyncclientbuilder#setdefaultcredentialsprovider(credentialsprovider) credential provider,
check and, if necessary, publish this httpresource.  this will perform the check regardless of the #isdirty() dirtiness and it will update the dirtiness. using this directly can be useful if there is ever a need to double-check dirtiness without having to #markdirty() mark it as dirty.  if you do mark this as dirty while this is running (e.g., asynchronously something invalidates a resource), then the resource will still be dirty at the end, but the success of it will still return based on the checks it ran.
determine if the response contains a watch whose value  this expects a response like:  "metadata": "xpack": "version": 6000002 
determine if the current #watchid watch exists.
reach out to the remote cluster to determine the usability of watcher.
determine if watcher exists ( exists) or does not exist ( does_not_exist).
determine if x-pack is installed and, if so, if watcher is both available and enabled so that it can be used.  if it is not both available and enabled, then we mark that it exists so that no follow-on work is performed relative to watcher. we do the same thing if the current node is not the elected master node.
logs every error field's value until it hits the end of an array.
success is relative with bulk responses because unless it's rejected outright, it returns with a 200.  individual documents can fail and since we know how we're making them, that means that .
create a new httphost from the current scheme, host, and port.
set the port for the httphost.  specifying the port as -1 will cause it to be defaulted to 9200 when the httphost is built.
create a new httphost based on the supplied host.
create a new publishablehttpresource.
determine if the current resource should replaced the checked one based on its version (or lack thereof).  this expects a response like (where resourcename is replaced with its value):  "resourcename": "version": 6000002 
determine if the current resourcename exists at the resourcebasepath endpoint.  this provides the base-level check for any resource that cares about existence and also its contents. the response will only ever be null if none was returned.
upload the resourcename to the resourcebasepath endpoint.
determine if the current resourcename exists at the resourcebasepath endpoint with a version greater than or equal to the expected version.  this provides the base-level check for any resource that does not need to care about its response beyond existence (and likely does not need to inspect its contents).  this expects responses in the form of:  "resourcename": "version": 6000002 
delete the resourcename using the resourcebasepath endpoint.  note to callers: this will add an "ignore" parameter to the request so that 404 is not an exception and therefore considered successful if it's not found. you can override this behavior by specifying any valid value for "ignore", at which point 404 responses will result in false and logged failure.
ensure that the response contains a version that is version#onorafter(version) on or after the
verify that the minimum version is supported on the remote cluster.  if it does not, then there is nothing that can be done except wait until it does. there is no publishing aspect to this operation.
check and publish all #resources sub-resources.
if the node is not the elected master node, then it should never check watcher or send watches (cluster alerts).
sets the builder#setconnecttimeout(int) connect timeout and builder#setsockettimeout(int) socket timeout.
adds the resources necessary for checking and publishing cluster alerts.
adds a validator for the #ssl_setting to prevent dynamic updates when secure settings also exist within that setting groups (ssl context). because it is not possible to re-read the secure settings during a dynamic update, we cannot rebuild the ssliosessionstrategy (see #configuresecurity(restclientbuilder, config, sslservice) if this exporter has been configured with secure settings
adds the resources necessary for checking and publishing monitoring templates.
create an httpexporter.
create the httphosts that will be connected too.
create a sniffer from the http exporter's config for the client.
create a multihttpresource that can be used to block bulk exporting until all expected resources are available.
create a restclientbuilder from the http exporter's config.
configure the restclientbuilder to use initial connection and socket timeouts.
create the default parameters to use with bulk indexing operations.
determine if this httpexporter is ready to use.
configure the restclientbuilder to use credentialsprovider user authentication andor
creates the optional credentialsprovider with the usernamepassword to use with all requests for user authentication.
configures the restclientbuilder#setdefaultheaders(header[]) default headers to use with all requests.
adds the resources necessary for checking and publishing monitoring pipelines.
determine the scheme from the scheme.  scheme http = scheme.fromstring("http"); scheme https = scheme.fromstring("https"); scheme httpscaps = scheme.fromstring("https"); same as https 
when on the elected master, we setup all resources (mapping types, templates, and pipelines) before we attempt to run the exporter. if those resources do not exist, then we will create them.
install cluster alerts (watches) into the cluster
create the pipeline required to handle past data as well as to future-proof ingestion for current documents (the pipeline is initially empty, but it can be replaced later with one that translates it as-needed).  this should only be invoked by the elected master node.  whenever we eventually make a backwards incompatible change, then we need to override any pipeline that already exists that is older than this one. this uses the elasticsearch version, down to the alpha portion, to determine the version of the last change.  "description": "...", "pipelines" : [ ... ], "version": 6000001 
when not on the elected master, we require all resources (mapping types, templates, and pipelines) to be available before we attempt to run the exporter. if those resources do not exist, then it means the elected master's exporter has not yet run, so the monitoring cluster (this one, as the local exporter) is not setup yet.
determine if this localexporter is ready to use.
determine if the ingest pipeline for pipelineid exists in the cluster or not with an appropriate minimum version.
create a new localexporter. expected usage:  final settings settings = settings.builder().put("xpack.monitoring.exporters._local.type", "local").build(); try (localexporter exporter = createlocalexporter("_local", settings)) ... 
checks that the monitoring templates have been created by the local exporter
checks that the monitoring documents all have the cluster_uuid, timestamp and source_node fields and belongs to the right data or timestamped index.
checks that the monitoring ingest pipelines have been created by the local exporter
generates a basic template that loosely represents a monitoring template.
create a pipeline with nothing in it whose description is literally "test".
creates a monitoringbulkrequest with the given number of monitoringbulkdoc in it.
iterate over the list of monitoringbulkdoc to create the corresponding list of monitoringdoc.
create a monitoringdoc from a monitoringbulkdoc.
exports the documents
test that we allow strings to be "" because logstash 5.2 - 5.3 would submit empty _id values for time-based documents
creates a watcher history index from the specified version.
creates a monitoring timestamped index using a given template version.
cancelstop the cleaning service.  this will kill any scheduled #future from running. it's possible that this will be executed concurrently with the stopped.
set the global retention. this is expected to be used by the cluster settings to dynamically control the global retention time.  even if the current license prevents retention updates, it will accept the change so that they do not need to re-set it if they upgrade their license (they can always unset it).
get the retention that can be used.  this will ignore the global retention if the license does not allow retention updates.
reschedule the cleaner if the service is not stopped.
combines an msearch with rollup + live aggregations into a searchresponse representing the union of the two responses. the response format is identical to a non-rollup search response (aka a "normal aggregation" response). if the msearch response returns the following:  [ "took":228, "timed_out":false, "_shards":..., "hits":..., "aggregations": "histo": "buckets":[ "key_as_string":"2017-05-15t00:00:00.000z", "key":1494806400000, "doc_count":1, "the_max": "value":1.0 ] , "took":205, "timed_out":false, "_shards":..., "hits":..., "aggregations": "filter_histo": "doc_count":1, "histo": "buckets":[ "key_as_string":"2017-05-14t00:00:00.000z", "key":1494720000000, "doc_count":1, "the_max": "value":19995.0 , "histo._count": "value":1.0e9 ]  it would be collapsed into:  "took": 228, "timed_out": false, "_shards": ..., "hits": ..., "aggregations": "histo": "buckets": [ "key_as_string": "2017-05-14t00:00:00.000z", "key": 1494720000000, "doc_count": 1000000000, "the_max": "value": 19995 , "key_as_string": "2017-05-15t00:00:00.000z", "key": 1494806400000, "doc_count": 1, "the_max": "value": 1 ]  it essentially takes the conventions listed in rolluprequesttranslator and processes them so that the final product looks like a regular aggregation response, allowing it to be reducedmerged into the response from the un-rolled index
helper method which unrolls a generic multibucket agg. prefer to use the other overload as a consumer of the api
unrolls multibucket aggregations (e.g. terms, histograms, etc). this overload signature should be called by other internal methods in this class, rather than directly calling the per-type methods.
verifies a live-only search response. essentially just checks for failure then returns the response since we have no work to do
takes an aggregation with rollup conventions and unrolls into a "normal" agg tree
translates a rollup-only search response back into the expected convention. similar to has to deal with the rollup response (no live response) see #combineresponses(multisearchresponse.item[], internalaggregation.reducecontext) for more details on the translation conventions
takes an aggregation with rollup conventions and unrolls into a "normal" agg tree
generic method to help iterate over sub-aggregation buckets and recursively unroll
translates leaf aggs (minmaxsumetc) into their rollup version. for simple aggs like `min`, this is nearly a 1:1 copy. the source is deserialized into a new object, and the field is adjusted according to convention. e.g. for a `min` agg:  "the_min": "min" : "field" : "some_field"  the translation would be:  "the_min": "min" : "field" : "some_field.min.value"  however, for `avg` metrics (and potentially others in the future), the agg is translated into a sum + sum aggs; one for count and one for sum. when unrolling these will be combined back into a single avg. note that we also have to rename the avg agg name to distinguish it from empty buckets. e.g. for an `avg` agg:  "the_avg": "avg" : "field" : "some_field"  the translation would be:  [ "the_avg.value": "sum" : "field" : "some_field.avg.value" , "the_avg._count": "sum" : "field" : "some_field.avg._count" ]  the conventions are:  agg type: same as source agg named: same as the source agg field: `agg_type.field_name.value`  if the agg is an avgagg, the following additional conventions are added:  agg type: becomes sumagg, instead of avgagg named: source name.value additionally, an extra sumagg is added:   named: `source name._count` field: `field name.agg type._count`    most of the leafs to easily clone them
the generic method that does most of the actual heavy-lifting when translating a multi-bucket valuesourcebuilder. this method is called by all the agg-specific methods (e.g. translatedatehistogram()) for this particular agg but is passed downwards for leaf usage a date_histogram, the factory will take return a new datehistogramaggbuilder with matching parameters. it is not a deep clone however; the returned object won't have children set.
translates a non-rollup aggregation tree into a rollup-enabled agg tree. for example, the source aggregation may look like this:  post foo_rollup_search "aggregations": "the_histo": "date_histogram" : "field" : "ts", "interval" : "1d" , "aggs": "the_max": "max": "field": "foo"  which is then translated into an aggregation looking like this:  post rolled_foo_search "aggregations" : "filter_histo" : "filter" : "bool" : "must" : [ "term" : "_rollup.version" : 1 , "term": "ts.date_histogram.interval" : "1d" ] , "aggregations" : "the_histo" : "date_histogram" : "field" : "ts.date_histogram.timestamp", "interval" : "1d" , "aggregations" : "the_histo._count" : "sum" : "field" : "ts.date_histogram._count" , "the_max" : "max" : "field" : "foo.max.value"  the various conventions that are applied during the translation are elucidated in the comments of the relevant method below. require. deserialize into a stream for cloning of a single aggbuilder, since some aggregations (e.g. avg) may result in two translated aggs (sum + count)
translate a normal date_histogram into one that follows the rollup conventions. notably, it adds a sum metric to calculate the doc_count in each bucket. e.g. this date_histogram:  post foo_rollup_search "aggregations": "the_histo": "date_histogram" : "field" : "ts", "interval" : "day"  is translated into:  post rolled_foo_search "aggregations" : "the_histo" : "date_histogram" : "field" : "ts.date_histogram.timestamp", "interval" : "day" , "aggregations" : "the_histo._count" : "sum" : "field" : "ts.date_histogram._count"  the conventions are:  named: same as the source histogram field: `timestamp field.date_histogram.timestamp` add a sumaggregation to each bucket:   named: `parent histogram name._count` field: `timestamp field.date_histogram._count`   add a filter condition:   query type: termquery field: `timestamp_field.date_histogram.interval` value: `source interval`   
translate a normal histogram into one that follows the rollup conventions. notably, it adds a sum metric to calculate the doc_count in each bucket. conventions are identical to a date_histogram (excepting date-specific details), so see a complete list of conventions, examples, etc
translate a normal terms agg into one that follows the rollup conventions. notably, it adds metadata to the terms, and a sum metric to calculate the doc_count in each bucket. e.g. this terms agg:  post foo_rollup_search "aggregations": "the_terms": "terms" : "field" : "foo"  is translated into:  post rolled_foo_search "aggregations" : "the_terms" : "terms" : "field" : "foo.terms.value" , "aggregations" : "the_terms._count" : "sum" : "field" : "foo.terms._count"  the conventions are:  named: same as the source terms agg field: `field name.terms.value` add a sumaggregation to each bucket:   named: `parent terms name._count` field: `field name.terms._count`   
find the set of date_histo's with the largest granularity interval
find the set of histo's with the largest interval
ensure that the metrics are supported by one or more job caps. there is no notion of "best" caps for metrics, it is either supported or not.
ensure that the terms aggregation is supported by one or more job caps. there is no notion of "best" caps for terms, it is either supported or not.
given the aggregation tree and a list of available job capabilities, this method will return a set of the "best" jobs that should be searched. it does this by recursively descending through the aggregation tree and independently pruning the list of valid job caps in each branch. when a leaf node is reached in the branch, the remaining jobs are sorted by "best'ness" (see #getcomparator() for the implementation) and added to a global set of "best jobs". once all branches have been evaluated, the final set is returned to the calling code. job "best'ness" is, briefly, the job(s) that have - the larger compatible date interval - fewer and larger interval histograms - fewer terms groups note: the final set of "best" jobs is not guaranteed to be minimal, there may be redundant effort due to independent branches choosing jobs that are subsets of other branches.
lifted from estestcase :s don't reuse this anywhere! create a copy of an original searchsourcebuilder object by running it through a bytesstreamoutput and reading it in again using a writeable.reader. the stream that is wrapped around the streaminput potentially need to use a namedwriteableregistry, so this needs to be provided too
check to see if the persistenttask's cluster state contains the rollup job(s) we are interested in
helper method used by startstop transportactions so that we can ensure only one task is invoked, or none at all. should not end up in a situation where there are multiple tasks with the same id... but if we do, this will help prevent the situation from getting worse.
the only entry point in this class. you hand this method an aggregation and an index pattern, and it returns a list of rolled documents that you can index
this is called when the persistent task signals that the allocated task should be terminated. termination in the task framework is essentially voluntary, as the allocated task can only be shut down from the inside.
attempt to gracefully cleanup the rollup job so it can be terminated. this tries to remove the job from the scheduler, and potentially any other cleanup operations in the future
this is called by the scheduleengine when the cron triggers.
attempt to stop the indexer if it is idle or actively indexing. if the indexer is aborted this will fail with an exception. note that stopping the job is not immediate. it updates the persistent task's status, but then the allocated task has to notice and stop itself (which may take some time, depending on where in the indexing cycle it is). this method will, however, return as soon as the persistent task has acknowledge the status update.
attempt to start the indexer. if the state is anything other than stopped, this will fail. otherwise, the persistent task's status will be updated to reflect the change. note that while the job is started, the indexer will not necessarily run immediately. that will only occur when the scheduler triggers it based on the cron
sets the internal state to indexerstate#stopping if an async job is running in the background and in such case this function is called, the state is directly set to indexerstate#stopped and #onfinish() will never be called.
creates a skeleton compositeaggregationbuilder from the provided job config.
checks the indexerstate and returns false if the execution should be stopped.
creates the range query that limits the search to documents that appear before the maximum allowed time (see #maxboundary and on or after the last processed time.
ctr
triggers a background job that builds the rollup index asynchronously iff there is no other job that runs and the indexer is started ( indexerstate#started.
executes a rollup test case
creates mappedfieldtype from the provided job. for simplicity all numbers are considered as longs.
read an address for elasticsearch suitable for the cli from the system properties.
send a command and assert the echo.
shutdown the connection to the remote cli without attempting to shut the remote down in an orderly way.
create the "echo" that we expect jline to send to the terminal while we're typing a command.
attempts an orderly shutdown of the cli, reporting any unconsumed lines as errors.
test the hijacking a scroll fails. this test is only implemented for rest because it is the only api where it is simple to hijack a scroll. it should excercise the same code as the other apis but if we were truly paranoid we'd hack together something to test the others as well.
tests count against index on a node that doesn't have any shards of the index.
builds that map that is returned in the header for each column.
run sql as text using the format parameter to specify the format rather than an accept header.
run sql as text using the accept header to specify the format rather than the format parameter.
wraps csv in the expectedresults into csv connection. use #executecsvquery to obtain resultset from this connection
executes a query on provided csv connection.  the supplied table name is only used for the test identification.
the properties used to build the connection.
read an address for elasticsearch suitable for the jdbc driver from the system properties.
tests that a rollup job created on a old cluster is correctly restarted after the upgrade.
tests that a single document survives. super basic smoke test.
tests that defends against scrolls broken in such a way that the remote elasticsearch returns infinite results. while elasticsearch shouldn't do this it has in the past and it is very when it does. it takes out the whole node. so this makes sure we defend against it properly.
if a pki realm is enabled, checks to see if ssl and client authentication are enabled on at least one network communication layer.
test if the node fails the check.
test if the node fails the check.
test if the node fails the check.
get the setting setting configuration for all security components, including those defined in extensions.
all tests run as a an administrative user but use es-security-runas-user to become a less privileged user.
a multi-stage test that: - create a new ca - uses that ca to create 2 node certificates - creates a 3rd node certificate using an auto-generated ca - checks that the first 2 node certificates trust one another - checks that the 3rd node certificate is _not_ trusted - checks that all 3 certificates have the right values based on the command line options provided during generation
checks whether there are keys in keystore that are trusted by truststore.
a multi-stage test that: - creates a zip of a pkcs12 cert, with an auto-generated ca - uses the generate ca to create a pem certificate - checks that the pkcs12 certificate and the pem certificate trust one another
parses the input file to retrieve the certificate information
this method handles writing out the certificate authority cert and private key if the certificate authority was generated by this invocation of the tool
returns the ca certificate and private key that will be used to sign certificates. these may be specified by the user or automatically generated
generates certificate signing requests and writes them out to the specified file in zip format
this method handles the collection of information about each instance that is necessary to generate a certificate. the user may be prompted or the information can be gathered from a file
checks for output file in the user specified options or prompts the user for the output file
this method handles the deletion of a file in the case of a partial write
generates signed certificates in pem format stored in a zip file
helper method to read a private key and support prompting of user for a key. to avoid passwords being placed as an argument we can prompt the user for their password if we encounter an encrypted key.
converts the inetaddress objects into a generalnames object that is used to represent subject alternative names.
generates a certificate signing request
based on the private key algorithm privatekey#getalgorithm() determines default signing algorithm used by certgenutils
generates a rsa key pair with the provided key size (in bits)
generates a signed certificate distinguished name (dn) certificate as an x509v3 extension. may be null certificate certificate empty, then use default algorithm certgenutils#getdefaultsignaturealgorithm(privatekey)
gets a random serial for a certificate that is generated from a securerandom
helper method to read a private key and support prompting of user for a key. to avoid passwords being placed as an argument we can prompt the user for their password if we encounter an encrypted key.
this method handles the deletion of a file in the case of a partial write
parses the input file to retrieve the certificate information
this method handles writing out the certificate authority in pkcs#12 format to a zip file.
generates certificate signing requests and writes them out to the specified file in zip format
checks whether the parent directories of path exist, and offers to create them if needed.
this method handles writing out the certificate authority in pem format to a zip file.
this method handles the collection of information about each instance that is necessary to generate a certificate. the user may be prompted or the information can be gathered from a file
this method handles the deletion of a file in the case of a partial write
returns the ca certificate and private key that will be used to sign certificates. these may be specified by the user or automatically generated
generates signed certificates in either pkcs#12 format or pem format, wrapped in a zip file if necessary.
wildcards in the request are treated as does the user have ___ privilege on every possible index that matches this pattern? or, expressed differently, does the user have ___ privilege on a wildcard that covers (is a superset of) this pattern?
this tests that the action responds correctly when the userrole has some, but not all of the privileges being checked.
this tests that the action responds correctly when the userrole has none of the privileges being checked.
this tests that action names in the request are considered "matched" by the relevant named privilege (in this case that deleteaction and indexaction are satisfied by indexprivilege#write).
tests that request types that do not support remote indices will be resolved as if all index names are local.
tests that all the request types that are known to support remote indices successfully pass them through the resolver
verifies that the behaviour tested in #testuserwithnorolescanperformremotesearch does not work for requests that are not remote-index-capable.
this test mimics #testuserwithnorolescanperformremotesearch() except that while the referenced index _looks_ like a remote index, the remote cluster name has not been defined, so it is actually a local index and access should be denied
this test mimics #testuserwithnorolescannotperformlocalsearch() but includes both local and remote indices, including wildcards
adds the authentication to the scrollcontext
compares the authentication that was stored in the scrollcontext with the current authentication. we cannot guarantee that all of the details of the authentication will be the same. some things that could differ include the roles, the name of the authenticating (or lookup) realm. to work around this we compare the username and the originating realm type.
checks for the scrollcontext if it exists and compares the authentication object from the scroll context with the current authentication context
this method is used to determine if a request should be executed as the system user, even if the request already has a user associated with it. in order for the user to be replaced by the system user one of the following conditions must be true:  the action is an internal action and no user is associated with the request the action is an internal action and the thread context contains a non-internal action as the originating action 
stashes the current context and executes the consumer as the proper user based on the origin of the action. this method knows nothing about listeners so it is important that callers ensure their listeners preserve their context and restore it appropriately.
returns true if the thread context contains the origin of the action and does not have any authentication
verifies that the given user can execute the given request (and action). if the user doesn't have the appropriate privileges for this actionrequest, an elasticsearchsecurityexception will be thrown.
performs authorization checks on the items within a bulkshardrequest. this inspects the bulkitemrequest items within the request, computes an implied action for each item's docwriterequest#optype(), and then checks whether that action is allowed on the targeted index. items that fail this checks are bulkitemrequest#abort(string, exception) aborted, with an denied exception. because a shard level request is for exactly 1 index, and there are a small number of possible item docwriterequest.optype types, the number of distinct authorization checks that need to be performed is very small, but the results must be cached, to avoid adding a high overhead to each bulk request.
special handling of the value to authorize for a put mapping request. dynamic put mapping requests use a concrete index, but we allow permissions to be defined on aliases so if the request's concrete index is not in the list of authorized indices, then we need to look to see if this can be authorized against an alias
resolves, and if necessary updates, the list of index names in the provided request in accordance with the user's authorizedindices.  wildcards are expanded at this phase to ensure that all security and execution decisions are made against a fixed set of index names that is consistent and does not change during the life of the request.   if the provided request is of a type that #allowsremoteindices(indicesrequest) allows remote indices, then the index names will be categorized into those that refer to resolvedindices#getlocal() local indices, and those that refer to resolvedindices#getremote() remote indices. this categorization follows the standard  thus an index name n will considered to be remote if-and-only-if all of the following are true  request supports remote indices  n is in the format cluster:index. it is allowable for cluster and index to contain wildcards, but the separator (:) must be explicit.  cluster matches one or more remote cluster names that are registered within this cluster.  in which case, any wildcards in the cluster portion of the name will be expanded and the resulting remote-index-name(s) will be added to the remote index list.  otherwise, n will be added to the local index list.
this test is a direct result of a issue where field level security permissions were not being merged correctly. the improper merging resulted in an allow all result when merging permissions from different roles instead of properly creating a union of their languages
this test is mainly to make sure we can read the default roles.yml config
retrieve a list of roles, if rolestoget is null or empty, fetch all roles
returns true if its safe to use the query cache for this query.
populates fields with the set of fields used by the query, or throws unsupportedoperationexception if it doesn't know how to do this.
adds the index alias .security to the underlying concrete index.
it is a requirement that empty string filters match empty string fields. in this case we require automatons from empty string to match the empty string. `automatons.predicate("").test("") == false` `automatons.predicate("").test("") == true`
if a field is missing for an event, its value for filtering purposes is the empty string or a singleton stream of the empty string. this a allows a policy to filter by the missing value using the empty string, ie `ignore_filters.users: ["", "elastic"]` will filter events with a missing user field (such as `anonymous_access_denied`) as well as events from the "elastic" username.
starts the service. the state is moved to org.elasticsearch.xpack.security.audit.index.indexaudittrail.state#starting at the beginning of the method. the service's components are initialized and if the current node is the master, the index template will be stored. the state is moved org.elasticsearch.xpack.security.audit.index.indexaudittrail.state#started and before returning the queue of messages that came before the service started is drained.
atomically writes to the specified file a line per entry in the specified map using the specified transform to convert each entry to a line. the writing is done atomically in the following sense: first the lines are written to a temporary file and if the writing succeeds then the temporary file is moved to the specified path, replacing the file if it exists. if a failure occurs, any existing file is preserved, and the temporary file is cleaned up.
prepares the index by creating it if it doesn't exist or updating the mappings if the mappings are out of date. after any tasks have been executed, the runnable is then executed.
returns the routing-table for this index, or null if the index does not exist.
resolves a concrete index name or alias to a indexmetadata instance. requires that if supplied with an alias, the alias resolves to at most one concrete index.
check if attributes of the paths have changed, warning to the given terminal if they have.
create a checker for the given paths, which will warn to the given terminal if changes are made.
extracts the rest remote address from the message context. if not found, returns null. transport messages that were created by rest handlers, should have this in their context.
check whether the given request is allowed within the current license state and setup, and return the name of any unlicensed feature. by default this returns an exception if security is not available by the current license or security is not enabled. sub-classes can override this method if they have additional requirements. sent to the requestor
calls the #innerpreparerequest(restrequest, nodeclient) method and then checks the license state. if the license state allows auth, the result from response will be returned indicating that security is not licensed. note: the implementing rest handler is called before the license is checked so that we do not trip the unused parameters check
refreshes the current in-use metadata.
creates an elasticsearchsecurityexception that indicates the token was expired. it is up to the client to re-authenticate and obtain a new token. the format for this response is defined in  href="https:tools.ietf.orghtmlrfc6750#section-3.1">
create a token based on the provided authentication and metadata. the created token will be stored in the security index.
creates an elasticsearchsecurityexception that indicates the request contained an invalid grant
this method performs the steps necessary to invalidate a token so that it may no longer be used.
rotate the current active key to the spare key created in the previous #generatesparekey() call.
this method performs the steps necessary to invalidate a token so that it may no longer be used. the process of invalidation involves a step that is needed for backwards compatibility with versions prior to 6.2.0; this step records an entry to indicate that a token with a given id has been expired. the second step is to record the invalidation for tokens that have been created on versions on or after 6.2; this step involves performing an update to the token document and setting the invalidated field to true
checks if the token has been stored as a revoked token to ensure we do not allow tokens that have been explicitly cleared.
uses the refresh token to refresh its associated token and returns the new token with an updated expiration date to the listener
performs checks on the retrieved source and returns an optional with the exception if there is an issue
looks in the context to see if the request provided a header with a user token and if so the token is validated, which includes authenticated decryption and verification that the token has not been revoked or is expired.
gets the token from the authorization header if the header begins with bearer 
performs the actual invalidation of a token
performs the actual bwc invalidation of a token and then kicks off the new invalidation method
find all stored refresh and access tokens that have not been invalidated or expired, and were issued against the specified realm.
creates an elasticsearchsecurityexception that indicates the token was expired. it is up to the client to re-authenticate and obtain a new token. the format for this response is defined in  href="https:tools.ietf.orghtmlrfc6750#section-3.1">
performs the actual refresh of the token with retries in case of certain exceptions that may be recoverable. the refresh involves retrieval of the token document and then updating the token document to indicate that the document has been refreshed.
creates a new token service
creates a new key unless present that is newer than the current active key and returns the corresponding metadata. note: this method doesn't modify the metadata used in this token service. see #refreshmetadata(tokenmetadata)
reads the authentication and metadata from the given token. this method does not validate whether the token is expired or not.
prunes the keys and keeps up to the latest n keys around
generates a secret key based off of the provided password and salt. this method is computationally expensive.
serializes a token to a string containing an encrypted representation of the token
creates a new token based on the values from the stream
iterates over the realms and attempts to lookup the run as user by the given username. the consumer will be called regardless of if the user is found or not, with a non-null user. we do not fail requests if the run as user is not found as that can leak the names of users that exist using a timing attack
consumes the user that resulted from attempting to authenticate a token against the realms. when the user is functionality is in use. when run as is not in use, #finishauthentication(user) is called, otherwise we try to lookup the run as user in #lookuprunasuser(user, string, consumer)
consumes the authenticationtoken provided by the caller. in the case of a null token, #handlenulltoken() is called. in the case of a non-null token, the realms are iterated over and the first realm that returns a non-null if no exception was caught while trying to authenticate the token
finishes the authentication process by ensuring the returned user is enabled and that the run as user is enabled if there is one. if authentication is successful, this method also ensures that the authentication is written to the threadcontext
writes the authentication to the threadcontext and then calls the listener if successful
looks to see if the request contains an existing authentication and if so, that authentication will be used. the consumer is called if no exception was thrown while trying to read the authentication and may be called with a null value
determines whether type is an internal realm-type that is provided by x-pack, including the reservedrealm
creates realm.factory factories for each internal realm type. this excludes the reservedrealm, as it cannot be created dynamically.
finds all values for the specified attribute
constructs a saml specific exception with a consistent message regarding saml signature validation failures
tests whether the provided function returns true for any of the idp's signing credentials.
processes the provided logoutrequest and extracts the nameid and sessionindex. returns these in a samlattributes object with an empty attributes list.  the recommended binding for logout (for maximum interoperability) is http-redirect. under this binding the signature is applied to the query-string (including parameter names and url-encodedbase64-encodeddeflated values). therefore in order to properly validate the signature, this method operates on a raw query- string.
map of saml-attribute name to configuration-setting name
the spec states that this should never happen (saml bindings spec, v2.0 wd6, section 3.4.4.1, point 3). onelogin includes newlines anyway.
creates a saml org.opensaml.saml.saml2.core.logoutresponse to the provided requestid
factory for saml realm. this is not a constructor as it needs to initialise a number of components before delegating to
creates a saml logoutrequest single logout request for the provided session, if the realm and idp configuration support slo. otherwise returns null
this is testing a test, but the real encryption tests are useless if our encryption routines don't do anything
randomly selects digital signature algorithm uri for given private key algorithm ( privatekey#getalgorithm()).
reads a key pair and associated certificate for given algorithm and key length for testing, for "ec" algorithm 256 key size is used, others use 2048 as default.
request a named attribute be provided as part of assertions. specified in a attributeconsumingservice
constructs an entitydescriptor that contains a single spssodescriptor.
tests that a user can login via a saml idp: it uses:  a real idp (shibboleth, running locally) a fake ui, running in this jvm, that roughly mimic kibana (see #httplogin, #httpacs) a fake web browser (apache http client)  it takes the following steps:  requests a "login" on the local ui walks through the login process at the idp receives a json response from the local ui that has a bearer token uses that token to verify the user details 
finds the target url and samlresponse for the html form from the provided content.
this is a simple mapping that maps the "thor" user in the "shibboleth" realm to the "kibana_users" role. we could do something more complex, but we have unit tests for role-mapping - this is just to verify that the mapping runs ok in a real environment.
we perform all requests to elasticsearch as the "kibana" user, as this is the user that will be used in a typical saml deployment (where kibana is providing the ui for the saml web sso interactions). before we can use the kibana user, we need to set its password to something we know.
verifies that the provided "refresh token" (see org.elasticsearch.xpack.security.authc.tokenservice) can be used to get a new valid access token and refresh token.
submits a saml assertion to the acs uri.
submits a shibboleth consent form to the provided uri. the consent form is a step that shibboleth inserts into the login flow to confirm that the user is willing to send their personal details to the application (sp) that they are logging in to.
provides the "login" handler for the fake webapp. this interacts with elasticsearch (using the rest client) to find the login page for the idp, and then sends a redirect to that page.
provides the "assertion-consumer-service" handler for the fake webapp. this interacts with elasticsearch (using the rest client) to perform a saml login, and just forwards the json response back to the client.
navigates to the login page on the local (in memory) http ui.
wraps a httphandler in a try-catch block that returns a 500 server error if an exception or an assertionerror occurs.
verifies that the provided "access token" (see org.elasticsearch.xpack.security.authc.tokenservice) is for the expected user with the expected name and roles.
submits a shibboleth login form to the provided uri.
processes the provided saml response within the provided token and, if valid, extracts the relevant attributes from it.
constructs an exception that can be distinguished (via #issamlexception as a saml specific exception used to distinguish "expected" exceptions (such as saml signature failures, or missing attributes) that should be treated as a simple authentication failure (with a clear cause)
enabling schema validation with `setvalidating(true)` in our documentbuilderfactory requires that we provide our own errorhandler implementation
this is needed in order to initialize the underlying opensaml library. it must be called before doing anything that potentially interacts with opensaml (whether in server code, or in tests). the initialization happens within do privileged block as the underlying apache xml security library has a permission check. the initialization happens with a specific context classloader as opensaml loads resources from its jar file.
constructs a documentbuilder with all the necessary features for it to be secure
enabling a user forces a doc to be written to the security index, and "user doc with empty password" has a special case code in the reserved realm.
asynchronous method to put a user. a put user request without a password hash is treated as an update and will fail with a upsert document as well; the upsert document sets the enabled flag of the user to true but if the document already exists, this method will not modify the enabled value.
retrieve a list of users, if usernames is null or empty, fetch all users
async method to change the password of a native or reserved user. if a reserved user does not exist, the document will be created with a hash of the provided password.
async method to retrieve a user and their password
handles updating a user that should already exist where their password should not change
asynchronous method to create a reserved user with the given password hash. the cache for the user will be cleared after the document has been indexed
tests that multiple concurrent run as requests can be authenticated successfully. there was a bug in the cache implementation used for our internal realms that caused some run as requests to fail even when the authentication was valid and the run as user existed. the issue was that when iterating the realms there would be failed lookups and under heavy concurrency, requests will wait for an existing load attempt in the cache. the original caller was thrown an executionexception with a nested nullpointerexception since the loader returned a null value, while the other caller(s) would get a null value unexpectedly
creates a new logger that is detached from the root logger and only has an appender that will output log messages to the terminal
general purpose http(s) call with json content-type and authorization header. ssl settings are read from the settings file, if any. user in the authorization header. password in the authorization header. supplier for the json string body of the request. handler of the response input stream.
collects passwords for all the users, then issues set requests. fails on the first failed request. in this case rerun the tool to redo all the operations.
validates the bootstrap password from the local keystore by making an '_authenticate' call. returns silently if server is reachable and password is valid. throws userexception otherwise.
sets one user's password using the elastic superuser credentials.
parses the users file. returns null, if the users file does not exist.
internally in this class, we try to load the file, but if for some reason we can't, we're being more lenient by logging the error and skipping all users. this is aligned with how we handle other auto-loaded files in security.
internally in this class, we try to load the file, but if for some reason we can't, we're being more lenient by logging the error and skipping all enries. this is aligned with how we handle other auto-loaded files in security.
accepts a mapping of user -> list of roles
parses the users_roles file. returns null if the users_roles file does not exist. the read file holds a mapping per line of the form "role -> users" while the returned map holds entries of the form "user -> roles".
checks the user exists with the given password
checks the role has the given users, or that the role does not exist if not users are passed.
checks the user does not exist in the users or users_roles files
tests searching for groups when the "user_attribute" field is set to "dn" (which is special)
tests that a client-side timeout in the asynchronous ldap sdk is treated as a failure, rather than simply returning no results.
tests searching for groups when the "user_attribute" field is set to a missing value
tests searching for groups when the "user_attribute" field is not set
creates a realm with the provided settings, rebuilds the ssl service to be aware of the new realm, and then returns the realmconfig
wraps the provided sessionlistener to preserve the threadcontext associated with the current thread. responses headers are not preserved, as they are not needed. response output should not yet exist, nor should any be produced within the realmldap-session.
given a username and password, open a connection to ldap, bind to authenticate, retrieve groups, map to roles and build the user. this user will then be passed to the listener
if the execution of this runnable has not already started, the runnable is cancelled and we pass an exception to the user listener
sets up a ldapsession using the connection pool that potentially holds existing connections to the server
sets up a ldapsession using the following process:  opening a new connection to the ldap server executes a bind request using the bind user executes a search to find the dn of the user closes the opened connection opens a new connection to the ldap server executes a bind request using the found dn and provided password creates a new ldapsession with the bound connection 
the contract for realm implementations is that they should log-and-return-null (and not call actionlistener#onfailure(exception)) if there is an internal exception that prevented them from performing an authentication. this method tests that when an ldap server is unavailable (invalid hostname), there is a null result rather than an exception.
securely escapes the username and inserts it into the template using messageformat
this iterates through the configured user templates attempting to open. if all attempts fail, the last exception is kept as the cause of the thrown exception
creates the connection pool that will be used by the session factory and initializes the health check support
this method is used to cleanup the connection pool if one is being used
this method performs an asynchronous ldap search operation that could have multiple results
this method performs an asynchronous ldap search operation that only expects at most one result. if more than one result is found then this is an error if no results are found, then null will be returned. if the ldap server returns an error resultcode then this is handled as a
performs the actual connection and following of a referral given a url string. this referral is being followed as it may contain a result that is relevant to our search
this method performs an asynchronous ldap search operation that could have multiple results
this method performs an asynchronous ldap search operation that only expects at most one result. if more than one result is found then this is an error. if no results are found, then null will be returned. if the ldap server returns an error resultcode then this is handled as a
returns true if the provide searchresult was successfully completed by the server. note: referrals are not considered a successful response for the purposes of this method.
this method performs an asynchronous ldap search operation that could have multiple results
if necessary, fork before executing the runnable. a deadlock will happen if the same thread which handles bind responses blocks on the bind call, waiting for the response which he itself should handle.
this method performs an asynchronous ldap search operation that could have multiple results
this method submits the bind request over one connection from the pool. the bind authentication is then reverted and the connection is returned to the pool, so that the connection can be safely reused, see bind credentials. bind calls are blocking and if a bind is executed on the ldap connection reader thread (as returned by ldaputils#isldapconnectionthread), the thread will be blocked until it is interrupted by something else such as a timeout timer. do not call bind outside this method or the ldap connection pool on which to submit the bind operation. the request object of the bind operation. the threads that will call the blocking bind operation, in case the calling thread is a connection reader, see: the runnable that continues the program flow after the bind operation. it is executed on the same thread as the prior bind.
this method submits the bind request over the ldap connection. its authentication status changes. the connection can be subsequently reused. this validates the bind credentials. bind calls are blocking and if a bind is executed on the ldap connection reader thread (as returned by ldaputils#isldapconnectionthread), the thread will be blocked until it is interrupted by something else such as a timeout timer. do not call bind outside this method or the ldap connection on which to submit the bind operation. the request object of the bind operation. the threads that will call the blocking bind operation, in case the calling thread is a connection reader, see: the runnable that continues the program flow after the bind operation. it is executed on the same thread as the prior bind.
validates client kerberos ticket received from the peer.  first performs service login using keytab, supports multiple principals in keytab and the principal is selected based on the request.  the gss security context establishment state is handled as follows:  if the context is established it will call actionlistener#onresponse with a tuple of username and outtoken for peer reply.  if the context is not established then it will call with a outtoken that needs to be sent to peer for further negotiation.  never calls actionlistener#onresponse with a null tuple.  on failure, it will call actionlistener#onfailure(exception) service.
performs authentication using provided keytab closed using logincontext#logout() after usage.
privileged wrapper for closing logincontext, does not throw exceptions but logs them as a debug message.
privileged wrapper that invokes action with subject.doas to perform work as given subject. subject.doas
privileged wrapper for closing gsscontext, does not throw exceptions but logs them as a debug message.
handles server response and returns new token if any to be sent to server. gss negotiation nothing to be sent.
performs authentication using provided principal name and password for client closed logincontext#logout() after usage.
gsscontext initiator side handling, initiates context establishment and returns the base64 encoded token to be sent to server.
creates spengoclient to interact with given service principal use #close() to logout logincontext and dispose interacts with.
creates principals and exports them to the keytab created in the directory.
build kerberos realm settings
privileged wrapper that invokes action with subject.doas to perform work as given subject. subject.doas subject and action see
if logged in logincontext is not available, it attempts login and returns logincontext
creates multiple principals in the kdc and adds them to a keytab file. exists and then always appends to it. created.
constructor for simplekdcldapserver, creates instance of kdc server and ldap backend server. also initializes and starts them with provided configuration.  to stop the kdc and ldap server use #stop() backend.conf and kdc.ldiff
stop simple kdc server
creates simple mapping that maps the users from 'kerberos' realm to the 'kerb_test' role.
usually principal names are in the form 'userinstance@realm'. this method removes '@realm' part from the principal name if will return the input string.
creates elasticsearchsecurityexception with 'www-authenticate' header with value as 'negotiate' scheme.
extract token from authorization header and if it is valid @value #negotiate_auth_header_prefix then returns start with @value #negotiate_auth_header_prefix else returns valid
sets 'www-authenticate' header if outtoken is not null on passed instance of if outtoken is provided and is not null or empty, then that is appended to 'negotiate ' and is used as header value for header 'www-authenticate' sent to the peer in the form 'negotiate oyh1mihyoamk...'. this is required by client for gss negotiation to continue further. sent to the peer. 'www-authenticate' header populated.
usually principal names are in the form 'userinstance@realm'. this method removes '@realm' part from the principal name if will return the input string.
formats the user data as a expressionmodel. the model does not have nested values - all values are simple java values, but keys may contain .. for example, the #metadata values will be stored in the model with a key of "metadata.key" where key is the key from the metadata object.
internally in this class, we try to load the file, but if for some reason we can't, we're being more lenient by logging the error and skippingremoving all mappings. this is aligned with how we handle other auto-loaded files in security.
this will map the groupdn's to es roles
if the user exists in the cache (keyed by the principle name), then the password is validated against a hash also stored in the cache. otherwise the subclass authenticates the user via doauthenticate
loads all mappings from the index. package private for unit testing
provides usage statistics for this store. the resulting map contains the keys  size - the total number of mappings stored in the index enabled - the number of mappings that are 
retrieves one or more mappings from the index. if names is null or set#isempty empty, then this retrieves all mappings. otherwise it retrieves the specified mappings by name.
requests a tls renegotiation. this means the we will request that the peer performs another handshake prior to the continued exchange of application data. this can only be requested if we are currently in application mode.
uses a transport client with ssl disabled. this test connects to the client profile, which should always fail as a non-ssl transport client cannot connect to a ssl profile
uses a transport client that only trusts the testnode certificate. this test connects to the client profile, which uses the testnode certificate and requires the client to present a certificate, so this connection will never work as the client has no certificate to present
on each node sets up the following profiles:  default: testnode keypair. requires client auth client: testnode-client-profile profile that only trusts the testclient cert. requires client auth no_client_auth: testnode keypair. does not require client auth 
uses a transport client that only trusts the testnode certificate. this test connects to the no_client_auth profile, which uses the testnode certificate and does not require to present a certificate, so this connection should always succeed
uses a transport client with the default jdk truststore; this truststore only trusts the known good public certificate authorities. this test connects to the default profile, which uses a self-signed certificate that will never be trusted by the default truststore so the connection should always fail
uses a transport client with a custom key pair; transportclient only trusts the testnode certificate and had its own self signed certificate. this test connects to the no_client_auth profile, which uses a truststore that does not trust the testclient-client-profile certificate but does not require client authentication
uses a transport client with the same settings as the internal cluster transport client to test connection to the no_client_auth profile. the internal transport client is not used here since we are connecting to a different profile. since the no_client_auth profile does not require client authentication, the standard transport client connection should always succeed as the settings are the same as the default profile except for the port and disabling the client auth requirement
uses a transport client that only trusts the testnode certificate. this test connects to the default profile, which uses the testnode certificate and requires the client to present a certificate, so this connection will never work as the client has no certificate to present
uses a transport client with ssl disabled. this test connects to the no_client_auth profile, which should always fail as a non-ssl transport client cannot connect to a ssl profile
uses a transport client with the same settings as the internal cluster transport client to test connection to the client profile. the internal transport client is not used here since we are connecting to a different profile. the client profile requires client auth and only trusts the certificate in the testclient-client-profile keystore so this connection will fail as the certificate presented by the standard transport client is not trusted by this profile
uses a transport client with ssl disabled. this test connects to the default profile, which should always fail as a non-ssl transport client cannot connect to a ssl profile
uses a transport client with the default jdk truststore; this truststore only trusts the known good public certificate authorities. this test connects to the no_client_auth profile, which uses a self-signed certificate that will never be trusted by the default truststore so the connection should always fail
uses a transport client with the default jdk truststore; this truststore only trusts the known good public certificate authorities. this test connects to the client profile, which uses a self-signed certificate that will never be trusted by the default truststore so the connection should always fail
uses a transport client with a custom key pair; transportclient only trusts the testnode certificate and had its own self signed certificate. this test connects to the client profile, which is only set to trust the testclient-client-profile certificate so the connection should always succeed
instantiates a new pattern rule.
integer.
get the subnet's netmask in decimal format. i.e.: getnetmask("255.255.255.0") returns the integer cidr mask
runs a test on an actual class implementing a custom interface and should implement the expected marker interface if and only if the specified violation parameter is false.
loads a class from the specified input stream and checks that if it implements a feature-aware custom then it extends the appropriate mix-in interface from x-pack. if the class does not, then the specified callback is invoked.
check the class directories specified by the arguments for classes in x-pack that implement customs but do not extend the appropriate marker interface that provides a mix-in implementation of clusterstate.featureaware#getrequiredfeature().
delete any left over machine learning datafeeds and jobs.
executes an api call using the admin context, waiting for it to succeed.
disable monitoring
enable monitoring and waits for monitoring documents to be collected and indexed in monitoring indices.this is the signal that the local exporter is started and ready for the tests.
waits for the security template and the machine learning templates to be created by the metadataupgrader
waits for pending tasks to complete
waits for the machine learning templates to be created and check the version is up to date
find the memory size (in bytes) of the machine this node is running on. takes container limits (as used by docker for example) into account.
calculates the delay until the next time the maintenance should be triggered. the next time is 30 minutes past midnight of the following day plus a random offset. the random offset is added in order to avoid multiple clusters running the maintenance tasks at the same time. a cluster with a given name shall have the same offset throughout its life.
stopping a lookback closes the associated job _after_ the stop call returns. this test ensures that a kill request submitted during this close doesn't put the job into the "failed" state.
check that when the ml plugin is disabled, you cannot create a job as the rest handler is not registered
an open job that later gets added to a calendar, should take the scheduled events into account
test an open job picks up changes to scheduled eventscalendars
validations to fail fast before trying to update the job state on master node:  check job exists check job is not marked as deleted check job's version is supported 
visible for testing
visible for testing
validate the stop request. throws an resourcenotfoundexception if there is no datafeed with id datafeedid
resolve the requested datafeeds and add their ids to one of the list arguments depending on datafeed state.
validate the close request. throws an exception on any of these conditions:  if the job does not exist if the job has a data feed the feed must be closed first if the job is opening 
resolve the requested jobs and add their ids to one of the list arguments depending on job state. opened jobs are added to openjobids and closing jobs added to closingjobids. failed jobs are added to openjobids if allowfailed is set otherwise an exception is thrown.
get the jobs that match the given expression. note that when the jobid is metadata#all all jobs are returned.
create a jobmanager
stores a job in the cluster state
validate the char filtertokenizertoken filter names used in the categorization analyzer config (if any). this validation has to be done server-side; it cannot be done in a client as that won't have loaded the appropriate analysis modulesplugins. the overall structure can be validated at parse time, but the exact names need to be checked separately, as plugins that provide the functionality can be installeduninstalled.
removes any temporary storage leftovers. removes all temp files and folder which might be there as a result of an unclean node shutdown or broken clients. do not call while there are running jobs.
delete temporary storage, previously allocated path to temporary storage if path can not be cleaned up
tries to find local storage for storing temporary data.
construct, stating which pipes are expected to be created. the corresponding c++ process creates the named pipes, so only one combination of wanted pipes will work with any given c++ process. the arguments to this constructor must be carefully chosen with reference to the corresponding c++ code. must not be a full path, nor have the .exe extension on windows. may be null or empty for processes not associated with a specific job.
connect the pipes created by the c++ process. this must be called after the corresponding c++ process has been started. there should not be very many of these, so a short timeout should be fine. however, at least a couple of seconds is recommended due to the vagaries of process scheduling and the way vms can completely stall for some hypervisor actions.
augments a list of command line arguments, for example that built up by the autodetectbuilder class.
report 1 byte read
get a reference to the singleton native process controller. the nativecontroller is created lazily to allow time for the c++ process to be started before connection is attempted. null is returned to tests where xpack.ml.autodetect_process=false. calls may throw an exception if initial connection to the c++ process fails.
report the counts now regardless of whether or not we are at a reporting boundary.
increments the date parse error count
increment the number of records written by 1 and increment the total number of fields read. note this is not the number of processed fields (by field etc) but the actual number of fields in the record in milliseconds from the epoch.
increments the out of order record count
send signals, make a longer period of sparse signals, then go up again the number of sparse buckets should not be to much, it could be normal.
send signals, then make a long pause, send another signal and then check whether counts are right.
test for sparsity on the last 2 buckets, should create a sparse bucket signal on the 2nd to last
test for sparsity on the last bucket should not create a sparse bucket signal
tail the inputstream provided to the constructor, handling each complete log document as it arrives. this method will not return until either end-of-file is detected on the inputstream or the inputstream throws an exception.
get the process id of the c++ process whose log messages are being read. this will arrive in the first log message logged by the c++ process. they all log a copyright message immediately on startup so it should not take long to arrive, but will not be available instantly after the process starts.
for testing - allows meddling with the logger, read buffer size and error store size.
expected to be called very infrequently.
get the process id of the c++ process whose log messages are being read. this will arrive in the first log message logged by the c++ process. they all log their version number immediately on startup so it should not take long to arrive, but will not be available instantly after the process starts.
extracts version information from the copyright string which assumes a certain format.
updates the normalized scores on the results.
recursively merges the scores returned by the normalization process into the results
launches a normalization process seeded with the quantiles state provided and normalizes the given results. normalizer
update the anomaly score field on all previously persisted buckets and all contained records
build the command to start the normalizer process.
flush the running job, ensuring that the native process has had the opportunity to process all data previously sent to it with none left sitting in buffers. (e.g. calculating interim results, time control, etc.)
request temporary storage to be used for the job
do a forecast for the running job.
stop the running job and mark it as finished.
passes data to the native process. this is a blocking call that won't return until all the data has been written to the process.  an elasticsearchstatusexception will be thrown is any of these error conditions occur:  if a configured field is missing from the csv header if json data is malformed and we cannot recover parsing if a high proportion of the records the timestamp field that cannot be parsed if a high proportion of the records chronologically out of order 
care must be taken to ensure this method is not called while data is being posted. the methods in this class that call it wait for all processing to complete first. the expectation is that external calls are only made when cleaning up after a fatal error.
closes job this communicator is encapsulating.
throws an exception if the process has exited
constructs an autodetect process builder deleted when the process completes
visible for testing
write the ml autodetect model options to emptyconffile.
requests that the controller daemon start an autodetect process.
accept the request do nothing with it but write the flush acknowledgement to #readautodetectresults()
returns epoch milli seconds
lower level functions to write record fields individually.
lower level functions to write records individually. after this function is called #writefield(string) must be called numfields times.
read the json object and write to the record array. nested objects are flattened with the field names separated by a '.'. e.g. for a record with a nested 'tags' object: ""name":"my.test.metric1","tags":"tag1":"blah","tag2":"boo", "time":1350824400,"value":12345.678" use 'tags.tag1' to reference the tag1 field in the nested object  array fields in the json are ignored read fields are written to this array. this array is first filled with empty strings and will never contain a null boolean array each element is true if that field was read because the end of the stream was reached
in some cases the parser doesn't recognise the '' of a badly formed json document and so may skip to the end of the second document. in this case we lose an extra record.
returns null at the eof or the next token
get the text representation of the current token unless it's a null. nulls are replaced with empty strings to match the way the rest of the product treats them (which in turn is shaped by the fact that csv cannot distinguish empty string and null).
there's a problem with the parser where in this case it skips over the first 2 records instead of to the end of the first record which is invalid json. this means we miss the next record after a bad one.
simple test push a list of records through the writer and check the output the writer accepts empty strings but not null strings
test the writefield and writenumfields methods of lengthencodedwriter
write the ml autodetect field options to the outputindex stream.
send a flush message to the c++ autodetect process. this actually consists of two messages: one to carry the flush id and the other (which might not be processed until much later) to fill the buffers and force prior messages through. autodetect process once it is complete.
transform the supplied control message to length encoded values and write to the outputstream. the number of blank fields to make up a full record is deduced from analysisconfig.
writes the control messages that are requested when flushing a job. those control messages need to be followed by a flush message in order for them to reach the c++ process immediately. list of supported controls:  advance time calculate interim results  (e.g. calculating interim results, time control, etc.)
create indexes of the output fields. this is the time field and all the fields configured for analysis and the control field. time is the first field and the last is the control field
transform the date in the input data and write all fields to the length encoded writer.  fields must be copied from input to output before this function is called. this should be the same size as the number of output (analysis fields) i.e. the size of the map returned by #outputfieldindexes()
accessible for testing only.
set up the field index mappings. this must be called before  finds the required input indexes in the header and sets the mappings to the corresponding output indexes.
write the header. the header is created from the list of analysis input fields, the time field and the control field.
get all the expected input fields i.e. all the fields we must see in the csv header
create a map of input index to output index. this does not include the time or control fields.
find the indexes of the input fields from the header
read the csv inputindex, transform to length encoded values and pipe to the outputstream. if any of the expected fields in the analysis inputindex or if the expected time field is missing from the csv header a exception is thrown
read the json inputindex, transform to length encoded values and pipe to the outputstream. no transformation is applied to the data the timestamp is expected in seconds from the epoch. if any of the fields in analysisfields or the datadescriptions timefield is missing from the josn inputindex an exception is thrown
testing methods of abstractdatatoprocesswriter but uses the concrete instances.
constructs a datatoprocesswriter depending on the data format and the time transformation. otherwise a csvdatatoprocesswriter
test parsing csv with the nul character code point (\0 or \u0000)
this test is designed to pick up n-squared processing in the state consumption code. the size of the state document is comparable to those that the c++ code will create for a huge model. 10 seconds is an overestimate of the time required to avoid spurious failures due to vm stalls - on a reasonable spec laptop this should take around 1 second.
splits bulk data streamed from the c++ process on '\0' characters. the data is expected to be a series of elasticsearch bulk requests in utf-8 json (as would be uploaded to the public rest api) separated by zero bytes ('\0').
ensure that we do not accept nan values
the purpose of this method is to avoid saturating the cluster state update thread when a lookback job is churning through buckets very fast and the memory usage of the job is changing regularly. the idea is to only update the established model memory associated with the job a few seconds after the new value has been received. if more updates are received during the delay period then they simply replace the value that originally caused the update to be scheduled. this rate limits cluster state updates due to established model memory changing to one per job per delay period. (in reality updates will only occur this rapidly during lookback. during real-time operation the limit of one model size stats document per bucket will mean there is a maximum of one cluster state update per job per bucket, and usually the bucket span is 5 minutes or more.)
this method is called from two places: - from the future used for delayed updates - when shutting down this result processor when shutting down the result processor it's only necessary to do anything if an update has been scheduled, but we want to do the update immediately. despite cancelling the scheduled update in this case, it's possible that it's already started running, in which case this method will get called twice in quick succession. but the second call will do nothing, as scheduledestablishedmodelmemoryupdate will have been reset to null by the first call.
given a model snapshot, get the corresponding state and write it to the supplied stream. if there are multiple state documents they are separated using '\0' when written to the stream. because we have a rule that we will not open a legacy job in the current product version we don't have to worry about legacy document ids here.
return a page of influencers for the given job and within the given date range uses a supplied client, so may run as the currently authenticated user
create the elasticsearch index and the mappings
get a job's model snapshot by its id
get the job's model size stats.
get the job's data counts
get model snapshots for the job ordered by descending restore priority.
search for anomaly records with the parameters in the uses a supplied client, so may run as the currently authenticated user
search for buckets with the parameters in the bucketsquerybuilder uses a supplied client, so may run as the currently authenticated user
get the "established" memory usage of a job, if it has one. in order for a job to be considered to have established memory usage it must: - have generated at least buckets_for_established_memory_size buckets of results - have generated at least one model size stats document - have low variability of model bytes in model size stats documents in the time period covered by the last buckets_for_established_memory_size buckets, which is defined as having a coefficient of variation of no more than established_memory_cv_threshold null, implying the latest bucket that exists in the results index these when available avoids one search specified job, or 0 if memory usage is not yet established
get a page of categorydefinitions for the given jobid. uses a supplied client, so may run as the currently authenticated user
check that a previously deleted job with the same id has not left any result or categorizer state documents due to a failed delete. any left over results would appear to be part of the new job. we can't check for model state as the id is based on the snapshot id which is a timestamp and so unpredictable however, it is unlikely a new job would have the same snapshot id as an old one.
update the job's data counts stats and figures.
execute the bulk action
the first time next() is called, the search will be performed and the first batch will be returned. any subsequent call will return the following batches.  note that in some implementations it is possible that when there are no results at all, the first time this method is called an empty deque is returned.
persist the memory usage data (blocking)
persist the quantiles (async)
once all the job data has been written this function will be called to commit the writes to the datastore.
persist the result bucket and its bucket influencers buckets are persisted with a consistent id
persist the memory usage data
execute the bulk action
persist the category definition
persist a model snapshot description
once the job state has been written calling this function makes it immediately searchable.
given a collection of strings, work out which (if any) of the grok patterns we're allowed to use matches it best. then append the appropriate grok language to represent that finding onto the supplied string builder.
given a collection of strings, and a grok pattern that matches some part of them all, return collections of the bits that come before (prefaces) and after (epilogues) the bit that matches.
the first time a particular field name is passed, simply return it. the second time return it with "2" appended. the third time return it with "3" appended. etc.
given a category definition regex and a collection of examples from the category, return a grok pattern that will match the category and pull out any likely fields. the extracted fields are given pretty generic names, but unique within the grok pattern provided. the expectation is that a user will adjust the extracted field names based on their domain knowledge.
release resources held by the analyzer (unless it's global).
get char filter factories for each configured char filter. each configuration element can be the name of an out-of-the-box char filter, or a custom definition.
get the tokenizer factory for the configured tokenizer. the configuration can be the name of an out-of-the-box tokenizer, or a custom definition.
get token filter factories for each configured token filter. each configuration element can be the name of an out-of-the-box token filter, or a custom definition.
convert a config to an analyzer. this may be a global analyzer or a newly created custom analyzer. in the case of a global analyzer the caller must not close it when they have finished with it. in the case of a newly created custom analyzer the caller is responsible for closing it. for closing it.
given a field value, convert it to a list of tokens using the configured analyzer.
verify that the config builder will build a valid config. this is not done as part of the basic build because it verifies that the names of analyzerstokenizersfilters referenced by the config are known, and the validity of these names could change over time. additionally, it has to be done server-side rather than client-side, as the client will not have loaded the appropriate analysis modulesplugins.
basically tokenise into [a-za-z0-9]+ strings, but also allowing underscores, dots and dashes in the middle. then discard tokens that are hex numbers or begin with a digit.
stops the datafeed job otherwise false is returned
issues a recovery message if appropriate and prepares for next report
updates the tracking of empty data cycles. if the number of consecutive empty data cycles reaches empty_data_warn_count, a warning is reported.
reports the problem if it is different than the last seen problem
this is used when the license expires.
this is used before the jvm is killed. it differs from stopalldatafeedsonthisnode in that it leaves the datafeed tasks in the "started" state, so that they get restarted on a different node.
creates a dataextractorfactory for the given datafeed-job combination.
processes a list of aggregations and writes a flat json document for each of its leaf aggregations. supported sub-aggregations include:   multibucketsaggregation  numericmetricsaggregation.singlevalue  percentiles 
constructs a processor that processes aggregations into json
write the aggregated documents one bucket at a time until batchsize key-value pairs have been written. buckets are written in their entirety and the check on batchsize run after the bucket has been written so more than batchsize key-value pairs could be written. the function should be called repeatedly until it returns false, at that point there are no more documents to write. false if there are no documents to write.
adds a leaf key-value. it returns the name of the key added or null when nothing was added. non-finite metric values are not added.
this format matches if the sample consists of one or more xml documents, all with the same root element name. if there is more than one document, only whitespace is allowed in between them. the last one does not necessarily have to be complete (as the sample could have truncated it).
given a collection of strings, and a grok pattern that matches some part of them all, return collections of the bits that come before (prefaces) and after (epilogues) the bit that matches.
given a collection of message snippets, work out which (if any) of the grok patterns we're allowed to use matches it best. then append the appropriate grok language to represent that finding onto the supplied string builder.
given a chosen grok pattern and a collection of message snippets, split the snippets into the matched section and the pieces before and after it. recurse to find more matches in the pieces before and after and update the supplied string builder.
if the snippets supplied begin with more than 1 character of common punctuation or whitespace then add all but the last of these characters to the overall pattern and remove them from the snippets. were added to overallpatternbuilder removed from the beginning.
this must only be called if #matchesall returns true.
build a grok pattern that will match all of the sample messages in their entirety.
this method attempts to find a grok pattern that will match all of the sample messages in their entirety.
this format matches if the sample consists of one or more json documents. if there is more than one, they must be newline-delimited. the documents must be non-empty, to prevent lines containing "" from matching.
given some sample values for a field, guess the most appropriate index mapping for the field. may be non-empty when the method is called, and this method may append to it. mapping will be compatible with all the provided values. must not be empty. for example  "type" : "keyword" .
given one or more sample records, find a timestamp field that is consistently present in them all. to be returned the timestamp field: - must exist in every record - must have the same timestamp format in every record if multiple fields meet these criteria then the one that occurred first in the first sample record is chosen. may be non-empty when the method is called, and this method may append to it. there is no consistent timestamp.
given the sampled records, guess appropriate elasticsearch mappings.
this method implements the simple algorithm for calculating levenshtein distance.
sum of the levenshtein distances between corresponding elements in the two supplied lists _excluding_ the biggest difference. the reason the biggest difference is excluded is that sometimes there's a "message" field that is much longer than any of the other fields, varies enormously between rows, and skews the comparison.
sometimes elasticsearch mappings for dates need to include the format. this method returns appropriate mappings settings: at minimum "type"="date", and possibly also a "format" setting.
find the first timestamp format that matches part of the supplied value, excluding a specified number of candidate formats.
interpret the fractional seconds component of a date to determine two things: 1. the separator character - one of colon, comma and dot. 2. the number of digits in the fractional component.
find the best timestamp format for matching an entire field value, excluding a specified number of candidate formats.
this format matches if the sample contains at least one newline and at least two non-blank lines.
this has to use different logic to the input pipe case to avoid the danger of creating a regular file when the named pipe does not exist when the method is first called.
the default path where named pipes will be created. on nix they can be created elsewhere (subject to security manager constraints), but on windows this is the only place they can be created.
open a named pipe created elsewhere for input.
the logic here is very similar to that of opening an input stream, because on windows java cannot create a regular file when asked to open a named pipe that doesn't exist.
makes the index readonly if it's not set as a readonly yet
returns the information about required upgrade action for the given indices
creates a new upgrade check
execute a client operation asynchronously, try to run an action with least privileges, when headers exist request headers, ideally including security headers the origin to fall back to if there are no security headers the action to execute the request object for the action the listener to call when the action is complete
execute a client operation and return the response, try to run an action with least privileges, when headers exist request headers, ideally including security headers the origin to fall back to if there are no security headers the client used to query the action to run
executes a consumer after setting the origin and wrapping the listener so that the proper context is restored
executes an asynchronous action using the provided client. the origin is set in the context and the listener is wrapped to ensure the proper context is restored
stashes the current context and sets the origin in the current context. the original context is returned as a stored context
returns all settings created in xpacksettings.
checks if the cluster state allows this node to add x-pack metadata to the cluster state, and throws an exception otherwise. this check should be called before installing any x-pack metadata to the cluster state, to ensure that the other nodes that are part of the cluster will be able to deserialize that metadata. note that if the cluster state already contains x-pack metadata, this check assumes that the nodes are already ready to receive additional x-pack metadata. having this check properly in place everywhere allows to install x-pack into a cluster using a rolling restart.
returns path to xpack codebase path
this method executes a search and ensures no stashed origin thread context was created, so that the regular node client was used, to emulate a run_as function
this method executes a search and checks if the thread context was enriched with the ml origin
read from a stream.
create a pipeline to upgrade documents from monitoringtemplateutils#old_template_version  "description" : "this pipeline upgrades documents ...", "version": 6000001, "processors": [ ]  the expectation is that you will call either strings#tostring(xcontentbuilder) or
create a pipeline that allows documents for different template versions to be upgraded.  the expectation is that you will call either strings#tostring(xcontentbuilder) or
create an empty pipeline.  "description" : "this is a placeholder pipeline ...", "version": 6000001, "processors": [ ]  the expectation is that you will call either strings#tostring(xcontentbuilder) or
parses a monitoring bulk request and builds the list of documents to be indexed.
read from a stream.
find the monitoringindex to use for the request.
this returns a set of aggregation builders which represent the configured set of date histograms. used by the rollup indexer to iterate over historical data
this returns a set of aggregation builders which represent the configured set of metrics. used by the rollup indexer to iterate over historical data
create a new datehistogramgroupconfig using the given configuration parameters.  the field and interval are required to compute the date histogram for the rolled up documents. the delay is optional and can be set to null. it defines how long to wait before rolling up new documents. the timezone is optional and can be set to null. when configured, the time zone value is resolved using ( datetimezone#forid(string) and must match a time zone identifier provided by the joda time library. 
this returns a set of aggregation builders which represent the configured set of date histograms. used by the rollup indexer to iterate over historical data
tests that a datehistogramgroupconfig can be serializeddeserialized correctly after the timezone was changed from datetimezone to string.
this returns a set of aggregation builders which represent the configured set of histograms. used by the rollup indexer to iterate over historical data
returns the authentication information, or null if the current request has no authentication info.
loads the xpacksecurityextensions from the given class loader
runs the consumer in a new context after setting a new version of the authentication that is compatible with the version provided. the original context is provided to the consumer. when this method returns, the original context is restored.
runs the consumer in a new context as the provided user. the original context is provided to the consumer. when this method returns, the original context is restored.
writes the authentication to the thread context
sets the user forcefully to the provided user. there must not be an existing user in the threadcontext otherwise an exception will be thrown. this method is package private for testing.
creates a new security context. if cryptoservice is null, security is disabled and usersettings#getuser() and usersettings#getauthentication() will always return null.
returns the authentication information, or null if the current request has no authentication info.
this method fetches all results for the given search request, parses them using the given hit parser and calls the listener once done.
populate the put privileges request using the given source, application name and privilege name the source must contain a top-level object, keyed by application name. the value for each application-name, is an object keyed by privilege name. the value for each privilege-name is a privilege object which much match the application and privilege names in which it is nested.
populate the put role request from the source and the role's name
sets the password. note: the char[] passed to this method will be cleared.
populate the change password request from the source in the provided content type
populate the put user request using the given source and username
generates x-content for this roledescriptor instance. in the security index, false if the x-content being generated is for api display purposes
read a list of privileges from the parser. the parser should be positioned at the
writes a single object value to the builder that contains each of the provided privileges. the privileges are grouped according to their conditionalclusterprivilege#getcategory() categories
construct a new applicationprivilegedescriptor from xcontent.
validates that an application name matches the following rules: - consist of a "prefix", optionally followed by either "-" or "_" and a suffix - the prefix must begin with a lowercase ascii letter - the prefix only contain ascii letter or digits - the prefix must be at least 3 characters long - the suffix must only contain strings#validfilename valid filename characters - no part of the name may contain whitespace if allowwildcard is true, then the names that end with a '', and would match a valid application name are also accepted.
validate that the provided privilege name is valid, and throws an exception otherwise
validate that the provided name is a valid privilege name or action name, and throws an exception otherwise
finds or creates an application privileges with the provided names. each element in name may be the name of a stored privilege (to be resolved from stored, or a bespoke action pattern.
checks whether the role query contains queries we know can't be used as dls role query.
fall back validation that verifies that queries during rewrite don't use the client to make remote calls. in the case of dls this can cause a dead lock if dls is also applied on these remote calls. for example in the case of terms query with lookup, this can cause recursive execution of the dls query until the get thread pool has been exhausted: https:github.comelasticx-pluginsissues3145
test special handling for _field_names field.
test filtering two vector fields
test filtering two binary dv fields
test special handling for _field_names field (three fields, to exercise termsenum better)
test filtering two stored fields (double)
test special handling for _source field.
test filtering two stored fields (float)
test filtering an index with no fields
test filtering two stored fields (long)
test filtering two sortedset dv fields
test filtering the only vector fields
test filtering two stored fields (binary)
test filtering two stored fields (int)
test filtering two int points
test that core cache key (needed for nrt) is working
test filtering two text fields
test filtering two string fields
test filtering two stored fields (string)
test filtering two sorted dv fields
test filtering two numeric dv fields
test _field_names where a field is permitted, but doesn't exist in the segment.
test filtering two sortednumeric dv fields
test where _field_names does not exist
test we have correct fieldinfos metadata
step through all characters of the provided string, and return the resulting state, or -1 if that did not lead to a valid state.
filter a list by a characterrunautomaton that defines the fields to retain.
wrap a single segment, exposing a subset of its fields.
filter a map by a characterrunautomaton that defines the fields to retain.
like #computenumdocs but caches results.
compute the number of live documents. this method is slow.
same test as in fieldsubsetreadertests, test that core cache key (needed for nrt) is working
checks if the permission matches the provided action, without looking at indices. to be used in very specific cases where indices actions need to be authorized regardless of their indices. the usecase for this is composite actions that are initially only authorized based on the action name (indices are not checked on the coordinating node), and properly authorized later at the shard level checking their indices as well.
authorizes the provided action against the provided indices, given the current cluster metadata
constructor that enables field-level security based on includeexclude rules. exclude rules have precedence over include rules.
gets a fieldpermissions instance that corresponds to the granted and denied parameters. the instance may come from the cache or if it gets created, the instance will be cached
returns a field permissions object that corresponds to the merging of the given field permissions and caches the instance if one was not found in the cache.
determines whether this permission grants the specified privilege on the given resource.  an applicationpermission consists of a sequence of permission entries, where each entry contains a single   this method returns true if, one or more of those entries meet the following criteria   the entry's application, when interpreted as an automaton automatons#pattern(string) pattern matches the application given in the argument (interpreted as a raw string)  the applicationprivilege#getautomaton automaton that defines the entry's actions entirely covers the automaton given in the argument (that is, the argument is a subset of the entry's automaton)  the entry's resources, when interpreted as an automaton automatons#patterns(string...) set of patterns entirely covers the resource given in the argument (also interpreted as an automaton automatons#pattern(string) pattern.  
a single applicationprivilege and the set of resources to which that privilege is applied. the resources are treated as a wildcard automatons#pattern.
returns whether at least one group encapsulated by this indices permissions is authorized to execute the specified action with the requested indicesaliases. at the same time if field andor document level security is configured for any group also the allowed fields and role queries are resolved.
ensures that we're currently on the start of an object, or that the next token is a start of an object.
builds and returns an automaton that will represent the union of all the given patterns.
builds and returns an automaton that represents the given pattern.
builds and returns an automaton that represents the given pattern.
validate the username
wraps the restrequest and returns a version that provides the filtered content
extracts the realm settings from a global settings object. returns a map of realm-name to realm-settings.
writes the authentication to the context. there must not be an existing authentication in the context and if there is an
provides the setting setting configuration for each internal realm type. this excludes the reservedrealm, as it cannot be configured dynamically.
constructs default authentication failure handler with provided default response headers. be sent as failure response.
creates an instance of elasticsearchsecurityexception with  also adds default failure response headers as configured for this  it may replace existing response headers if the cause is an instance of reststatus.unauthorized and adds headers to it, else it will create a new instance of elasticsearchsecurityexception
hash a password using the openbsd bcrypt scheme. modified from the original to take a securestring instead of the original using bcrypt.gensalt)
decode a string encoded using bcrypt's base64 scheme to a byte array. note that this is not compatible with the standard mime-base64 encoding.
perform the central password hashing step in the bcrypt scheme of rounds of hashing to apply
generate a salt for use with the bcrypt.hashpw() method hashing to apply - the work factor therefore increases as 2log_rounds.
encode a byte array using bcrypt's slightly-modified base64 encoding scheme. note that this is not compatible with the standard mime-base64 encoding. @exception illegalargumentexception if the length is invalid
returns a hasher instance of the appropriate algorithm and associated cost as indicated by the name. name identifiers for the default costs for bcrypt and pbkdf2 return the he default bcrypt and pbkdf2 hasher instead of the specific instances for the associated cost.
returns a hasher instance that can be used to verify the hash by inspecting the hash prefix and determining the algorithm used for its generation. if no specific algorithm prefix, can be determined hasher.noop is returned.
generates an array of length random bytes using java.security.securerandom
parse an expressionrolemapping from the provided xcontent
parse an expressionrolemapping from the provided xcontent
descriptive error messages.
returns true if the named field, matches any of the provided values.
a comparison of number objects that compares by floating point when either value is a float or double otherwise compares by numbers#tolongexact long.
constructs a predicate that matches correctly based on the type of the provided parameter.
this is how the ids were formed in v5.4
if earliestrecordtimestamp has not been set (i.e. is null) then set it to timestamp
data store id of this record.
updates source with the new values in this object returning a new job.
create a config validating only structure, not exact analyzertokenizerfilter names
parse a categorization_analyzer from configuration or cluster state. a custom parser is needed due to the complexity of the format, with many elements able to be specified as either the name of a built-in element or an object containing a custom definition. the parser is strict when parsing config and lenient when parsing cluster state.
this method is only used in the unit tests - in production code this config is always parsed as a fragment.
create a categorization_analyzer that mimics what the tokenizer and filters built into the ml c++ code do. this is the default analyzer for categorization to ensure that people upgrading from previous versions get the same behaviour from their categorization jobs before and after upgrade.
returns the job types that are compatible with a node running on nodeversion
call this method to validate that the job json provided by a user is valid. throws an exception if there are any problems; normal return implies valid.
returns the timestamp before which data is not accepted by the job. this is the latest record timestamp minus the job latency.
make a best estimate of the job's memory footprint using the information available. if a job has an established model memory size, then this is the best estimate. otherwise, assume the maximum model memory limit will eventually be required. in either case, a fixed overhead is added to account for the memory required by the program code and stack.
return the list of fields that have been set and are invalid to be set when the job is created e.g. model snapshot id should not be set at job creation.
get all input data fields mentioned in the job configuration, namely analysis fields and the time field.
builds a job. this should be used when an existing job is being built as opposed to #build(date).
case-insensitive from string method. works with either json, json, etc.
returns a list with the byfieldname, overfieldname and partitionfieldname that are not null
returns the set of byoverpartition terms
appends to the given stringbuilder the default description for the given detector
returns the default description for the given detector
checks the configuration is valid  check that if non-null bucketspan and latency are >= 0 check that if non-null latency is <= max_latency check there is at least one detector configured check all the detectors are configured correctly check that overlapping_buckets is set appropriately check that multiple_bucketspans are set appropriately if per partition normalization is configured at least one detector must have a partition field and no influences can be used 
return the set of fields required by the analysis. these are the influencer fields, metric field, partition field, by field and over field of each detector, plus the summary count field and the categorization field name of the job. null and empty strings are filtered from the config.
creates a completely random update when the job is null or a random update that is is valid for the given job
creates a new analysislimits object after validating it against external limitations and filling missing values with their defaults. validations:  check model memory limit doesn't exceed the max_model_mem setting 
test the analysisconfig#analysisfields() method which produces a list of analysis fields from the detectors
create the elasticsearch mapping for state. state could potentially be huge (target document size is 16mb and there can be many documents) so all analysis by elasticsearch is disabled. the only way to retrieve state is by knowing the id of a particular document.
anomalyrecord fields to be added under the 'properties' section of the mapping
create the elasticsearch mapping for modelsnapshot. the '_all' field is disabled but the type is searchable
the type is disabled so datacounts aren't searchable and the '_all' field is disabled
create the elasticsearch mapping for categorydefinition. the '_all' field is disabled as the document isn't meant to be searched.
creates a default mapping which has a dynamic template that treats all dynamically added fields as keywords. this is needed so that the per-job term fields will not be automatically added as fields of type 'text' to the index mappings of newly rolled indices.
inserts "_meta" containing useful information like the version into the mapping template.
create the elasticsearch mapping for results objects the mapping has a custom all field containing the _field_value fields e.g. by_field_value, over_field_value, etc. the custom all field #all_field_values must be set in the index settings. a custom all field is preferred over the usual '_all' field as most fields do not belong in '_all', disabling '_all' and using a custom all field simplifies the mapping. these fields are copied to the custom all field  by_field_value partition_field_value over_field_value anomalycause.correlated_by_field_value anomalycause.by_field_value anomalycause.partition_field_value anomalycause.over_field_value anomalyrecord.influencers.influencer_field_values influencer.influencer_field_value 
delete a list of model snapshots and their corresponding state documents.
delete all results marked as interim
asynchronously delete all result types (buckets, records, influencers) from cutofftime
validates a datafeedconfig in relation to the job it refers to
applies the update to the given datafeedconfig
calculates a sensible default frequency for a given bucket span.  the default depends on the bucket span:   <= 2 mins -> 1 min  <= 20 mins -> bucket span 2  <= 12 hours -> 10 mins  > 12 hours -> 1 hour  if the datafeed has aggregations, the default frequency is the closest multiple of the histogram interval based on the rules above.
checks that a searchresponse has an ok status code and no shard failures
returns the date histogram interval as epoch millis if valid, or throws an elasticsearchexception with the validation error
find and return (date) histogram in aggregations
get the interval from histogramaggregation or throw an illegalstateexception if histogramaggregation is not a histogramaggregationbuilder or a
expands an expression into the set of matching names. for example, given a set of names ["foo-1", "foo-2", "bar-1", bar-2"], expressions resolve follows:  "foo-1" : ["foo-1"] "bar-1" : ["bar-1"] "foo-1,foo-2" : ["foo-1", "foo-2"] "foo-" : ["foo-1", "foo-2"] "-1" : ["bar-1", "foo-1"] "" : ["bar-1", "bar-2", "foo-1", "foo-2"] "_all" : ["bar-1", "bar-2", "foo-1", "foo-2"]  this only applies to wild card expressions, if expression is not a wildcard then setting this true will not suppress the exception
creates a nameresolver that has no aliases
surrounds with double quotes the given input if it contains any non-word characters. any double quotes contained in input will be escaped. that contains input surrounded by double quotes otherwise
returns the path to the parent field if fieldpath is nested or fieldpath itself. or fieldpath itself
a more rest-friendly object.requirenonnull()
creates an error message that explains there are shard failures, displays info for the first failure (shardreason) and kindly asks to see more info in the logs
creates a formatter according to the given pattern see datetimeformatter for the syntax of the accepted patterns (e.g. contains a date but not a time)
check the given timevalue is a multiple of the baseunit
first tries to parse the date first as a long and convert that to an epoch time. if the long number has more than 10 digits it is considered a time in milliseconds else if 10 or less digits it is in seconds. if that fails it tries to parse the string using if the date string cannot be parsed -1 is returned. parsed.
convert the scheduled event to a detection rule. the rule will have 2 time based conditions for the start and end of the event. the rule's start and end times are aligned with the bucket span so the start time is rounded down to a bucket interval and the end time rounded up.
tests the reloading of sslcontext when a pem key and certificate are used.
tests the reloading of a trust config backed by pem files when there is an exception during reloading. an exception is caused by truncating the certificate file that is being monitored
tests the reloading of a keystore when there is an exception during reloading. an exception is caused by truncating the keystore that is being monitored
tests the reloading of a key config backed by pem files when there is an exception during reloading. an exception is caused by truncating the key file that is being monitored
creates a closeablehttpclient that only trusts the given certificate(s)
test the reloading of sslcontext whose trust config is backed by pem certificate files.
tests reloading a keystore that is used in the keymanager of sslcontext
tests the reloading of a truststore when there is an exception during reloading. an exception is caused by truncating the truststore that is being monitored
tests the reloading of sslcontext when the trust store is modified. the same store is used as a truststore (for the reloadable sslcontext used in the httpclient) and as a keystore for the mockwebserver
ending in "."
decodes the othername cn from the certificate
decode the length of the field. can only support length encoding up to 4 octets.  in berder encoding, length can be encoded in 2 forms:   short form. one octet. bit 8 has value "0" and bits 7-1 give the length.  long form. two to 127 octets (only 4 is supported here). bit 8 of first octet has value "1" and bits 7-1 give the number of additional length octets. second and following octets give the length, base 256, most significant digit first.  
for constructed field, return a parser for its content.
get the value as integer
converts a hexadecimal string to a byte array
removes the dsa params headers that openssl adds to dsa private keys as the information in them is redundant
performs key stretching in the same manner that openssl does. this is basically a kdf that uses n rounds of salted md5 (as many times as needed to get the necessary number of key bytes)  https:www.openssl.orgdocsman1.1.0cryptopem_write_bio_privatekey_traditional.html
parses a der encoded dsa key to a dsaprivatekeyspec using a minimal derparser
creates a privatekey from the contents of a file. supports pkcs#1, pkcs#8 encoded formats of encrypted and plaintext rsa, dsa and ec(secp256r1) keys
creates a privatekey from the contents of breader that contains an encrypted private key encoded in pkcs#8
creates a cipher from the contents of the dek-info header of a pem file. rfc 1421 indicates that supported algorithms are defined in rfc 1423. rfc 1423 only defines des-cbs and triple des (ede) in cbc mode. aes in cbc mode is also widely used though ( 3 different variants of 128, 192, 256 bit keys ) for the cipher
creates a privatekey from the contents of breader that contains an ec private key encoded in openssl traditional format.
creates a privatekey from the contents of breader that contains an plaintext private key encoded in pkcs#8
parses a der encoded ec key to an ecprivatekeyspec using a minimal derparser
decrypts the password protected contents using the algorithm and iv that is specified in the pem headers of the file
creates a privatekey from the contents of breader that contains an rsa private key encoded in openssl traditional format.
parses a der encoded private key and reads its algorithm identifier object oid.
removes the ec headers that openssl adds to ec private keys as the information in them is redundant
parses a der encoded rsa key to a rsaprivatecrtkeyspec using a minimal derparser
creates a privatekey from the contents of breader that contains an dsa private key encoded in openssl traditional format.
create a new sslservice that parses the settings for the ssl contexts that need to be created, creates them, and then caches them for use later
returns information about each certificate that is referenced by any ssl configuration. this includes certificates used for identity (with a private key) and those used for trust, but excludes certificates that are provided by the jre. due to the nature of keystores, this may include certificates that are available, but never used such as a ca certificate that is no longer in use, or a server certificate for an unrelated host.
creates an sslcontext based on the provided configuration
invalidates the sessions in the provided sslsessioncontext
parses the settings to load all sslconfiguration objects that will be used.
returns the existing sslcontextholder for the configuration
returns the existing sslconfiguration for the given settings
creates an sslcontext based on the provided configuration and trustkey managers
create a new sslsocketfactory based on the provided configuration. the socket factory will also properly configure the ciphers and protocols on each socket that is created
maps the supported protocols to an appropriate ssl context algorithm. we make an attempt to use the "best" algorithm when possible. the names in this method are taken from the  href="http:docs.oracle.comjavase8docstechnotesguidessecuritystandardnames.html">jca standard algorithm name documentation for java 8.
creates a new sslservice that supports dynamic creation of sslcontext instances. instances created by this service will not be cached and will not be monitored for reloading. this dynamic server does have access to the cached and monitored instances that have been created during initialization
returns the existing sslcontextholder for the configuration
creates an sslengine based on the provided configuration. this sslengine can be used for a connection that requires hostname verification assuming the provided host and port are correct. the sslengine created by this method is most useful for clients with hostname verification enabled certificate
returns the intersection of the supported ciphers with the requested ciphers. this method will also optionally log if unsupported ciphers were requested.
returns a unique set of directories that need to be monitored based on the provided file paths
collects all of the directories that need to be monitored for the provided sslconfiguration instances and ensures that they are being watched for changes
provides the list of paths to files that back this configuration
returns information about each certificate that referenced by this ssl configurations. this includes certificates used for identity (with a private key) and those used for trust, but excludes certificates that are provided by the jre.
read all certificate-key pairs from a pkcs#12 container. return the password for that key. if it returns null, then the key-pair for that alias is not read.
reads the provided paths and parses them into certificate objects
resolves a path with or without an environment as we may be running in a transport client where we do not have access to the environment
creates a x509extendedtrustmanager based on the trust material in the provided keystore
returns a x509extendedkeymanager that is built from the provided keystore
merges the default trust configuration with the provided trustconfig
loads a built-in template and returns its source.
checks if a versioned template exists, and if it exists checks if the version is greater than or equal to the current version.
checks if template with given name exists and if it matches the version predicate given
loads a resource from the classpath and returns it as a bytesreference
parses and validates that the source is not empty.
loads a json template as a resource and puts it into the provided map
advance the calendar to the particular hour paying particular attention to daylight saving problems.
constructs a new cronexpression based on the specified parameter. new object should represent if the string expression cannot be parsed into a valid cronexpression
returns the next datetime after the given datetime which satisfies the cron expression. a time that is previous to the given time)
add a field in which this hop will look for terms that are highly linked to previous hops and optionally the guiding query.
adds a term that should be excluded from results
adds a term to the set of allowed values - the boost defines the relative importance when pursuing connections in subsequent hops. the boost value appears as part of the query.
graph exploration can be set to timeout after the given period. search operations involved in each hop are limited to the remaining time available but can still overrun due to the nature of their "best efforts" timeout support. when a timeout occurs partial results are returned.
add a stage in the graph exploration. each hop represents a stage of querying elasticsearch to identify terms which can then be connnected to other terms in a subsequent hop. are considered in this stage
given a watch, this extracts and decodes the relevant auth header and returns the principal of the user that is executing the watch.
convert the counters to a nested map, using the "." as a splitter to create deeper maps
execute the current #action().  this executes in the order of:  throttling conditional check transformation action 
returns a org.elasticsearch.common.bytes.bytesreference containing the toxcontent output in binary format. builds the request as the provided contenttype
parse a timevalue with support for fractional values.
constructs a new xcontentsource out of the given bytes reference.
decrypts the provided char array and returns the plain-text chars
encrypts the provided char array and returns the encrypted values in a char array
parses the xcontent and returns the appropriate executable condition. expecting the following format:  "condition_type" : ... condition body 
called whenever an watch is checked, ie. the condition of the watch is evaluated to see if the watch should be executed.
notifies this status that the givne actions were acked. if the current state of one of these actions is then we'll it'll change to actionstatus.ackstatus.state#acked acked (when set to actionstatus.ackstatus.state#acked acked, the ackthrottler will throttle the execution of the action.
ctor for found watch
this is the function that does the bulk of the logic of taking the appropriate es dependencies like nodeinfo, clusterstate. alongside these objects and the list of deprecation checks, this function will run through all the checks and build out the final list of issues that exist in the cluster. concrete indices
write a cursor to a string for serialization across xcontent.
the namedwriteables required to deserialize cursors.
read a cursor from a string.
find commonalities between the given comparison in the given list. the method can be applied both for conjunctive (and) or disjunctive purposes (or).
test that empty commands are skipped. this includes commands that are just new lines.
build the connection.
use this vm options to run in intellij or eclipse: -dorg.jline.terminal.type=xterm-256color -dorg.jline.terminal.jna=false -dorg.jline.terminal.jansi=false -dorg.jline.terminal.exec=false -dorg.jline.terminal.dumb=true
constructor for tests.
handle an exception while communication with the server. extracted into a method so that tests can bubble the failure.
serializes the search source to a byte array.
deserializes the search source from a byte array.
collect the necessary fields, modifying the searchsourcebuilder to retrieve them from the document.
all of the named writeables needed to deserialize the instances of
all of the named writeables needed to deserialize the instances of
parses an sql statement into execution plan
parses an expression - used only in tests
find the one and only order by in a plan.
create components used by the sql plugin.
actual implementation of the action. statically available to support embedded mode.
if the newcursor is empty, returns an empty cursor. otherwise, creates a new cliformattercursor that wraps the newcursor.
tests for cliformatter#formatwithheader, values of exactly the minimum column size, column names of exactly the minimum column size, column headers longer than the minimum column size, and values longer than the minimum column size.
tests for cliformatter#formatwithoutheader and truncation of long columns.
returns a random querybuilder or null
hint about how many results to fetch at once.
create a new cliformatter for formatting responses similar to the provided columns and rows.
format the provided sqlqueryresponse for the cli including the header lines.
serializes the provided value in sql-compatible way based on the client mode
the key that must be sent back to sql to access the next page of results.
constructor used for specifying a more descriptive message (typically 'did you mean') instead of the default one.
build a marker unresolvedfunction with an error message about the function being missing.
build a functiondefinition for a binary function that is not aware of time zone and does not support distinct.
build a functiondefinition for a no-argument function that is not aware of time zone and does not support distinct.
build a functiondefinition for a unary function that operates on a datetime.
build a functiondefinition for a unary function that is not aware of time zone and does not support distinct.
build a functiondefinition for a unary function that is not aware of time zone but does support distinct.
constructor specifying alternate functions for testing.
all of the named writeables needed to deserialize the instances of
returns true if the processor defintion builds a query that tracks scores, false otherwise. used for testing
trims the trailing whitespace characters from the given string. uses character#iswhitespace(char) to determine if a character is whitespace or not.
extract a substring from the given string, using start index and length of the extracted substring.
trims the leading whitespace characters from the given string. uses character#iswhitespace(char) to determine if a character is whitespace or not.
used in painless scripting
wrapper for nulls around the given function. if true, nulls are passed through, otherwise the function is short-circuited and null returned.
wrapper for nulls around the given function. if true, nulls are passed through, otherwise the function is short-circuited and null returned.
build a binary 'pyramid' from the given list:  and \ and and \ \ a b c d  using the given combiner. while a bit longer, this method creates a balanced tree as oppose to a plain recursive approach which creates an unbalanced one (either to the left or right).
creates the list of groupby keys
get the conversion from one type to another.
returns true if the from type can be converted to the to type, false - otherwise
returns the type compatible with both left and right types  if one of the types is null - returns another type if both types are numeric - returns type with the highest precision int < long < float < double if one of the types is string and another numeric - returns numeric
find all subclasses of a particular class.
the test class for some subclass of node or null if there isn't such a class or it doesn't extend
scans the .class files to identify all classes and checks if they are subclasses of node.
test node#replacechildren implementation on #subclass.
build a list of arguments to use when calling builds subclasses of node.
find the longest constructor of the given class. by convention, for all subclasses of node, this constructor should have "all" of the state of the node. all other constructors should all delegate to this constructor.
test node#transformpropertiesonly(java.util.function.function, class) implementation on #subclass which tests the implementation of implementations in the process.
make an argument to feed the #subclass's ctor.
make an argument to feed to the constructor for tobuildclass.
builds a nodeinfo for nodes without any properties.
transform the properties on node, returning a new instance of n if any properties change.
render the properties of this node one by one like foo bar baz. these go inside the
render this node as a tree like  project[[if#0]] \_filter[if#1] \_subqueryalias[test] \_esrelation[test][if#2] 
removes the query part of the uri
parses the url provided by the user and
build an error message from a parse failure.
parse a failure from the response. the stream is not closed when the parsing is complete. the caller must close it.
parses a generic value from the xcontent stream
translates a like pattern to pattern for es index name expression resolver. note the resolver only supports (not ?) and has no notion of escaping. this is not really an issue since we don't allow anyway in the pattern.
translates a like pattern to a lucene wildcard. this methods pays attention to the custom escape char which gets converted into \ (used by lucene).  % -> _ -> ? escape character - can be 0 (in which case every regex gets escaped) or should be followed by % or _ (otherwise an exception is thrown) 
converts rest column metadata into jdbc column metadata
read the next page of results and returning the scroll id to use to fetch the next page.
converts the object from json representation to the specified jdbctype  the returned types needs to correspond to es-portion of classes returned by typeconverter#classnameof
returns true if the type represents a signed number, false otherwise  it needs to support both params and column types
converts millisecond after epoc to time
translates numeric jdbc type into corresponding java class  see javax.sql.rowset.rowsetmetadataimpl#getcolumnclassname and https:db.apache.orgderbydocs10.5refrrefjdbc20377.html
converts object val from columntype to type
converts millisecond after epoc to date
skips a multi-line comment starting at the current position i, returns the length of the comment
skips jdbc escape sequence starting at the current position i, returns the length of the sequence
skips a line comment starting at the current position i, returns the length of the comment
returns number of parameters in the specified sql query
skips a string starting at the current position i, returns the length of the string
the sqlexception is the only type of exception the jdbc api can throw (and that the user expects). if we remove it, we need to make sure no other types of exceptions (runtime or otherwise) are thrown
returns the parameters if the sql statement is parametrized
resolves a pattern to multiple, separate indices.
resolves only the names, differentiating between indices and aliases. this method is required since the other methods rely on mapping which is tied to an index (not an alias).
resolves a pattern to one (potentially compound meaning that spawns multiple indices) mapping.
get the esindex
for a given root query (or a set of "includes" root constraints) find the related terms. these will be our start points in the graph navigation.
step out from some existing vertex terms looking for useful connections
shuts down the trigger service as well to make sure there are no lingering threads also no need to check anything, as this is final, we just can go to status stopped
reload watches and start scheduling them
ensure that watcher can be reloaded, by checking if all indices are marked as up and ready in the cluster state
wraps an abstract runnable to easier supply onfailure and dorun methods via lambdas this ensures that the uncaught exception handler in the executing threadpool does not get called
start the watcher service, load watches in the background
this reads all watches from the .watches indexalias and puts them into memory for a short period of time, before they are fed into the trigger service.
stops the watcher service and marks its services as paused
reload the watcher service, does not switch the state from stopped to started, just keep going
stop execution of watches on this node, do not try to reload anything, but still allow manual watch execution, i.e. via the execute watch api
reloads all the reloadable services in watcher.
stop certain parts of watcher, when there are no watcher indices on this node by checking the shardrouting note that this is not easily possible, because of the execute watch api, that needs to be able to execute anywhere! this means, only certain components can be stopped
if the index operation happened on a watcher shard and is of doc type watcher, we will remove the watch id from the trigger service
check if the routing table has changed and local shards are affected
this returns a mapping of the shard it to the index of the shard allocation ids in that list. the idea here is to have a basis for consistent hashing in order to decide if a watch needs to be triggered locally or on another system, when it is being indexed as a single watch action. example: - shardid(".watch", 0) - all allocation ids sorted (in the cluster): [ "a", "b", "c", "d"] - local allocation id: b (index position 1) - then store the size of the allocation ids and the index position data.put(shardid(".watch", 0), new tuple(1, 4))
listen for cluster state changes. this method will start, stop or reload the watcher service based on cluster state information. the method checks, if there are local watch indices up and running.
single watch operations that check if the local trigger service should trigger for this concrete watch watch parsing could be optimized, so that parsing only happens on primary and where the shard is supposed to be put into the trigger service at some point, right no we dont care note, we have to parse on the primary, because otherwise a failure on the replica when parsing the watch would result in failing the replica
in case of an error, we have to ensure that the triggerservice does not leave anything behind todo: if the configuration changes between preindex and postindex methods and we add a watch, that could not be indexed todo: this watch might not be deleted from the triggerservice. are we willing to accept this? todo: this could be circumvented by using a threadlocal in preindex(), that contains the watch and is cleared afterwards
reload the configuration if the alias pointing to the watch index was changed or the index routing table for an index was changed
create a mock cluster state, the returns the specified index as watch index
resolve the failure reason, when a reason can be extracted from the response body: ex: "errormessages":[],"errors":"customfield_10004":"epic name is required."  see https:docs.atlassian.comjirarestcloud for the format of the error response body.
turns the v1 api contexts into 2 distinct lists, images and links. the v2 api has separated these out into 2 top level fields.
renders the attachment in slack compatible structure:  https:api.slack.comdocsattachments#attachment_structure
finds a setting, and then a secure setting if the setting is null, or returns null if one does not exist. this differs from other getsetting calls in that it allows for null whereas the other methods throw an exception. note: if your setting was not previously secure, than the string reference that is in the setting object is still insecure. this is only constructing a new securestring with the char[] of the insecure setting.
loads the standard java mail properties as settings from the given account settings. the standard settings are not that readable, therefore we enabled the user to configure those in a readable way... this method first loads the smtp settings (which corresponds to all java mail mail.smtp. settings), and then replaces the readable keys to the official "unreadable" keys. we'll then use these settings when crea
intentionally not emitting path as it may come as an information leak
build the email. note that adding items to attachments or inlines after this is called is incorrect.
extract the id from json payload, so we know which id to poll for
use the default time to sleep between polls if it was not set
trigger the initial report generation and catch possible exceptions
returns some statistics about the watches loaded in the trigger service
removes the job associated with the given name from this trigger service.
checks whether the given time is the same or after the scheduled time of this schedule. if so, the scheduled time is returned a new scheduled time is computed and set. otherwise (the given time is before the scheduled time), -1 is returned.
reads the contents of parser to create the correct input
empty the currently queued tasks and wait for current executions to finish.
execute triggered watches, which have been successfully indexed into the triggered watches index
updates and persists the status of the given watch if the watch is missing (because it might have been deleted by the user during an execution), then this method does nothing and just returns without throwing an exception
this clears out the current executions and sets new empty current executions this is needed, because when this method is called, watcher keeps running, so sealing executions would be a bad idea
create a tuple of triggered watches and their corresponding contexts, usable for sync and async processing
gets a watch but in a synchronous way, so that no async calls need to be built
calling this method makes the class stop accepting new executions and throws and exception instead. in addition it waits for a certain amount of time for current executions to finish before returning
tries to put an watch execution class for a watch in the current executions
create a bulk request from the triggered watches with a specified document type
checks if any of the loaded watches has been put into the triggered watches index for immediate execution note: this is executing a blocking call over the network, thus a potential source of problems
creates a http proxy from the system wide settings
enriches the config object optionally with proxy information
returns all the headers, with keys being lowercased, so they are always consistent in the payload
write a request via toxcontent, but filter certain parts of it - this is needed to not expose secrets
in order to test, that .watches and .triggered-watches indices can also point to an alias, we will rarely create those after starting watcher the idea behind this is the possible use of the migration helper for upgrades, see https:github.comelasticelasticsearch-migration
get the "actions" from the watch history hit.
a hard failure is where an exception is thrown by the script condition.
create a watch with the specified id and input.  the actionconditions are
stores the specified watchrecord. any existing watchrecord will be overwritten.
check if everything is set up for the history store to operate fully. checks for the current watcher history index and if it is open.
stores the specified watchrecord. if the specified watchrecord already was stored this call will fail with a version conflict.
this test makes sure that when an action encounters an error it should not be subject to throttling. also, the ack status of the action in the watch should remain awaits_successful_execution as long as the execution fails.
extracts the specified field out of data map, or alternative falls back to the action value
merges the defaults provided as the second parameter into the content of the first while applying a function on both map key and map value.
reads a new watcher search request instance for the specified parser.
method to get indexmetadata of a index, that potentially is behind an alias.
iterates through the "properties" field of mappings and returns any predicates that match in the form of issue-strings.
returns status of shards currently finished snapshots  this method is executed on master node and it's complimentary to the returns similar information but for already finished snapshots. 
validates snapshot request
removes a finished snapshot from the cluster state. this can happen if the previous master node processed a cluster state update that marked the snapshot as finished, but the previous master node died before removing the snapshot in progress from the cluster state. it is then the responsibility of the new master node to end the snapshot and remove it from the cluster state.
initializes the snapshotting process.  this method is used by clients to start snapshot. it makes sure that there is no snapshots are currently running and creates a snapshot record in cluster state metadata.
retrieves snapshot from repository
removes record of running snapshot from cluster state and notifies the listener when this action is complete
deletes snapshot from repository.  if the snapshot is still running cancels the snapshot first and then deletes it from the repository.
calculates the list of shards that should be included into the current snapshot
deletes snapshot from repository
returns list of indices with missing shards, and list of indices that are closed
deletes a snapshot from the repository, looking up the snapshot reference before deleting. if the snapshot is still running cancels the snapshot first and then deletes it from the repository.
checks if a repository is currently in use by one of the snapshots
gets the repositorydata for the given repository.
returns a list of currently running snapshots from repository sorted by snapshot creation date
returns a list of snapshots from repository sorted by snapshot creation date if false, they will throw an error
cleans up shard snapshots that were running on removed nodes
check if any of the indices to be deleted are currently being snapshotted. fail as deleting an index that is being snapshotted (with partial == false) makes the snapshot fail.
returns status of the currently running snapshots  this method is executed on master node 
starts snapshot.  creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.
removes the snapshot deletion from snapshotdeletionsinprogress in the cluster state.
finalizes a snapshot deletion in progress if the current node is the master but it was not master in the previous cluster state and there is still a lingering snapshot deletion in progress in the cluster state. this means that the old master failed before it could clean up an in-progress snapshot deletion. we attempt to delete the snapshot files and remove the deletion from the cluster state. it is possible that the old master was in a state of long gc and then it resumes and tries to delete the snapshot that has already been deleted by the current master. this is acceptable however, since the old master's snapshot deletion will just respond with an error but in actuality, the snapshot was deleted and a call to get snapshots would reveal that the snapshot no longer exists.
check if any of the indices to be closed are currently being snapshotted. fail as closing an index that is being snapshotted (with partial == false) makes the snapshot fail.
finalizes the shard in repository and then removes it from cluster state  this is non-blocking method that runs on a thread from snapshot thread pool
returns status of shards that are snapshotted on the node and belong to the given snapshot  this method is executed on data node 
creates shard snapshot
checks if any shards were processed that the new master doesn't know about
updates the shard snapshot status by sending a updateindexshardsnapshotstatusrequest to the master node
checks if any new shards should be snapshotted on this node
updates the shard status on master node
tests that a shrunken index (created via the shrink apis) and subsequently snapshotted can be restored when the node the shrunken index was created on is no longer part of the cluster.
checks that snapshots can be restored and have compatible version
restores snapshot specified in the restore request.
optionally updates index settings in indexmetadata by removing settings listed in ignoresettings and merging them with settings in changesettings.
check if any of the indices to be closed are currently being restored from a snapshot and fail closing if such an index is found as closing an index that is being restored makes the index unusable (it cannot be recovered).
checks if a repository is currently in use by one of the snapshots
reads shard failure information from stream input
serializes snapshot failure information into json
constructs new snapshot shard failure object
gets a new snapshotinfo instance for a snapshot that is incompatible with the current version of the cluster.
constructs snapshot information from stream input
this method creates a snapshotinfo from internal x-content. it does not handle x-content written with the external version as external x-content is only for display purposes and does not need to be parsed.
returns snapshot rest status
tests that a snapshot with a corrupted global state file can still be deleted
tests that a snapshot with a corrupted global state file can still be restored
tests that a snapshot of multiple indices including one with a corrupted index metadata file can still be used to restore the non corrupted indices
this test ensures that when a shard is removed from a node (perhaps due to the node leaving the cluster, then returning), all snapshotting of that shard is aborted, so all store references held onto by the snapshot are released. see https:github.comelasticelasticsearchissues20876
execute the unrestorable test use case
constructs a snapshot from the stream input.
generate snapshot state from code
filters out list of available indices based on the list of selected indices.
reads restore info from streaminput
constructs a new snapshot from a input stream
blocks an io operation on the blob fails and throws an exception when unblocked
build a random search request.
the source of the document as string (can be null).
we need to declare parse fields for each metadata field, except for _id, _index and _type which are handled individually. all other fields are parsed to an entry in the fields map
returns bytes reference, also uncompress the source if needed.
rendering of the inner xcontent object without the leading field name. this way the structure innertoxcontent renders and fromxcontent parses correspond to each other.
throw an assertionerror if there are still in-flight contexts.
shortcut ids to load, we load only "from" and up to "size". the phase controller handles this as well since the result is always size shards for q_t_f
returns true iff the given search source builder can be early terminated by rewriting to a match none query. or in other words if the execution of a the search request can be early terminated without executing it. this is for instance not possible if a global aggregation is part of this request or if there is a suggest builder present.
try to load the query results from the cache or execute the query phase directly if the cache cannot be used.
this method does a very quick rewrite of the query and returns true if the query can potentially match any documents. this method can have false positives while if it returns false the query won't match any documents on the current shard.
should be called before executing the main query and after all other parameters have been set.
test that getting more than the allowed number of docvalue_fields throws an exception
test that getting more than the allowed number of script_fields throws an exception
return a sorteddocvalues instance that can be used to sort root documents with this mode, the provided values and filters for rootinner documents. for every root document, the values of its inner documents will be aggregated. allowed modes: min, max note: calling the returned instance on docs that are not root docs is illegal the returned instance can only be evaluate the current and upcoming docs
return a numericdoublevalues instance that can be used to sort documents with this mode and the provided values. when a document has no value, missingvalue is returned. allowed modes: sum, avg, median, min, max
return a binarydocvalues instance that can be used to sort root documents with this mode, the provided values and filters for rootinner documents. for every root document, the values of its inner documents will be aggregated. if none of the inner documents has a value, then missingvalue is returned. allowed modes: min, max note: calling the returned instance on docs that are not root docs is illegal the returned instance can only be evaluate the current and upcoming docs
return a sorteddocvalues instance that can be used to sort documents with this mode and the provided values. allowed modes: min, max
return a numericdocvalues instance that can be used to sort documents with this mode and the provided values. when a document has no value, missingvalue is returned. allowed modes: sum, avg, median, min, max
return a numericdocvalues instance that can be used to sort root documents with this mode, the provided values and filters for rootinner documents. for every root document, the values of its inner documents will be aggregated. if none of the inner documents has a value, then missingvalue is returned. allowed modes: sum, avg, min, max note: calling the returned instance on docs that are not root docs is illegal the returned instance can only be evaluate the current and upcoming docs
return a binarydocvalues instance that can be used to sort documents with this mode and the provided values. when a document has no value, missingvalue is returned. allowed modes: min, max
return a numericdoublevalues instance that can be used to sort root documents with this mode, the provided values and filters for rootinner documents. for every root document, the values of its inner documents will be aggregated. if none of the inner documents has a value, then missingvalue is returned. allowed modes: sum, avg, min, max note: calling the returned instance on docs that are not root docs is illegal the returned instance can only be evaluate the current and upcoming docs
a case insensitive version of #valueof(string)
test for edge case where field level boosting is applied to field that doesn't exist on documents on one shard. there was an issue reported in https:github.comelasticelasticsearchissues18710 where a `multi_match` query using the fuzziness parameter with a boost on one of two fields returns the same document score if both documents are placed on different shard. this test recreates that scenario and checks that the returned scores are different.
returns whether collection within the provided reader can be early-terminated if it sorts with sortandformats.
in a package-private method so that it can be tested without having to wire everything (mapperservice, etc.)
returns true if the provided query returns docs in index order (internal doc ids).
ctr
returns query total hit count if the query is a matchalldocsquery or a termquery and the reader has no deletions, -1 otherwise.
ctr
ctr
creates a topdocscollectorcontext from the provided searchcontext.
returns and nulls out the top docs for this search results. this allows to free up memory once the top docs are consumed.
returns and nulls out the aggregation for this search results. this allows to free up memory once the aggregation is consumed.
returns and nulls out the profiled results for this search, or potentially null if result was empty. this allows to free up memory once the profiled result is consumed.
creates the collector tree from the provided collectors and wraps each collector with a profiler
creates a multi collector from the provided subs
filters documents with a query score greater than minscore
creates collector limiting the collection to the first numhits documents
creates the collector tree from the provided collectors
creates a collector that throws taskcancelledexception if the search is cancelled
filters documents based on the provided query
test that we can parse the `rescore` element either as single object or as array
add an aggregation to perform as part of the search.
read from a stream.
from index to start the search from. defaults to 0.
indicate that _source should be returned with every hit, with an "include" andor "exclude" set which can include simple wildcard elements. an optional list of include (optionally wildcarded) pattern to filter the returned _source an optional list of exclude (optionally wildcarded) pattern to filter the returned _source
add a sort against the given field name. the name of the field to sort by
an optional terminate_after to terminate the search after collecting terminateafter documents
add an aggregation to perform as part of the search.
adds a field to load from the doc values and return as part of the search request.
indicates whether the response should contain the stored _source for every hit
the sort values that indicates which docs this request should "search after". the sort values of the search_after must be equal to the number of sort fields in the query and they should be of the same type (or parsable as such). defaults to null.
sets the stored fields to load and return as part of the search request. if none are specified, the source of the document will be returned.
the number of search hits to return. defaults to 10.
adds a sort builder.
adds a sort against the given field name and the sort ordering. the name of the field the sort ordering
parse some xcontent into this searchsourcebuilder, overwriting any values specified in the xcontent. use this if you need to set up different defaults than a regular searchsourcebuilder would have and use #fromxcontent(xcontentparser, boolean) if you have normal defaults.
create a shallow copy of this source replaced #querybuilder, #postquerybuilder, and #slicebuilder. used by
adds a script field under the given name with the provided script. the name of the field the script
sets the boost a specific index or alias will receive when the query is executed against it. the index or alias to apply the boost against the boost to apply to the index
read from a stream.
test case for #5132: source filtering with wildcards broken when given multiple patterns https:github.comelasticelasticsearchissues5132
explicitly tests what you can't list as a sortvalue. what you can list is tested by #randomsearchafterbuilder().
read from a stream.
returns the inner sortfield.type expected for this sort field.
make sure min_score works if functions is empty, see https:github.comelasticelasticsearchissues10253
returns the groupshardsiterator for the provided request.
converts this querybuilder to a lucene query.
returns a docidset per segments containing the matching docs for the specified slice.
adds a field name field to the list of fields to load.
adds the field names fieldnames to the list of fields to load.
test case for issue #4361: https:github.comelasticelasticsearchissues4361
serialization constructor.
parse a fieldandformat from some xcontent.
load field values for highlighting.
create random highlight builder that is put under test
mutate the given highlighter builder so the returned one is different in one aspect
test that build() outputs a searchcontexthighlight that is has similar parameters than what we have in the random highlightbuilder
creates random highlighter, renders it to xcontent and back to new instance that should be equal to original
setup for the whole base test class
create array of unique strings. if not unique, e.g. duplicates field names would be dropped in fieldoptions.builder#matchedfields(set), resulting in test glitches
test parsing empty highlight or empty fields blocks
read from a stream.
transfers field options present in the input abstracthighlighterbuilder to the receiving
set a tag scheme that encapsulates a built in pre and post tags. the allowed schemes are styled and default.
read from a stream.
fixes problems with broken analysis chains if positions and offsets are messed up that can lead to
test phrase boosting over normal term matches. note that this will never pass with the plain highlighter because it doesn't support the concept of terms having a different weight based on position.
the fhv can spend a long time highlighting degenerate documents if phraselimit is not set. its default is now reasonably low.
read from a stream.
write common parameters to streamoutput
test that orientation parameter correctly persists across cluster restart
test case for issue 6150: https:github.comelasticelasticsearchissues6150
copy constructor.
constructs a new distance based sort on a geo point like field.
sets the nestedsortbuilder to be used for fields that are inside a nested object. the nestedsortbuilder takes a `path` argument and an optional nested filter that the nested objects should match with in order to be taken into account for sorting.
read from a stream.
creates a new geodistancesortbuilder from the query held by the xcontentparser in side effect of this method call further parameters, e.g. in ' "foo": "order" : "asc" '. when parsing the inner object, the field name can be passed in via this argument
defines which distance to use for sorting in the case a document contains multiple geo points. possible values: min and max
sets the nested filter that the nested objects should match with in order to be taken into account for sorting. and retrieve with #getnestedsort()
constructs a new distance based sort on a geo point like field.
sets the nested path if sorting occurs on a field that is inside a nested object. by default when sorting on a field inside a nested object, the nearest upper nested object is selected as nested path. and retrieve with #getnestedsort()
issue 3073
write this object's fields to a streamoutput.
read from a stream.
copy constructor.
sets the nested filter that the nested objects should match with in order to be taken into account for sorting.
sets the nested path if sorting occurs on a field that is inside a nested object. by default when sorting on a field inside a nested object, the nearest upper nested object is selected as nested path.
constructs a new sort based on a document field. the field name.
sets the nestedsortbuilder to be used for fields that are inside a nested object. the nestedsortbuilder takes a `path` argument and an optional nested filter that the nested objects should match with in order to be taken into account for sorting.
test that the sort builder mode gets transfered correctly to the sortfield
test that if coercion is used, a point gets normalized but the original values in the builder are unchanged
test the nested sort gets rewritten
test the nested filter gets rewritten
test we can either set nested sort via pathfilter or via nested sort builder, not both
test that the sort builder nested object gets created in the sortfield
test that the sort builder order gets transfered correctly to the sortfield
test that if validation is strict, invalid points throw an error
test that the sort builder nested object gets created in the sortfield
test the nested sort gets rewritten
test that the sort builder order gets transfered correctly to the sortfield
test that the sort builder mode gets transfered correctly to the sortfield
test the nested filter gets rewritten
test we can either set nested sort via pathfilter or via nested sort builder, not both
test that min, max mode work on non-numeric fields, but other modes throw exception
test that missing values get transfered correctly to the sortfield
test two syntax variations: - "sort" : "fieldname" - "sort" : "fieldname" : "asc"
test parsing random syntax variations
test array syntax variations: - "sort" : [ "fieldname", "fieldname2" : "asc" , ...]
test parsing order parameter if specified as `order` field in the json instead of the `reverse` field that we render in toxcontent
return the minimal value from a set of values.
create a nestedsortbuilder with random path and filter of the given depth.
test that filters and inner nested sorts get rewritten
sets the nested filter that the nested objects should match with in order to be taken into account for sorting.
sets the nested path if sorting occurs on a field that is inside a nested object. for sorting by script this needs to be specified.
read from a stream.
defines which distance to use for sorting in the case a document contains multiple values. for scriptsorttype#string, the set of possible values is restricted to sortmode#min and sortmode#max
sets the nestedsortbuilder to be used for fields that are inside a nested object. the nestedsortbuilder takes a `path` argument and an optional nested filter that the nested objects should match with in order to be taken into account for sorting.
test the nested filter gets rewritten
test the nested sort gets rewritten
test that the correct comparator sort is returned, based on the script type
test that the sort builder nested object gets created in the sortfield
test that the sort builder mode gets transfered correctly to the sortfield
test we can either set nested sort via pathfilter or via nested sort builder, not both
script sort of type scriptsorttype does not work with sortmode#avg, sortmode#median or sortmode#sum
return a field type. we use numberfieldmapper.numberfieldtype by default since it is compatible with all sort modes tests that require other field types can override this.
test that build() outputs a sortfield that is similar to the one we would get when parsing the xcontent the sort builder is rendering out
test that creates new sort from a random test sort and checks both for equality
tests that we use an optimization shrinking the batch to the size of the shard. thus the integer.max_value window doesn't oom us.
returns a new topdocs with the topn from the incoming one, or the same topdocs if the number of hits is already <= topn.
modifies incoming topdocs (in) by replacing the top hits with resorted's hits, and then resorting all hits.
creates random rescorer, renders it to xcontent and back to new instance that should be equal to original
test that build() outputs a rescorecontext that has the same properties than the test builder
setup for the whole base test class
create random shape that is put under test
read from a stream.
creates a new queryrescorerbuilder instance
convert this record to a map from timingtype to times.
sole constructor.
read from a stream.
switch to a new profile.
sole constructor. this profilers instance will initially wrap one queryprofiler.
recursive helper to finalize a node in the dependency tree
helper method to add a new node to the dependency tree. initializes a new list in the dependency tree, saves the query and generates a new queryprofilebreakdown to track the timings of this query the element to profile the assigned token for this element
after the query has been run and profiled, we need to merge the flat timing map with the dependency graph to build a data structure that mirrors the original query tree
returns a queryprofilebreakdown for a scoring query. scoring queries (e.g. those that are past the rewrite phase and are now being wrapped by createweight() ) follow a recursive progression. we can track the dependency tree by a simple stack the only hiccup is that the first scoring query will be identical to the last rewritten query, so we need to take special care to fix that
internal helper to add a child to the current parent node
return the number of times that #start() has been called.
return an approximation of the total time spent between consecutive calls of #start and #stop.
helper method to convert profiler into internalprofileshardresults, which can be serialized to other nodes, emitted as json, etc. the profilers to convert into results shard
creates a human-friendly representation of the collector name. internalbucket collectors use the aggregation name in their tostring() method, which makes the profiled output a bit nicer.
read from a stream.
read from a stream.
set the collector that is associated with this profiler.
this test verifies that the output is reasonable for a simple, non-nested query
tests a series of three nested boolean queries with a single "leaf" match query. the rewrite process will "collapse" this down to a single bool, so this tests to make sure nothing catastrophic happens during that fairly substantial rewrite
this test makes sure no profile results are returned when profiling is disabled
this test simply checks to make sure nothing crashes. test indexes 100-150 documents, constructs 20-100 random queries and tries to profile them
tests a boolean query with no children clauses
this test verifies that the output is reasonable for a nested query
this test generates 1-10 random queries and executes a profiled and non-profiled search for each query. it then does some basic sanity checking of score and hits to make sure the profiling doesn't interfere with the hits being returned
read from a stream.
test that parsing works for a randomly created aggregations object with a randomized aggregation tree. the test randomly chooses an and randomly sets the `humanreadable` flag when rendering the if set, this will also add random xcontent fields to tests that the parsers are lenient to future additions to rest responses
returns whether one of the parents is a bucketsaggregator.
reduces the given aggregations to a single one and returns it. in most cases, the assumption will be the all given aggregations are of the same type (the same type as this aggregation). for best efficiency, when implementing, try reusing an existing instance (typically the first in the given list) to save on redundant object construction.
write a size under the assumption that a value of 0 means unlimited.
read from a stream.
read from a stream.
registers sub-factories with this factory. the sub-factory will be responsible for the creation of sub-aggregators under the aggregator created by this factory. the sub-factories
add a sub aggregation to this aggregation.
create all aggregators so that they can be consumed with multiple buckets.
rewrites the underlying aggregation builders into their primitive form. if the builder did not change the identity reference must be returned otherwise the builder will be rewritten infinitely.
read from a stream.
test serialization and deserialization of the test aggregatorfactory.
generic test that creates new aggregatorfactory from the test aggregatorfactory and checks both for equality and asserts equality on the two queries.
generic test that checks that the tostring method renders the xcontent correctly.
setup for the whole base test class.
test serialization and deserialization of the test aggregatorfactory.
generic test that creates new aggregatorfactory from the test aggregatorfactory and checks both for equality and asserts equality on the two queries.
parse a token of type xcontentparser.token.value_number or xcontentparser.token.string to a double. in other cases the default value is returned instead.
wrap the given collectors into a single instance.
sub-tests that need a more complex mock can overwrite this
added to randomly run with more assertions on the index searcher level, like org.apache.lucene.util.lucenetestcase#newsearcher(indexreader), which can't be used because it also wraps in the indexsearcher's indexreader with other implementations that we can't handle. (e.g. parallelcompositereader)
create a factory for the given aggregation builder.
added to randomly run with more assertions on the index reader level, like org.apache.lucene.util.lucenetestcase#wrapreader(indexreader), which can't be used because it also wraps in the indexreader with other implementations that we can't handle. (e.g. parallelcompositereader)
divides the provided indexsearcher in sub-searcher, one for each segment, builds an aggregator for each sub-searcher filtered by the provided query and returns the reduced internalaggregation.
constructs a new aggregator factory. the aggregation name if an error occurs creating the factory
utility method. given an aggregatorfactory that creates returns an aggregator that can collect any bucket.
reduces the given lists of addaggregation.
constructs a new aggregator.
called after collection of all document is done.
most aggregators don't need scores, make sure to extend this method if your aggregator needs them.
some top aggs (eg. date_histogram) that are executed on unmapped fields, will generate an estimate count of buckets - zero. when the sub aggregator is then created, it will take this estimation into account. this used to cause and an arrayindexoutofboundsexception...
making sure that if there are multiple aggregations, working on the same field, yet require different value source type, they can all still work. it used to fail as we used to cache the valuesource by the field name. if the cached value source was of type "bytes" and another aggregation on the field required to see it as "numeric", it didn't work. now we cache the value sources by a custom key (field name + valuesource type) so there's no conflict there.
returns the aggregations keyed by aggregation name.
directly write all the aggregations without their bounding object. used by sub-aggregations (non top level aggs)
rewrites the given aggregation into its primitive form. aggregations that for instance fetch resources from remote hosts or can simplify optimize itself should do their heavy lifting during #rewrite(queryrewritecontext). this method rewrites the aggregation until it doesn't change anymore.
constructs a new aggregation builder.
only for histogram order: backwards compatibility logic to read a bucketorder from a streaminput. the order. false to skip this flag (order always present).
read a bucketorder from a streaminput.
only for histogram order: backwards compatibility logic to write a bucketorder to a stream. writing the order. false to skip this flag.
write a bucketorder to a streamoutput.
determine if the ordering strategy matches the expected one. check instead.
create a new ordering strategy to sort by multiple criteria.
parse a bucketorder from xcontent.
counts the number of inner buckets inside the provided internalbucket
counts the number of inner buckets inside the provided aggregation
constructs a new pipeline aggregator factory. the aggregation name
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
this test forces the geoboundsaggregator to resize the bigarrays it uses to ensure they are resized correctly
test that when passing in keyed filters as a map they are equivalent
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
make sure that a request using a script does not get cached and a request not using a script does get cached.
read from a stream.
read from a stream.
the number of search hits to return. defaults to 10.
adds highlight to perform as part of the search.
add a sort against the given field name. the name of the field to sort by
adds a script field under the given name with the provided script. the name of the field the script
read from a stream.
sets the stored fields to load and return as part of the search request. to disable the stored fields entirely (source and metadata fields) use storedfield("_none_").
adds a sort builder.
indicate that _source should be returned with every hit, with an "include" andor "exclude" set which can include simple wildcard elements. an optional include (optionally wildcarded) pattern to filter the returned _source an optional exclude (optionally wildcarded) pattern to filter the returned _source
indicates whether the response should contain the stored _source for every hit
adds a sort builder.
indicate that _source should be returned with every hit, with an "include" andor "exclude" set which can include simple wildcard elements. an optional list of include (optionally wildcarded) pattern to filter the returned _source an optional list of exclude (optionally wildcarded) pattern to filter the returned _source
adds a script field under the given name with the provided script. the name of the field the script
from index to start the search from. defaults to 0.
adds a field to load from doc values and return as part of the search request.
indicate how the _source should be fetched.
adds a sort against the given field name and the sort ordering. the name of the field the sort ordering
tests top_hits inside of terms. while not strictly a unit test this is a fairly common way to run top_hits and serves as a good example of running top_hits inside of another aggregation.
read from a stream.
read from a stream.
set the values to compute percentiles from.
expert: set the number of significant digits in the values. only relevant when using percentilesmethod#hdr.
expert: set the compression. higher values improve accuracy but also memory usage. only relevant when using percentilesmethod#tdigest.
expert: set the compression. higher values improve accuracy but also memory usage. only relevant when using percentilesmethod#tdigest.
read from a stream.
expert: set the number of significant digits in the values. only relevant when using percentilesmethod#hdr.
read from a stream.
read from a stream.
read from a stream.
set a precision threshold. higher values improve accuracy but also increase memory usage.
read from a stream.
add k to the hash table associated with bucket. return -1 if the value was already in the set or the new set size if it was added.
mock of the script service. the script that is run looks at the "states" context variable visible when executing the script and simply returns the count. this should be equal to the number of input internalscriptedmetrics that are reduced in total.
test that an explicitly added _agg param is honored
we cannot use mockito for mocking queryshardcontext in this case because script-related methods (e.g. queryshardcontext#getlazyexecutablescript) is final and cannot be mocked
test that the _agg param is implicitly added
test that uses the score of the documents
we cannot use mockito for mocking queryshardcontext in this case because script-related methods (e.g. queryshardcontext#getlazyexecutablescript) is final and cannot be mocked
without combine script, the "states" map should contain a list of the size of the number of documents matched
test that combine script sums the list produced by the "mapscript"
mock of the script service. the script that is run looks at the "_aggs" parameter to verify that it was put in place by internalscriptedmetric.
set the reduce script.
set parameters that will be available in the init,
read from a stream.
set the map script.
set the init script.
set the combine script.
read from a stream.
read from a stream.
read from a stream.
read from a stream.
read from a stream.
read from a stream.
read from a stream.
read from a stream.
return a map of field names and data
return the covariance between two fields
creates a results object from the given stream
return the value for two fields in an upper triangular matrix, regardless of row col location.
computes final covariance, variance, and correlation
creates and computes result from provided stats
return the correlation coefficient between two fields
marshalls matrixstatsresults
merges the descriptive statistics of a second data set (e.g., per shard) running computations taken from: http:prod.sandia.govtechlibaccess-control.cgi2008086212.pdf
updates running statistics with a documents field values
ctor to create an instance of running statistics
update covariance matrix
merges two covariance matrices
get the variance for the given field
read from a stream.
get the mean for the given field
get the correlation between the two fields
get the distribution shape for the given field
get the covariance between the two fields
get the number of samples for the given field. == doccount - nummissing
get the distribution skewness for the given field
test merging stats across observation shards
resolves the topmost aggregator pointed by this path using the given root as a point of reference.
resolves the aggregator pointed by this path using the given root as a point of reference.
validates this path over the given aggregator as a point of reference.
resolves the value pointed by this path given an aggregations root
sets the valuetype for the value produced by this aggregation
sets the field to use for this aggregation.
sets the time zone to use for this aggregation
sets the script to use for this aggregation.
sets the format to use for the output of the aggregation.
read an aggregation from a stream that serializes its targetvaluetype. this should only be used by subclasses that override
read from a stream.
sets the value to use when the aggregation finds a missing value in a document
sets the format to use for the output of the aggregation.
read from a stream.
sets the value to use when the aggregation finds a missing value in a document
sets the valuetype for the value produced by this aggregation
sets the field to use for this aggregation.
get a value source given its configuration. a return value of null indicates that no value source could be built.
resolve a valuessourceconfig given configuration parameters.
return the original values source, before we apply `missing`.
read from a stream.
sets the valuetype for the value produced by this aggregation
sets the format to use for the output of the aggregation.
first value from a derivative is null, so this makes sure the cusum can handle that
do a derivative on a date histogram with time zone cet at dst start
also check for time zone shifts that are not one hour, e.g. "asiakathmandu, 1 jan 1986 - time zone change (ist  npt), at 00:00:00 clocks were turned forward 00:15 minutes
do a derivative on a date histogram with time zone cet at dst end
generates a mock histogram to use for testing. each mockbucket holds a doc count, key and document values which can later be used to compute metrics and compare against the real aggregation results. gappiness can be controlled via parameters
read from a stream.
test for https:github.comelasticelasticsearchissues17701
read from a stream.
parse a string gappolicy into the byte enum gappolicy in string format (e.g. "ignore")
deserialize the gappolicy from the input stream
test first and second derivative on the sing
test first and second derivative on the sing
sets the gap policy to use for this aggregation.
read from a stream.
sets the format to use on the output of this aggregation.
read from a stream.
read from a stream.
read from a stream.
read from a stream.
set the value of sigma to use when calculating the standard deviation bounds
read from a stream.
set the percentages to calculate percentiles for in this aggregation
check that we don't rely on the percent array order and that the iterator returns the values in the original order
read from a stream.
test for issue #30608. under the following circumstances: a. multi-bucket agg in the first entry of our internal list b. regular agg as the immediate child of the multi-bucket in a c. regular agg with the same name as b at the top level, listed as the second entry in our internal list d. finally, a pipeline agg with the path down to b bucketmetrics reduction would throw a class cast exception due to bad subpathing. this test ensures it is fixed. note: we have this test inside of the `avg_bucket` package so that we can get access to the package-private `doreduce()` needed for testing this
sets the gap policy to use for this aggregation.
sets the format to use on the output of this aggregation.
used for serialization testing, since pipeline aggs serialize themselves as a named object but are parsed as a regular object with the name passed in.
sets the window size for this aggregation
version of holt that can "forecast", not exposed as a whitelisted function for moving_fn scripts, but here as compatibilitycode sharing for existing moving_avg agg. can be removed when moving_avg is gone.
version of holt-winters that can "forecast", not exposed as a whitelisted function for moving_fn scripts, but here as compatibilitycode sharing for existing moving_avg agg. can be removed when moving_avg is gone.
returns an empty set of predictions, filled with nans
read from a stream.
read from a stream.
these models are all minimizable, so they should not throw exceptions
test simple moving average on single value field
if the minimizer is turned on, but there isn't enough data to minimize with, it will simply use the default settings. which means our mock histo will match the generated result (which it won't if the minimizer is actually working, since the coefficients will be different and thus generate different data) we can simulate this by setting the window size == size of histo
only some models can be minimized, should throw exception for: simple, linear
simple, unweighted moving average
holt winters (triple exponential) moving avg
calculates the moving averages for a specific (model, target) tuple based on the previously generated mock histogram. computed values are stored in the testvalues map.
read from a stream.
read from a stream.
sets the gap policy to use for this aggregation.
sets the format to use on the output of this aggregation.
read from a stream.
read from a stream.
calculates the "cost" of a model. e.g. when run on the training data, how closely do the predictions match the test data uses least absolute differences to calculate error. note that this is not scale free, but seems to work fairly well in practice
runs the simulated annealing algorithm and produces a model with new coefficients that, theoretically fit the data better and generalizes to future forecasts without overfitting. generated from a cost for the model
read from a stream.
sets the window size for the moving average. this window will "slide" across the series, and the values inside that window will be used to calculate the moving avg value size of window
sets the number of predictions that should be returned. each prediction will be spaced at the intervals specified in the histogram. e.g "predict: 2" will return two new buckets at the end of the histogram with the predicted values. number of predictions to make
sets a movavgmodel for the moving average. the model is used to define what type of moving average you want to use on the series a movavgmodel which has been prepopulated with settings
sets the format to use on the output of this aggregation.
sets the gappolicy to use on the output of this aggregation.
sets a movavgmodel for the moving average. the model is used to define what type of moving average you want to use on the series a movavgmodel which has been prepopulated with settings
read from a stream.
extracts an integer from the settings map, otherwise throws an exception
predicts the next `n` values in the series.
extracts a boolean from the settings map, otherwise throws an exception
returns an empty set of predictions, filled with nans
extracts a 0-1 inclusive double from the settings map, otherwise throws an exception
calculate a holt-linear (doubly exponential weighted) moving average
read from a stream.
parse a string seasonalitytype into the byte enum
deserialize the seasonalitytype from the input stream
read from a stream.
sets the format to use on the output of this aggregation.
read from a stream.
sets the gappolicy to use on the output of this aggregation.
sets the lag to use when calculating the serial difference.
make sure that a request using a script does not get cached and a request not using a script does get cached.
test date histogram aggregation with day interval, offset and extended bounds (see https:github.comelasticelasticsearchissues23776)
the script will change to document date values to the following:  doc 1: [ feb 2, mar 3] doc 2: [ mar 2, apr 3] doc 3: [ mar 15, apr 16] doc 4: [ apr 2, may 3] doc 5: [ apr 15, may 16] doc 6: [ apr 23, may 24]
test date histogram aggregation with hour interval, timezone shift and extended bounds (see https:github.comelasticelasticsearchissues12278)
https:github.comelasticelasticsearchissues31392 demonstrates an edge case where a date field mapping with "format" = "epoch_millis" can lead for the date histogram aggregation to throw an error if a non-utc time zone with daylight savings time is used. this test was added to check this is working now
jan 2 feb 2 feb 15 mar 2 mar 15 mar 23
when dst ends, local time turns back one hour, so between 2am and 4am wall time we should have four buckets: "2015-10-25t02:00:00.000+02:00", "2015-10-25t02:00:00.000+01:00", "2015-10-25t03:00:00.000+01:00", "2015-10-25t04:00:00.000+01:00".
heuristic used to determine the size of shard-side priorityqueues when selecting the top n terms from a distributed index. the number of terms required in the final reduce phase. the number of shards being queried.
make sure that a request using a script does not get cached and a request not using a script does get cached.
required method to build the child aggregations of the given bucket (identified by the bucket ordinal).
utility method to build empty aggregations of the sub aggregators.
utility method to return the number of documents that fell in the given bucket (identified by the bucket ordinal)
test that when passing in keyed filters as list or array, the list stored internally is sorted by key also check the list passed in is not modified by this but rather copied
make sure that a request using a script does not get cached and a request not using a script does get cached.
shift buckets by random offset between [2..interval]. from setup we have 1 doc per values from 1..numdocs. special care needs to be taken for expecations on counts in first and last bucket.
make sure that a request using a script does not get cached and a request not using a script does get cached.
set offset so day buckets start at 6am. index first 12 hours for two days, with one day gap.
read from a stream.
wrap the provided aggregator so that it behaves (almost) as if it had been collected directly.
replay the wrapped collector, but only on a selection of buckets.
test a case where we know exactly how many of each term is on each shard so we know the exact error value for each term. to do this we search over 3 one-shard indices.
wrap the provided aggregator so that it behaves (almost) as if it had been collected directly.
replay the wrapped collector, but only on a selection of buckets.
make sure that a request using a script does not get cached and a request not using a script does get cached.
test querying ranges on date mapping specifying a format with tofrom values specified as strings
make sure that a request using a script does not get cached and a request not using a script does get cached.
read from a stream.
read from a stream.
resolve any strings in the ranges so we have a number value for the from and to of each range. the ranges are also sorted before being returned.
read from a stream.
read from a stream.
read from a stream.
checks the invariant that bucket keys are always non-null, even if null keys were originally provided.
set the max num docs to be returned per value.
read from a stream.
set the max num docs to be returned from each shard.
sole constructor. the number of top-scoring docs to collect for each bucket
uses the sampler aggregation to find the minimum value of a field out of the top 3 scoring documents in a search.
sets the script to use for this source
sets the field to use for this source
sets the format to use for the output of the aggregation.
sets the sortorder to use to sort values produced this source
sets the sortorder to use to sort values produced this source
sets the valuetype for the value produced by this source
visits all non-deleted documents in iterator and fills the provided queue with the top composite buckets extracted from the collection. documents that contain a top composite bucket are added in the provided builder if it is not null. returns true if the queue is full and the current leadsourcebucket did not produce any competitive composite buckets.
returns true if a sorteddocsproducer should be used to optimize the execution.
sets the interval on this source.
sets the interval on this source. if both #interval() and #datehistograminterval() are set, then the #datehistograminterval() wins.
sets the time zone to use for this aggregation
copies the current value in slot.
check if the current candidate should be added in the queue.
constructs a composite queue with the specified size and sources.
builds the compositekey for slot.
format obj using the provided docvalueformat. if the format is equals to docvalueformat#raw, the object is returned as is for numbers and a string for bytesrefs.
replay the documents that might contain a top bucket and pass top buckets to the #deferredcollectors.
the first pass selects the top composite buckets from all matching documents.
replay the top buckets from the matching documents.
set the separator used to join pairs of bucket keys
get the filters. this will be an unmodifiable map
read from a stream.
read from a stream.
read from a stream.
read from a stream, for internal use only.
sets the order in which the buckets will be returned. a tie-breaker may be added to avoid non-deterministic ordering.
set the minimum count of matching documents that buckets need to have and return this builder so that calls can be chained.
set extended bounds on this histogram, so that buckets would also be generated on intervals that did not match any documents.
set the offset on this builder, as a time value, and return the builder so that calls can be chained.
set the interval on this builder, and return the builder so that calls can be chained. if both #interval() and #datehistograminterval() are set, then the
set a new order on this builder and return the builder so that calls can be chained. a tie-breaker may be added to avoid non-deterministic ordering.
set the interval on this builder, and return the builder so that calls can be chained. if both #interval() and #datehistograminterval() are set, then the
return the interval as a date time unit if applicable. if this returns
this method works almost exactly the same as internaldatehistogram#reducebuckets(list, reducecontext), the different here is that we need to round all the keys we see using the highest level rounding returned across all the shards so the resolution of the buckets is the same and they can be reduced together.
stream from a stream.
read from a stream.
stream from a stream.
read from a stream.
parse the bounds and perform any delayed validation. returns the result of the parsing.
read from a stream.
construct a random extendedbounds in pre-parsed form.
convert an extended bounds in parsed for into one in unparsed form.
stream from a stream.
read from a stream.
build roundings, computed dynamically as roundings are time zone dependent. the current implementation probably should not be invoked in a tight loop.
set extended bounds on this builder: buckets between minbound and buckets. if maxbound is less that minbound, or if either of the bounds are not finite.
read from a stream, for internal use only.
set the interval on this builder, and return the builder so that calls can be chained.
sets the order in which the buckets will be returned. a tie-breaker may be added to avoid non-deterministic ordering.
set the minimum count of matching documents that buckets need to have and return this builder so that calls can be chained.
set a new order on this builder and return the builder so that calls can be chained. a tie-breaker may be added to avoid non-deterministic ordering.
returns the weight for this filter aggregation, creating it if necessary. this is done lazily so that the weight is only created if the aggregation collects documents reducing the overhead of the aggregation in teh case where no documents are collected. note that as aggregations are initialsed and executed in a serial manner, no concurrency considerations are necessary here.
read from a stream.
read from a stream.
the name of this aggregation set the filter to use, only documents that match this filter will fall into the bucket defined by this
read from a stream.
the name of this aggregation the filters to use with this aggregation
set the key to use for the bucket for documents not matching any filter.
returns the weights for this filter aggregation, creating it if necessary. this is done lazily so that the weights are only created if the aggregation collects documents reducing the overhead of the aggregation in the case where no documents are collected. note that as aggregations are initialsed and executed in a serial manner, no concurrency considerations are necessary here.
set the path to use for this nested aggregation. the path must match the path to a nested object in the mappings. if it is not specified then this aggregation will go back to the root document.
the name of this aggregation the path to use for this nested aggregation. the path must match the path to a nested object in the mappings.
read from a stream.
read from a stream.
read from a stream.
read from a stream.
uses the significant text aggregation to find the keywords in text fields
test documents with arrays of text
uses the significant terms aggregation to find the keywords in text fields
uses the significant terms aggregation on an index with unmapped field
uses the significant terms aggregation to find the keywords in numeric fields
read from a stream.
sets the shard_size - indicating the number of term buckets each shard will return to the coordinating node (the node that coordinates the search execution). the higher the shard size is, the more accurate the results are.
set the minimum document count terms should have in order to appear in the response.
set the minimum document count terms should have on the shard in order to appear in the response.
sets the size - indicating how many term buckets should be returned (defaults to 10)
set the minimum document count terms should have on the shard in order to appear in the response.
set the minimum document count terms should have in order to appear in the response.
read from a stream.
sets the shard_size - indicating the number of term buckets each shard will return to the coordinating node (the node that coordinates the search execution). the higher the shard size is, the more accurate the results are.
sets the size - indicating how many term buckets should be returned (defaults to 10)
read from a stream.
get the maximum global ordinal value for the provided valuessource or -1 if the values source is not an instance of valuessource.bytes.withordinals.
computes which global ordinals are accepted by this includeexclude instance.
read from a stream.
converts a longterms into a doubleterms, returning the value of the specified long terms as doubles.
read from a stream.
make sure that a request using a script does not get cached and a request not using a script does get cached.
sets the shard_size - indicating the number of term buckets each shard will return to the coordinating node (the node that coordinates the search execution). the higher the shard size is, the more accurate the results are.
sets the order in which the buckets will be returned. a tie-breaker may be added to avoid non-deterministic ordering.
expert: set the collection mode.
set a new order on this builder and return the builder so that calls can be chained. a tie-breaker may be added to avoid non-deterministic ordering.
set the minimum document count terms should have in order to appear in the response.
sets the size - indicating how many term buckets should be returned (defaults to 10)
read from a stream.
set the minimum document count terms should have on the shard in order to appear in the response.
read from a stream.
internal optimization for ordering internalterms.buckets by a sub aggregation.  in this phase, if the order is based on sub-aggregations, we need to use a different comparator to avoid constructing buckets for ordering purposes (we can potentially have a lot of buckets and building them will cause loads of redundant object constructions). the "special" comparators here will fetch the sub aggregation values directly from the sub aggregators bypassing bucket creation. note that the comparator attached to the order will still be used in the reduce phase of the aggregation.
read from a stream.
read from a stream.
test equality and hashcode properties
test serialization and deserialization
creates random suggestion builder, renders it to xcontent and back to new instance that should be equal to original
setup for the whole base test class.
this parsing method assumes that the leading "suggest" field name has already been parsed by the caller
merges the result of another suggestion into this suggestion. for internal usage.
trims the number of options per suggest text term to the requested size. for internal usage.
read from a stream.
adds an org.elasticsearch.search.suggest.suggestionbuilder instance under a user defined name. the order in which the suggestions are added, is the same as in the response.
read from a stream.
sets the maximum suggestions to be returned per suggest text term.
transfers the text, prefix, regex, analyzer, field, size and shard size settings from the original suggestionbuilder to the target suggestioncontext
creates a new suggestion.
setup for the whole base test class
creates random suggestion builder, renders it to xcontent and back to new instance that should be equal to original
searching for a rare phrase shouldn't provide any suggestions if confidence > 1. this was possible before we rechecked the cutoff score during the reduce phase. failures don't occur every time - maybe two out of five tries but we don't repeat it to save time.
test that suggestion works if prefix is either provided via completionsuggestionbuilder#text(string) or
create a randomized suggestion.entry
sets the level of fuzziness used to create suggestions using a fuzziness instance. the default value is fuzziness#one which allows for an "edit distance" of one.
sets the maximum automaton states allowed for the fuzzy expansion
read from a stream.
sets the minimum length of the input, which is not checked for fuzzy alternatives, defaults to 1
sets the minimum length of input string before fuzzy suggestions are returned, defaulting to 3.
returns total in-heap bytes used by all suggesters. this method has cpu cost o(numindexedfields). separately in the returned completionstats
same as #prefix(string) with fuzziness of fuzziness
sets query contexts for completion see org.elasticsearch.search.suggest.completion.context.categoryquerycontext and org.elasticsearch.search.suggest.completion.context.geoquerycontext
read from a stream.
test serialization and deserialization
sets the maximum automaton states allowed for the regular expression expansion
reduces suggestions to a single suggestion containing at most top completionsuggestion#getsize() options across toreduce
test serialization and deserialization
verifies that all field paths specified in contexts point to the fields with correct mappings
parses query contexts for this mapper
sets the query-time boost of the context. defaults to 1.
parse a list of categoryquerycontext using parser. a querycontexts accepts one of the following forms:  object: categoryquerycontext string: categoryquerycontext value with prefix=false and boost=1 array: [categoryquerycontext, ..]  a categoryquerycontext has one of the following forms:  object: "context": <string>, "boost": <int>, "prefix": <boolean> string: "string" 
loads a named categorycontextmapping instance from a map. see contextmappings#load(object, version) acceptable map param: path
parse a set of charsequence contexts at index-time. acceptable formats:  array: [<string>, ..] string: "string" 
parse a list of geoquerycontext using parser. a querycontexts accepts one of the following forms:  object: geoquerycontext string: geoquerycontext value with boost=1 precision=precision neighbours=[precision] array: [geoquerycontext, ..]  a geoquerycontext has one of the following forms:  object:  geo point "lat": <double>, "lon": <double>, "precision": <int>, "neighbours": <[int, ..]> "context": <string>, "boost": <int>, "precision": <int>, "neighbours": <[int, ..]> "context": <geo point>, "boost": <int>, "precision": <int>, "neighbours": <[int, ..]>  string: geo point  see geopoint(string) for geo point
parse a set of charsequence contexts at index-time. acceptable formats:  array: [<geo point>, ..] stringobjectarray: "geo point"  see geopoint(string) for geo point
sets the precision level for computing the geohash from the context geo point. defaults to using index-time precision level
sets the query-time boost for the context defaults to 1
sets the precision levels at which geohash cells neighbours are considered. defaults to only considering neighbours at the index-time precision level
writes a list of objects specified by the defined contextmappings see contextmapping#toxcontent(xcontentbuilder, params)
loads contextmappings from configuration expected configuration: list of maps representing contextmapping ["name": .., "type": .., .., ..]
maps an output context list to a map of context mapping names and their values see org.elasticsearch.search.suggest.completion.context.contextmappings.typedcontextfield
wraps a completionquery with context queries
returns a context mapping by its name
creates a linear interpolation smoothing model. note: the lambdas must sum up to one. the trigram lambda the bigram lambda the unigram lambda
read from a stream.
note: this method closes the tokenstream, even on exception, which is awkward because really the caller who called analyzer#tokenstream should close it, but when trying that there are recursion issues when we try to use the same tokenstream twice in the same recursion...
adds a candidategenerator to this suggester. the phrase term before the candidates are scored.
sets the confidence level for this suggester. the confidence level defines a factor applied to the input phrases score which is used as a threshold for other suggest candidates. only candidates that score higher than the threshold will be included in the result. for instance a confidence level of 1.0 will only return suggestions that score higher than the input phrase. if set to 0.0 the top n candidates are returned. the default is 1.0
internal copy constructor that copies over all class fields except for the field which is set to the one provided in the first argument
setup highlighting for suggestions. if this is called a highlight field is returned with suggestions wrapping changed tokens with pretag and posttag.
sets the likelihood of a term being a misspelled even if the term exists in the dictionary. the default it 0.95 corresponding to 5% or the real words are misspelled.
sets the maximum percentage of the terms that at most considered to be misspellings in order to form a correction. this method accepts a float value in the range [0..1) as a fraction of the actual query terms a number >=1 as an absolute number of query terms. the default is set to 1.0 which corresponds to that only corrections with at most 1 misspelled term are returned.
read from a stream.
sets the gram size for the n-gram model used for this suggester. the default value is 1 corresponding to unigrams. use
adds additional parameters for collate scripts. previously added parameters on the same builder will be overwritten.
setup for the whole base test class
test that creates new smoothing model from a random test smoothing model and checks both for equality
test the wordscorer emitted by the smoothing model
mutate the given model so the returned smoothing model is different
mutate the given model so the returned smoothing model is different
mutate the given model so the returned smoothing model is different
sets the maximum suggestions to be returned per suggest text term.
sets the maximum edit distance candidate suggestions can have in order to be considered as a suggestion. can only be a value between 1 and 2. any other value result in an bad request error being thrown. defaults to 2.
read from a stream.
create random directcandidategeneratorbuilder
creates random candidate generator, renders it to xcontent and back to new instance that should be equal to original
creates a random termsuggestionbuilder
read from a stream.
a factor that is used to multiply with the size in order to inspect more candidate suggestions. can improve accuracy at the cost of performance. defaults to 5.
sets a minimal threshold in number of documents a suggested term should appear in. this can be specified as an absolute number or as a relative percentage of number of documents. this can improve quality by only suggesting high frequency terms. defaults to 0f and is not enabled. if a value higher than 1 is specified then the number cannot be fractional.
the minimum length a suggest text term must have in order to be corrected. defaults to 4.
sets the number of minimal prefix characters that must match in order be a candidate suggestion. defaults to 1. increasing this number improves suggest performance. usually misspellings don't occur in the beginning of terms.
sets the maximum edit distance candidate suggestions can have in order to be considered as a suggestion. can only be a value between 1 and 2. any other value result in an bad request error being thrown. defaults to
s how similar the suggested terms at least need to be compared to the original suggest text tokens. a value between 0 and 1 can be specified. this value will be compared to the string distance result of each candidate spelling correction.  default is 0.5
sets a maximum threshold in number of documents a suggest text token can exist in order to be corrected. can be a relative percentage number (e.g 0.4) or an absolute number to represent document frequencies. if an value higher than 1 is specified then fractional can not be specified. defaults to 0.01.  this can be used to exclude high frequency terms from being suggested. high frequency terms are usually spelled correctly on top of this this also improves the suggest performance.
puts the object into the context
schedule the release of a resource. the time when releasable#close() will be called on this object is function of the provided lifetime.
returns the filter associated with listed filtering aliases.  the list of filtering aliases should be obtained by calling metadata.filteringaliases. returns null if no filtering is required.
get the current usage statistics for this node. the discoverynode for this node whether to include rest action usage in the returned statistics node
the main entry point. the exit code is 0 if the java version is at least 1.8, otherwise the exit code is 1.
chooses additional jvm options for elasticsearch.
delimits the specified jvm options by spaces.
parse the line-delimited jvm options from the specified buffered reader for the specified java major version. valid jvm options are:   a line starting with a dash is treated as a jvm option that applies to all versions   a line starting with a number followed by a colon is treated as a jvm option that applies to the matching java major version only   a line starting with a number followed by a dash followed by a colon is treated as a jvm option that applies to the matching java specified major version and all larger java major versions   a line starting with a number followed by a dash followed by a number followed by a colon is treated as a jvm option that applies to the specified range of matching java major versions   for example, if the specified java major version is 8, the following jvm options will be accepted:           and the following jvm options will not be accepted:         if the version syntax specified on a line matches the specified jvm options, the jvm option callback will be invoked with the jvm option. if the line does not match the specified syntax for the jvm options, the invalid line callback will be invoked with the contents of the entire line.
the main entry point. the exit code is 0 if the jvm options were successfully parsed, otherwise the exit code is 1. if an improperly formatted line is discovered, the line is output to standard error.
register new resource watcher that will be checked in the given frequency
clears any state with the filewatcher, making all files show up as new
utility method to detect whether a thread is a network thread. typically used in assertions to make sure that we do not call blocking code from networking threads.
attempts to a decode a message from the provided bytes. if a full message is not available, null is returned. if the message is a ping, an empty bytesreference will be returned. this is dependent on the available memory.
validates the first 6 bytes of the message header and returns the length of the message. if 6 bytes are not available, it returns -1. this is dependent on the available memory.
sends back an error response to the caller via the given channel
serializes the given message into a bytes representation
this method handles the message receive part for both request and responses
consumes bytes that are available from network reads. this method returns the number of bytes consumed in this call. this is dependent on the available memory.
sends a message to the given channel, using the given callbacks.
writes the tcp message header into a bytes reference.
returns all profile settings for the given settings object
ensures this transport is still started open
executed for a received response error
parse a hostname+port range spec into its equivalent addresses
called once the channel is closed for instance due to a disconnect or a closed socket etc.
fetches all shards for the search request from this remote connection. this is used to later run the search on the remote end.
collects all nodes on the connected cluster and returns passes a nodeid to discoverynode lookup function that returns null if the node id is not found.
ensures that this cluster is connected. if the cluster is connected this operation will invoke the listener immediately.
get the information about remote nodes to be rendered on _remoteinfo requests.
returns a connection to the remote cluster, preferably a direct connection to the provided discoverynode. if such node is not connected, the returned connection will be a proxy connection that redirects to it.
creates a new remoteclusterconnection
collects all nodes of the given clusters and returns passes a (clusteralias, nodeid) to discoverynode function on success.
this method updates the list of remote clusters. it's intended to be used as an update consumer on the settings infrastructure
connects to all remote clusters in a blocking fashion. this should be called on node startup to establish an initial connection to all configured seed nodes.
returns a connection to the given node on the given remote cluster
ensures that the given cluster alias is connected. if the cluster is connected this operation will invoke the listener immediately.
returns a client to the remote cluster if the given cluster alias exists.
disconnected from the given node, if not connected, will do nothing.
connects to a node with the given connection profile. if the node is already connected this method has no effect. once a successful is established, it can be validated before being exposed.
returns a connection for the given node if the node is connected. connections returned from this method must not be closed. the lifecycle of this connection is maintained by this connection manager
copy constructor, using another profile as a base
adds a number of connections for one or more types. each type can only be added once.
sets a connect timeout for this connection profile
returns the number of connections per type for this profile. this might return a count that is shared with other types such that the sum of all connections per type might be higher than #getnumconnections(). for instance if this method but the connections are not distinct.
sets a handshake timeout for this connection profile
creates a new connectionprofile based on the added connections.
takes a connectionprofile resolves it to a fully specified (i.e., no nulls) profile
builds a connection profile that is dedicated to a single channel type. use this when opening single use connections
returns one of the channels out configured for this handle. the channel is selected in a round-robin fashion.
constructs a new mockchannel instance intended for accepting requests.
constructs a new mockchannel instance intended for handling the actual incoming outgoing traffic.
removes and returns all responsecontext instances that match the predicate
called by the transport implementation when a response or an exception has been received for a previously sent request (before any processing or deserialization was done). returns the appropriate response handler or null if not found.
adds a new response context and associates it with a new request id.
executes a high-level handshake using the given connection and returns the discovery node of the node the connection was established with. the handshake will fail if the cluster name on the target node doesn't match the local cluster name.
registers a new request handler
registers a new request handler
returns either a real transport connection or a local node connection if we are using the local node optimization.
called by the transport implementation when an incoming request arrives but before any parsing of it has happened (with the exception of the requestid and action)
registers a new request handler
cancels timeout handling. this is a best effort only to avoid running it. remove the requestid from #responsehandlers to make sure this doesn't run.
registers a new request handler
build the service. updates for #trace_log_exclude_setting and #trace_log_include_setting.
establishes and returns a new connection to the given node. the connection is not maintained by this service, it's the callers responsibility to close the connection once it goes out of scope.
connect to the specified node with the given connection profile
returns true iff the action name starts with a valid prefix.
deserializes a new instance of the return type from the stream. called by the infra when de-serializing the response.
awaits for all of the pending connections to complete. will throw an exception if at least one of the connections fails.
groups indices per cluster by splitting remote cluster-alias, index-name pairs on #remote_cluster_index_separator. all indices per cluster are collected as a list in the returned map keyed by the cluster alias. local indices are grouped under
registers a proxy request handler that allows to forward requests for the given action to another node. to be used when the response type changes based on the upcoming request (quite rare)
registers a proxy request handler that allows to forward requests for the given action to another node. to be used when the response type is always the same (most of the cases).
this method ensures that compression is complete and returns the underlying bytes.
set the number of available processors that netty uses for sizing various resources (e.g., thread pools).
if the specified cause is an unrecoverable error, this method will rethrow the cause on a separate thread so that it can not be caught and bubbles up to the uncaught exception handler.
turns the given bytesreference into a bytebuf. note: the returned bytebuf will reference the internal pages of the bytesreference. don't free the bytes of reference before the bytebuf goes out of scope.
constructs a standardtokenizer filtered by a standardfilter, a lowercasefilter, a stopfilter, and a snowballfilter
correct offset order when doing both parts and concatenation: powershot is a synonym of power
tests using a keyword set for the keyword marker filter.
verifies that both keywords and patterns cannot be specified together.
tests using a regular expression pattern for the keyword marker filter.
test patternanalyzer when it is configured with a non-word pattern.
test patternanalyzer when it is configured with a custom pattern. in this case, text is tokenized on the comma ","
test patternanalyzer when it is configured with a whitespace pattern. behavior can be similar to whitespaceanalyzer (depending upon options)
test patternanalyzer against a large document.
creates a multiplextokenfilter on the given input with a set of filters
test turkish lowercasing
parses a list of mappingcharfilter style rules into a normalize char map
check that the deprecated name "htmlstrip" does not issues a deprecation warning for indices created before 6.3.0
check that the deprecated name "htmlstrip" issues a deprecation warning for indices created since 6.3.0
check that the deprecated name "edgengram" does not issues a deprecation warning for indices created before 6.4.0
check that the deprecated name "edgengram" issues a deprecation warning for indices created since 6.3.0
check that the deprecated name "ngram" does not issues a deprecation warning for indices created before 6.4.0
check that the deprecated name "ngram" issues a deprecation warning for indices created since 6.3.0
parses a list of mappingcharfilter style rules into a custom byte[] type table
correct offset order when doing both parts and concatenation: powershot is a synonym of power
tests that emulates a frozen elected master node that unfreezes and pushes his cluster state to other nodes that already are following another elected master node. these nodes should reject this cluster state and prevent them from following the stale master.
verify that the proper block is applied when nodes loose their master
verify that nodes fault detection works after master (re) election
test that cluster recovers from a long gc on master that causes other nodes to elect a new one
this test isolates the master from rest of the cluster, waits for a new master to be elected, restores the partition and verifies that all node agree on the new cluster state
test that no split brain occurs under partial network partition. see https:github.comelasticelasticsearchissues2488
test cluster join with issues in cluster state publishing
a 4 node cluster with m_m_n set to 3 and each node has one unicast endpoint. one node partitions from the master node. the temporal unicast responses is empty. when partition is solved the one ping response contains a master node. the rejoining node should take this master node and connect.
adds an asymmetric break between a master and one of the nodes and makes sure that the node is removed form the cluster, that the node start pinging and that the cluster reforms when healed.
creates a new blockingclusterstatepublishresponsehandler
allows to wait for all non master nodes to reply to the publish event up to a timeout
called for each failure obtained from non master nodes
called for each response obtained from non master nodes
returns a list of nodes which didn't respond yet
test that we do not loose document whose indexing request was successful, under a randomly selected disruption scheme we also collect & report the type of indexing failures that occur.  this test is a superset of tests run in the jepsen test suite, with the exception of versioned updates
test that a document which is indexed on the majority side of a partition, is available from the minority side, once the partition is healed
tests that indices are properly deleted even if there is a master transition in between. test for https:github.comelasticelasticsearchissues11665
this test creates a scenario where a primary shard (0 replicas) relocates and is in post_recovery on the target node but already deleted on the source node. search request should still work.
creates mock ec2 endpoint providing the list of started nodes to the describeinstances api call
we build the list of nodes from azure management api information can be cached using `cloud.azure.refresh_interval` property if needed. setting `cloud.azure.refresh_interval` to `-1` will cause infinite caching. setting `cloud.azure.refresh_interval` to `0` will disable caching (default).
gets the next committed state to process.  the method tries to batch operation by getting the cluster state the highest possible committed states which succeeds the first committed state in queue (i.e., it comes from the same master).
clear the incoming queue. any committed state will be failed
indicates that a cluster state was successfully processed. any committed state that is  note: successfully processing a state indicates we are following the master it came from. any committed state from another master will be failed by this method
mark a previously added cluster state as committed. this will make it available via #getnextclusterstatetoprocess() when the cluster state is processed (or failed), the supplied listener will be called
add an incoming, not yet committed cluster state
mark that the processing of the given state has failed. all committed states that are
returns all pending states, committed or not
elects a new master out of the possible nodes, returning it. returns null if no master has been elected.
returns a list of the next possible masters.
master nodes go before other nodes, with a secondary sort by id
( electmasterservice.mastercandidate#unrecovered_cluster_version for not recovered)
adds a ping if newer than previous pings from the same node
starts a new joining thread if there is no currently active one and join thread controlling is started
join a newly elected master.
in the case we follow an elected master the new cluster state needs to have the same elected master and the new cluster state version needs to be equal or higher than our cluster state version. if the first condition fails we reject the cluster state and throw an error. if the second condition fails we ignore the cluster state.
in the case we follow an elected master the new cluster state needs to have the same elected master this method checks for this and throws an exception if needed
does simple sanity check of the incoming cluster state. throws an exception on rejections.
the main function of a join thread. this function is guaranteed to join the cluster or spawn a new join thread upon failure to do so.
cleans any running joining thread and calls #rejoin
ensures that the joining node's major version is equal or higher to the minclusternodeversion. this is needed to ensure that if the master is already fully operating under the new major version, it doesn't go back to mixed version mode
ensures that the joining node has a version that's compatible with all current nodes
ensures that the joining node has a version that's compatible with a given version range
ensures that all indices are compatible with the given node version. this will ensure that all indices in the given metadata will not be created with a newer version of elasticsearch as well as that all indices are newer or equal to the minimum index compatibility version.
checks if there is an on going request to become master and if it has enough pending joins. if so, the node will become master via a clusterstate update task.
processes or queues an incoming join request.  note: doesn't do any validation. this should have been done before.
waits for enough incoming joins from master eligible nodes to complete the master election  you must start accumulating joins before calling this method. see #startelectioncontext()  the method will return once the local node has been elected as master or some failuretimeout has happened. the exact outcome is communicated via the callback parameter, which is guaranteed to be called. object
accumulates any future incoming join request. pending join requests will be processed in the final steps of becoming a master or when #stopelectioncontext(string) is called.
stopped accumulating joins. all pending joins will be processed. future joins will be processed immediately
resolves a list of hosts to a list of transport addresses. each host is resolved into a transport address (or a collection of addresses if the number of ports is greater than one). host lookups are done in parallel using specified executor service up to the specified resolve timeout.
a variant of #ping(consumer, timevalue), but allows separating the scheduling duration from the duration used for request level time outs. this is useful for testing
tests tha node can become a master, even though the last cluster state it knows contains nodes that conflict with the joins it got and needs to become a master
test not waiting on publishing works correctly (i.e., publishing times out)
check if enough master node responded to commit the change. fails the commit if there are no more pending master nodes but not enough acks to commit.
tries marking the publishing as failed, if a decision wasn't made yet
tries and commit the current state, if a decision wasn't made yet
publishes a cluster change event to other nodes. if at least minmasternodes acknowledge the change it is committed and will be processed by the master and the other nodes.  the method is guaranteed to throw a org.elasticsearch.discovery.discovery.failedtocommitclusterstateexception if the change is not committed and should be rejected. any other exception signals the something wrong happened but the change is committed.
tries marking the publishing as failed, if a decision wasn't made yet
make sure that nodes in clusterstate are pinged. any pinging to nodes which are not part of the cluster will be stopped
generates a xml response that describe the ec2 instances
refreshes the settings for the amazonec2 client. the new client will be build using these new settings. the old client is usable until released. on release it will be destroyed instead of being returned to the cache.
test for network.host: _ec2:publicip_
test for network.host: _ec2:publicdns_
test for network.host: _ec2_
test for network.host: _ec2:publicipv4_
test for network.host: _ec2:privatedns_
test for network.host: _ec2:privateipv4_
test for network.host: _ec2:privateip_
test that we don't have any regression with network host core settings such as network.host: _local_
parse settings for a single client.
utility test method to test different settings
register an existing node as a gce node
we build the list of nodes from gce management api information can be cached using `cloud.gce.refresh_interval` property if needed.
for issue https:github.comelasticelasticsearchissues16967: when using multiple regions and one of them has no instance at all, this was producing a npe as a result.
for issue https:github.comelasticelasticsearch-cloud-gceissues43
a cluster state supersedes another state if they are from the same master and the version of this state is higher than that of the other state.  in essence that means that all the changes from the other cluster state are also reflected by the current one
returns a built (on demand) routing nodes view of the routing table.
tests whether or not the custom should be serialized. the criteria are:  the output stream must be at least the minimum supported version of the custom the output stream must have the feature required by the custom (if any) or not be a transport client   that is, we only serialize customs to clients than can understand the custom based on the version of the client and the features that the client has. for transport clients we can be lenient in requiring a feature in which case we do not send the custom but for connected nodes we always require that the node has the required feature.
add the given allocation decider to the given deciders collection, erroring if the class name is already used.
for interoperability with transport clients older than 6.3, we need to strip customs from the cluster state that the client might not be able to deserialize
return a new allocationdecider instance with builtin deciders as well as those from plugins.
builds a predicate that will accept a cluster state only if it was generated after the current has (re-)joined the master
randomly adds, deletes or updates repositories in the metadata
randomly updates index routing table in the cluster state
randomly updates routing table in the cluster state
takes an existing cluster state and randomly adds, removes or updates a metadata part using randompart generator. if a new part is added the prefix value is used as a prefix of randomly generated part name.
randomly updates nodes in the cluster state
randomly adds, deletes or updates in-progress snapshot and restore records in the cluster state
makes random settings changes
generates random alias
randomly add, deletes or updates indices in the metadata
randomly updates index routing table in the cluster state
randomly adds, deletes or updates index templates in the metadata
randomly creates or removes cluster blocks
makes random metadata changes
takes an existing cluster state and randomly adds, removes or updates a cluster state part using randompart generator. if a new part is added the prefix value is used as a prefix of randomly generated part name.
returns a new instance of snapshotdeletionsinprogress which removes the given entry from the invoking instance.
returns a new instance of snapshotdeletionsinprogress which adds the given entry to the invoking instance.
read simple diff from the stream
creates simple diff with changes
returns a new diff map with the given key removed, does not modify the invoking instance. if the key does not exist in the diff map, the same instance is returned.
disconnects from all nodes except the ones provided as parameter
retrieve the latest nodes stats, calling the listener when complete
refreshes the clusterinfo in a blocking fashion
retrieve the latest indices stats, calling the listener when complete
create a fake nodestats for the given node and usage
wait for the next cluster state which satisfies statepredicate
sets the last observed state to the currently applied cluster state and returns it
read simple diff from the stream
test basic properties of the clusterchangedevent class: (1) make sure there are no null values for any of its properties (2) make sure you can't create a clusterchangedevent with any null values
test whether the clusterchangedevent returns the correct value for whether the local node is master, based on what was set on the cluster state.
test custom metadata change checks
test the routing table changes checks.
test the index metadata change check.
test nodes addedremovedchanged checks.
retrieves the cluster state for the given indices and then checks that the cluster state returns coherent data for both routing table and metadata.
returns the indices deleted in this event
determines whether or not the current cluster state represents an entirely new cluster, either when a node joins a cluster for the first time or when the node receives a cluster state update from a brand new cluster (different uuid from the previous cluster), which will happen when a master node is elected that has never been part of the cluster before.
returns a set of custom meta data types when any custom metadata for the cluster has changed between the previous cluster state and the new cluster state. custom meta data types are returned iff they have been added, updated or removed between the previous and the current state
returns the indices created in this event
returns true iff the routing table has changed for the given index. note that this is an object reference equality test, not an equals test.
reads restore status from stream input
returns state corresponding to state code
writes restore status to stream output
serializes single restore operation
reads restore status from stream input
makes sure that the indices being waited on before snapshotting commences are populated with all shards in the relocating or initializing state.
creates a new clusterstatehealth instance considering the current cluster state and the provided index names.
checks if an inactive primary shard should cause the cluster health to go red. an inactive primary shard in an index should cause the cluster health to be red to make it visible that some of the existing data is unavailable. in case of index creation, snapshot restore or index shrinking, which are unexceptional events in the cluster lifecycle, cluster health should not turn red for the time where primaries are still in the initializing state but go to yellow instead. however, in case of exceptional events, for example when the primary shard cannot be assigned to a node or initialization fails at some point, cluster health should still turn red. nb: this method should not be called on active shards nor on non-primary shards.
creates a discoverynode representing the local node.
creates a new discoverynode.  note: if the version of the node is unknown version#minimumcompatibilityversion() should be used for the current version. it corresponds to the minimum version this elasticsearch version can communicate with. if a higher version is used the node might not be able to communicate with the remote node. after initial handshakes node versions will be discovered and updated. 
creates a new discoverynode by reading from the stream provided as argument
extract node roles from the given settings
generates a human-readable string for the discovernodefilters. example: _id:"id1 or blah",name:"blah or name2"
checks that a node can be safely added to this node collection. note: if this method returns a non-null value, calling #add(discoverynode) will fail with an exception
returns the changes comparing this nodes to the provided nodes.
resolve a node with a given id
get a node by its address
get a map of the coordinating only nodes (nodes which are neither master, nor data, nor ingest nodes) arranged by their ids
get a map of the discovered master and data nodes arranged by their ids
adds a disco node to the builder. will throw an illegalargumentexception if the supplied node doesn't pass the pre-flight checks performed by #validateadd(discoverynode)
resolves a set of node "descriptions" to concrete and existing node ids. "descriptions" can be (resolved in this order): - "_local" or "_master" for the relevant nodes - a node id - a wild card pattern that will be matched against node names - a "attr:value" pattern, where attr can be a node role (master, data, ingest etc.) in which case the value can be true or false, or a generic node attribute name in which case value will be treated as a wildcard and matched against the node attribute values.
returns true if the local node is the elected master node.
returns the master node, or null if there is no master node
update mappings synchronously on the master node, waiting for at most been applied to the master node and propagated to data nodes.
send a shard failed request to the master node to update the cluster state when a shard on the local node failed.
send a shard failed request to the master node to update the cluster state with the failure of a shard on another node. this means that the shard should be failed because a write made it into the primary but was not replicated to this shard copy. if the shard does not exist anymore but still has an entry in the in-sync set, remove its allocation id from the in-sync set.
executes the actual test
is there a global block with the provided status?
returns true iff non of the given have a clusterblocklevel#metadata_write in place where the like the deletion of an index to free up resources on nodes.
returns true if one of the global blocks as its disable state persistence flag set.
asserts that the current stack trace does not involve a cluster state applier
removes a timeout listener for updated cluster states.
removes an applier of updated cluster states.
adds a cluster state listener that is expected to be removed during a short period of time. if provided, the listener will be notified once a specific time has elapsed. note: the listener is not removed on timeout. this is the responsibility of the caller.
submits a batch of cluster state update tasks; submitted updates are guaranteed to be processed together, potentially with more tasks of the same executor. that share the same executor will be executed batches on this executor
returns the tasks that are pending.
submits a batch of cluster state update tasks; submitted updates are guaranteed to be processed together, potentially with more tasks of the same executor. that share the same executor will be executed batches on this executor
the local node.
note, this test can only work as long as we have a single thread executor executing the state update tasks!
all the shards (replicas) for all indices in this routing table.
builds the routing table. note that once this is called the builder must be thrown away. if you need to build a new routingtable as a copy of this one you'll need to build a new routingtable.builder.
all the active primary shards for the provided indices grouped (each group is a single element, consisting of the primary shard). this is handy for components that expect to get group iterators, but still want in some cases to iterate over all primary shards (and not just one shard in replication group).
all the shards (replicas) for the provided index.
all shards for the provided shardid
all shards for the provided index and shard id
parses the preference type given a string
ensures that all changes to the hash-function shard selection are bwc
this test asserts that replicas failed to execute resync operations will be failed but not marked as stale.
updates the unassigned info and recovery source on the current unassigned shard
moves a shard from unassigned to initialize state
mark a shard as started and adjusts internal statistics.
relocate a shard to another node, adding the target initializing shard as well as assigning it.
moves the assigned replica shard to primary.
calculates routingnodes statistics by iterating over all shardroutings in the cluster to ensure the book-keeping is correct. for performance reasons, this should only be called from asserts this method does nothing.
returns the active primary shard for the given shard id or null if no primary is found or the primary is not active.
applies the relevant logic to handle a cancelled or failed shard. moves the shard to unassigned or completely removes the shard (if relocation target). - if shard is a primary, this also fails initializing replicas. - if shard is an active primary, this also promotes an active replica to primary (if such a replica exists). - if shard is a relocating primary, this also removes the primary relocation target shard. - if shard is a relocating replica, this promotes the replica relocation target to a full initializing replica, removing the relocation source information. this is possible as peer recovery is always done from the primary. - if shard is a (primary or replica) relocation target, this also clears the relocation information on the source shard.
cancels the give shard from the routing nodes internal statistics and cancels the relocation if the shard is relocating.
creates an iterator over shards interleaving between nodes: the iterator returns the first shard from the first node, then the first shard of the second node, etc. until one shard from each node has been returned. the iterator then resumes on the first node by returning the second shard and continues until all shards from all the nodes have been returned.
returns true iff all replicas are active for the given shard routing. otherwise false
removes and ignores the unassigned shard (will be ignored for this run, but will be added back to unassigned once the metadata is constructed again). typically this is used when an allocation decision prevents a shard from being allocated such that subsequent consumers of this api won't try to allocate this shard again.
cancels a relocation of a shard that shard must relocating.
initializes the current unassigned shard and moves it from the unassigned list.
moves assigned primary to unassigned and demotes it to a replica. used in conjunction with #promoteactivereplicashardtoprimary when an active replica is promoted to primary.
applies the relevant logic to start an initializing shard. moves the initializing shard to started. if the shard is a relocation target, also removes the relocation source. if the started shard is a primary relocation target, this also reinitializes currently initializing replicas as their recovery source changes
removes relocation source of an initializing non-primary shard. this allows the replica shard to continue recovery from the primary even though its non-primary relocation source has failed.
marks a shard as temporarily ignored and adds it to the ignore unassigned list. should be used with caution, typically, the correct usage is to removeandignore from the iterator.
drains all unassigned shards and returns it. this method will not drain ignored shards.
figure out if an existing scheduled reroute is good enough or whether we need to cancel and reschedule.
returns the number of shards that are unassigned and currently being delayed.
finds the next (closest) delay expiration of an delayed shard in nanoseconds based on current time. returns 0 if delay is negative. returns -1 if no delayed shard is found.
calculates the delay left based on current time (in nanoseconds) and the delay defined by the index settings. only relevant if shard is effectively delayed (see #isdelayed()) returns 0 if delay is negative
puts primary shard indexroutings into initializing state
verifies that delayed allocation calculation are correct.
verifies that when a shard fails, reason is properly set and details are preserved.
tests that during reroute when a node is detected as leaving the cluster, the right unassigned meta is set
the unassigned meta is kept when a shard goes to initializing, but cleared when it moves to started.
returns the total number of shards within all groups
returns an iterator over active and initializing shards, ordered by the adaptive replica selection forumla. making sure though that its random within the active shards of the same (or missing) rank, and initializing shards are the last to iterate through.
adjust the for all other nodes' collected stats. in the original ranking paper there is no need to adjust other nodes' stats because cassandra sends occasional requests to all copies of the data, so their stats will be updated during that broadcast phase. in elasticsearch, however, we do not have that sort of broadcast-to-all behavior. in order to prevent a node that gets a high score and then never gets any more requests, we must ensure it eventually returns to a more normal score and can be a candidate for serving requests. this adjustment takes the "winning" node's statistics and adds the average of those statistics with each non-winning node. let's say the winning node had a queue size of 10 and a non-winning node had a queue of 18. the average queue size is (10 + 18) 2 = 14 so the non-winning node will have statistics added for a queue size of 14. this is repeated for the response time and service times as well.
returns an iterator over active and initializing shards. making sure though that its random within the active shards, and initializing shards are the last to iterate through.
returns shards based on nodeattributes given such as node name , node attribute, node ip supports node specifications in cluster api
determine the shards of an index with a specific state
the number of shards on this node that will not be eventually relocated.
determine the number of shards with a specific state
determine the shards with a specific state
add a new shard to this node
returns a list of shards that match one of the states listed in shardroutingstate states
initializes an index, to be restored from snapshot
calculates the number of primary shards in active state in routing table
initializes an existing index, to be restored from a snapshot
adds a new shard routing (makes a copy of it), with reference data used from the index shard routing table if it needs to be created.
calculates the number of nodes that hold one or more shards of this index the excludednodes parameter.
calculates the number of primary shards in the routing table the are in
initializes a new empty index, to be restored from a snapshot
initializes a new empty index, with an option to control if its from an api or not.
this tests that a new delayed reroute is scheduled right after a delayed reroute was run
puts primary shard indexroutings into initializing state
reverse engineer the in sync aid based on the given indexroutingtable
relocate the shard to another node.
a short description of the shard.
returns true if the routing is the relocation source for the given routing
cancel relocation of a shard. the shards state must be set to relocating.
set the shards state to started. the shards state must be initializing or relocating. any relocation will be canceled.
moves the shard to unassigned state.
removes relocation source of a non-primary shard. the shard state must be initializing. this allows the non-primary shard to continue recovery from the primary even though its non-primary relocation source has failed.
reinitializes a replica shard, giving it a fresh allocation id
make the active shard primary unless it's not primary
set the unassigned primary shard to non-primary
writes shard information to streamoutput without writing index name and shard id
returns true if the current routing is identical to the other routing in all but meta fields, i.e., unassigned info
returns true if the routing is the relocation target of the given routing
initializes an unassigned shard on a node.
tests serialization and deserialization.
creates a new routingallocation
returns whether the given node id should be ignored from consideration when allocationdeciders is deciding whether to allocate the specified shard id to that node. the node will be ignored if the specified shard failed on that node, triggering the current round of allocation. since the shard just failed on that node, we don't want to try to reassign it there, if the node is still a part of the cluster.
prevent set of insyncallocationids to grow unboundedly. this can happen for example if we don't write to a primary but repeatedly shut down nodes that have active replicas. we use number_of_replicas + 1 (= possible active shard copies) to bound the insyncallocationids set
assume following scenario: indexing request is written to primary, but fails to be replicated to active replica. the primary instructs master to fail replica before acknowledging write to client. in the meanwhile, the node of the replica was removed from the cluster (deassociatedeadnodes). this means that the shardrouting of the replica was failed, but it's allocation id is still part of the in-sync set. we have to make sure that the failshard request from the primary removes the allocation id from the in-sync set.
don't remove allocation id of failed active primary if there is no replica to promote as primary.
assume following scenario: indexing request is written to primary, but fails to be replicated to active replica. the primary instructs master to fail replica before acknowledging write to client. in the meanwhile, primary fails for an unrelated reason. master now batches both requests to fail primary and replica. we have to make sure that only the allocation id of the primary is kept in the in-sync allocation set before we acknowledge request to client. otherwise we would acknowledge a write that made it into the primary but not the replica but the replica is still considered non-stale.
only trim set of allocation ids when the set grows
tests that higher prioritized primaries and replicas are allocated first even on the balanced shard allocator see https:github.comelasticelasticsearchissues13249 for details
returns true if there is at least one node that returned a type#yes decision for allocating this shard.
generates x-content for the node-level decisions, creating the outer "node_decisions" object in which they are serialized.
generates x-content for a discoverynode that leaves off some of the non-critical fields.
warn about the given disk usage if the low or high watermark has been passed
applies the failed shards. note, only assigned shardrouting instances that exist in the routing table should be provided as parameter. also applies a list of allocation ids to remove from the in-sync set for shard copies for which there are no routing entries in the routing table.  if the same instance of clusterstate is returned, then no change has been made.
internal helper to cap the number of elements in a potentially long list for logging.
checks if the are replicas with the auto-expand feature that need to be adapted. returns an updated cluster state if changes were necessary, or the identical cluster if no changes were required.
removes delay markers from unassigned shards based on current time stamp.
reroutes the routing table based on the live nodes.  if the same instance of clusterstate is returned, then no change has been made.
reset failed allocation counter for unassigned shards
unassigned an shards that are associated with nodes that are no longer part of the cluster, potentially promoting replicas if needed.
applies the started shards. note, only initializing shardrouting instances that exist in the routing table should be provided as parameter and no duplicates should be contained.  if the same instance of the clusterstate is returned, then no change has been made.
read in a routingexplanations object
write the routingexplanations object
checks if a watermark string is a valid percentage or byte size value,
creates a move decision for the shard not being allowed to remain on its current node.
creates a decision for whether to move the shard to a different node to form a better cluster balance.
creates a move decision for the shard being able to remain on its current node, so the shard won't be forced to move to another node.
removes allocation ids from the in-sync set for shard copies for which there is no routing entries in the routing table. this method is called in allocationservice before any changes to the routing table are made.
updates in-sync allocations with routing changes that were made to the routing table.
updates the current metadata based on the changes of this routingchangesobserver. specifically we update indexmetadata#getinsyncallocationids() and indexmetadata#primaryterm(int) based on the changes made during this allocation.
remove allocation id of this shard from the set of in-sync shard copies
increases the primary term if #increaseprimaryterm was called for this shard id.
creates a allocateunassigneddecision from the given decision and the assigned node, if any.
returns a throttle decision, with the individual node-level decisions that comprised the final throttle decision if in explain mode.
builds the internal model from all shards in the given to a node. this method will skip shards in the state a shadow shard in the state shardroutingstate#initializing on the target node which we respect during the allocation balancing process. in short, this method recreates the status-quo in the cluster.
makes a decision on whether to move a started shard to another node. the following rules apply to the movedecision return object: 1. if the shard is not started, no decision will be taken and movedecision#isdecisiontaken() will return false. 2. if the shard is allowed to remain on its current node, no attempt will be made to move the shard and 3. if the shard is not allowed to remain on its current node, then movedecision#getallocationdecision() will be populated with the decision of moving to another node. if movedecision#forcemove() () returns true, then 4. if the method is invoked in explain mode (e.g. from the cluster allocation explain apis), then
makes a decision about moving a single shard to a different node to form a more optimally balanced cluster. this method is invoked from the cluster allocation explain api only.
move started shards that can not be allocated to a node anymore for each shard to be moved this function executes a move operation to the minimal eligible node with respect to the weight function. if a shard is moved the shard will be set to shard is created with an incremented version in the state
balances the nodes on the cluster model according to the weight function. the configured threshold is the minimum delta between the weight of the maximum node and the minimum node according to the distribute shards evenly per index. the balancer tries to relocate shards only if the delta exceeds the threshold. in the default case the threshold is set to 1.0 to enforce gaining relocation only, or in other words relocations that move the weight delta closer to 0.0
balances the nodes on the cluster model according to the weight function. the actual balancing is delegated to #balancebyweights()
make a decision for allocating an unassigned shard. this method returns a two values in a tuple: the first value is the decision taken to allocate the unassigned shard, the second value is the is of type type#no, then the assigned node will be null.
allocates all given shards on the minimal eligible node for the shards index with respect to the weight function. all given shards must be unassigned.
this builds a initial index ordering where the indices are returned in most unbalanced first. we need this in order to prevent over allocations on added nodes from one index when the weight parameters for global balance overrule the index balance at an intermediate state. for example this can happen if we have 3 nodes and 3 indices with 3 primary and 1 replica shards. at the first stage all three nodes hold 2 shard for each index. now we add another node and the first index is balanced moving three shards from two of the nodes over to the new node since it has no shards yet and global balance for the node is way below average. to re-balance we need to move shards back eventually likely to the nodes we relocated them from.
tries to find a relocation from the max node to the minimal node for an arbitrary shard of the given index on the balance model. iff this method returns a true the relocation has already been executed on the simulation model as well as on the cluster.
utility method for rejecting the current allocation command based on provided reason
handle case where a disco node cannot be found in the routing table. usually means that it's not a data node.
initializes an unassigned shard on a node and removes it from the unassigned
read from a stream.
utility method for rejecting the current allocation command based on provided exception
read from a stream.
reads a allocationcommands from a streaminput
writes allocationcommands to a streamoutput
executes all wrapped commands on a given routingallocation
reads allocationcommands from a xcontentparser  "commands" : [ "allocate" : "index" : "test", "shard" : 0, "node" : "test" ] 
read from a stream.
given the diskusage for a node and the size of the shard, return the percentage of free disk if the shard were to be allocated to the node.
returns the size of all shards that are currently being relocated to the node, but may not be finished transferring yet. if subtractshardsmovingaway is true then the size of shards moving away is subtracted from the total size of all shards
returns the expected shard size for the given shard or the default value provided if not enough information are available to estimate the shards size.
returns a diskusage for the routingnode using the average usage of other nodes in the disk usage map.
returns a decision whether the given primary shard can be forcibly allocated on the given node. this method should only be called for unassigned primary shards where the node has a shard copy on disk. note: all implementations that override this behavior should take into account the results of #canallocate(shardrouting, routingnode, routingallocation) before making a decision on force allocation, because force allocation should only be considered if all deciders return decision#no.
the shard routing passed to #canallocate(shardrouting, routingnode, routingallocation) is not the initializing shard to this node but: - the unassigned shard routing in case if we want to assign an unassigned shard to this node. - the initializing shard routing if we want to assign the initializing shard to this node instead - the started shard routing in case if we want to check if we can relocate to this node. - the relocating shard routing if we want to relocate to this node now instead. this method returns the corresponding initializing shard that would be allocated to this node.
test for https:groups.google.comdmsgelasticsearchy-sy_hyob-8ezdfnt9vo44j
sets the same routing for all indices
utility method that allows to resolve an index expression to its corresponding single concrete index. callers should make sure they provide proper org.elasticsearch.action.support.indicesoptions that require a single index as a result. the indices resolution must in fact return a single index when using this method, an illegalargumentexception gets thrown otherwise. and the expression that can be resolved to an alias or an index name.
identifies whether the first argument (an array containing index names) is a pattern that matches all indices
returns true iff the given expression resolves to the given index name otherwise false
utility method that allows to resolve an index expression to its corresponding single write index. and the expression that can be resolved to an alias or an index name.
iterates through the list of indices and selects the effective list of required aliases for the given index. only aliases where the given predicate tests successfully are returned. if the indices list contains a non-required reference to the index itself - null is returned. returns null if no filtering is required.
resolves the search routing if in the expression aliases are used. if expressions point to concrete indices or aliases with no routing defined the specified routing is used.
resolves the provided cluster expression to matching cluster names. this method only supports exact or wildcard matches.
finds the specific index aliases that match with the specified aliases directly or partially via wildcards and that point to the specified concrete indices or match partially with the indices via wildcards. present for that index
returns the indexmetadata for this index.
checks if at least one of the specified aliases exists in the specified concrete indices. wildcards are supported in the alias names for partial matches.
returns indexing routing for the given aliasorindex. resolves routing from the alias metadata used in the write index.
finds all mappings for types and concrete indices. types are expanded to include all types that match the glob patterns in the types array. empty types array, null or "_all" will be expanded to all types available for the given indices. only fields that match the provided field filter will be returned (default is a predicate that always returns true, which can be overridden via plugins)
validates an alias filter by parsing it using the provided org.elasticsearch.index.query.queryshardcontext
validates an alias filter by parsing it using the provided org.elasticsearch.index.query.queryshardcontext
validate a proposed alias.
allows to partially validate an alias, without knowing which index it'll get applied to. useful with index templates containing aliases. checks also that it is possible to parse the alias filter via org.elasticsearch.common.xcontent.xcontentparser, without validating it as a filter though.
checks the mappings for compatibility with the current version
elasticsearch v6.0 no longer supports indices created pre v5.0. all indices that were created before elasticsearch v5.0 should be re-indexed in elasticsearch 5.x before they can be opened by this version of elasticsearch.
checks that the index can be upgraded to the current version of the master node.  if the index does not need upgrade it returns the index metadata unchanged, otherwise it returns a modified index metadata. if index cannot be updated the method throws an exception.
checks if the are replicas with the auto-expand feature that need to be adapted. returns a map of updates, which maps the indices to be updated to the desired number of replicas. the map has the desired number of replicas as key and the indices to update as value, as this allows the result of this method to be directly applied to routingtable.builder#updatenumberofreplicas.
updates the cluster block only iff the setting exists in the given settings
delete some indices from the cluster state.
validate the name for an index or alias against some static rules.
returns a default number of routing shards based on the number of shards of the index. the default number of routing shards will allow any index to be split at least once and at most 10 times by a factor of two. the closer the number or shards gets to 1024 the less default split operations are supported
validate the name for an index against some static rules and a cluster state.
validates the settings and mappings for shrinking an index.
creates an index in the cluster state and waits for the specified number of shard copies to become active (as specified in createindexclusterstateupdaterequest#waitforactiveshards()) before sending the response on the listener. if the index creation was successfully applied on the cluster state, then createindexclusterstateupdateresponse#isacknowledged() will return true, otherwise it will return false and no waiting will occur for started shards ( createindexclusterstateupdateresponse#isshardsacknowledged() will also be false). if the index creation in the cluster state was successful and the requisite shard copies were started before the timeout, then createindexclusterstateupdateresponse#isshardsacknowledged() will return true, otherwise if the operation timed out, then it will return false.
returns the unique alias metadata per concrete index. (note that although alias can point to the same concrete indices, each alias reference may have its own routing and filters)
build the operation.
build the operation.
sometimes, the default mapping exists and an actual mapping is not created yet (introduced), in this case, we want to return the default mapping in case it has some default mapping definitions.  note, once the mapping type is introduced, the default mapping is applied on the actual typed mappingmetadata, setting its routing, timestamp, and so on if needed.
returns the source shard ids to shrink into the given shard id.
selects the source shards for a local shard recovery. this might either be a split or a shrink operation.
returns the routing factor for and shrunk index with the given number of target shards. this factor is used in the hash function in hashing routing of documents even if the number of shards changed (ie. a shrunk index). are not divisible by the number of target shards.
returns the source shard id to split the given target shard off
adds human readable version and creation date settings. this method is used to display the settings in a human readable format in rest api
test whether the current index uuid is the same as the given one. returns true if either are _na_
batch method to apply all the queued refresh operations. the idea is to try and batch as much as possible so we won't create the same index all the time for example for the updates on the same mapping and generate a single cluster change event out of all of those.
refreshes mappings if they are not the same between original and parsed version
test resolving _all pattern (null, empty array or "_all") for random indicesoptions
test resolving wildcard pattern that matches no index of alias for random indicesoptions
finds index templates whose index pattern matched with the given index name. the result is sorted by indextemplatemetadata#order descending.
writes repository metadata to stream output
converts the serialized compressed form of the mappings into a parsed map.
purge tombstone entries. returns the number of entries that were purged. tombstones are purged if the number of tombstones in the list is greater than the input parameter of maximum allowed tombstones. tombstones are purged until the list is equal to the maximum allowed.
returns true if the graveyard contains a tombstone for the given index.
checks that when nodes leave the cluster that the auto-expand-replica functionality only triggers after failing the shards on the removed nodes. this ensures that active shards on other live nodes are not failed if the primary resided on a now dead node. instead, one of the replicas on the live nodes first gets promoted to primary, and the auto-expansion (removing replicas) only triggers in a follow-up step.
serializes information about a single repository
returns a repository with a given name or null if such repository doesn't exist
get the real number of parallel threads.
validate the parameters that the user has passed.
write to the status file. the status file will contain a string describing the exit status of the test. it will be success if the test returned success (return code 0), a numerical code if it returned a non-zero status, or in_progress or timed_out.
validate that source parameters look sane.
reads and returns the full contents of the specified file.
parses scm output and returns uri of scm.
returns a string representing current build time.
parses scm output and returns commit of scm.
determines which scm is in use (git or none) and captures output of the scm command for later parsing.
given a list of files, computes and returns an md5 checksum of the full contents of all files.
parses scm output and returns branch of scm.
converts bytes to a hexadecimal string representation and returns it.
computes and returns an md5 checksum of the contents of all files in the input maven fileset.
returns a string containing every element of the given list, with each element separated by a comma.
converts a maven fileset to a list of file objects.
creates a new outputbufferthread to consume the given inputstream.
pretty-print the environment to a stringbuilder.
add environment variables to a processbuilder.
runs the specified command and saves each line of the command's output to the given list and each line of the command's stderr to the other list.
optional .hadoop.fs.fspermissionproto permission = 4;
optional string owner = 5;
required string path = 2;
optional string path = 2;
required string path = 2;
optional .hadoop.fs.fspermissionproto permission = 4;
optional bytes encryption_data = 15;  locations = 12 alias = 13 childrennum = 14 
optional bytes ec_data = 17;  storagepolicy = 16 
optional string symlink = 9;
optional string path = 2;
optional string owner = 5;
optional .hadoop.fs.fspermissionproto permission = 4;
optional .hadoop.fs.fspermissionproto permission = 4;
optional string group = 6;
optional string group = 6;
required string path = 2;
optional string owner = 5;
optional .hadoop.fs.fspermissionproto permission = 4;
optional string symlink = 9;
optional string path = 2;
optional .hadoop.fs.fspermissionproto permission = 4;
optional .hadoop.fs.fspermissionproto permission = 4;
required .hadoop.fs.filestatusproto.filetype filetype = 1;
optional string symlink = 9;
optional string group = 6;
required string protocol = 1;  protocol name 
required string protocol = 1;  protocol name 
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated uint32 methods = 2;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated uint64 versions = 2;  protocol version corresponding to the rpc kind. 
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated uint32 methods = 2;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
required string rpckind = 2;  rpc kind 
repeated uint64 versions = 2;  protocol version corresponding to the rpc kind. 
required string rpckind = 1;  rpc kind 
required string protocol = 1;  protocol name 
required string rpckind = 1;  rpc kind 
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
required string protocol = 1;  protocol name 
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
required string protocol = 1;  protocol name 
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
required string protocol = 1;  protocol name 
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolversionproto protocolversions = 1;
repeated .hadoop.common.protocolsignatureproto protocolsignature = 1;
required string rpckind = 2;  rpc kind 
required string rpckind = 1;  rpc kind 
required string rpckind = 2;  rpc kind 
optional string errormsg = 5;  if request fails, often contains strack trace 
optional .hadoop.common.rpccallercontextproto callercontext = 7;  call context 
optional .hadoop.common.rpctraceinfoproto traceinfo = 6;  tracing info 
required string context = 1;
optional string protocol = 3;
optional string errormsg = 5;  if request fails, often contains strack trace 
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
required string method = 1;
required bytes clientid = 4;  globally unique client id 
required string context = 1;
required string mechanism = 2;
optional string exceptionclassname = 4;  if request fails 
optional .hadoop.common.rpckindproto rpckind = 1;
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
required string method = 1;
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional bytes token = 3;
optional .hadoop.common.rpccallercontextproto callercontext = 7;  call context 
required .hadoop.common.rpcresponseheaderproto.rpcstatusproto status = 2;
optional .hadoop.common.rpccallercontextproto callercontext = 7;  call context 
optional bytes signature = 2;
optional string exceptionclassname = 4;  if request fails 
required string method = 1;
optional .hadoop.common.rpcresponseheaderproto.rpcerrorcodeproto errordetail = 6;  in case of error 
optional .hadoop.common.rpccallercontextproto callercontext = 7;  call context 
optional string serverid = 4;
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional .hadoop.common.rpctraceinfoproto traceinfo = 6;  tracing info 
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional string exceptionclassname = 4;  if request fails 
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional .hadoop.common.rpcrequestheaderproto.operationproto rpcop = 2;
optional string serverid = 4;
required string mechanism = 2;
optional string serverid = 4;
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional string protocol = 3;
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
required .hadoop.common.rpcsaslproto.saslstate state = 2;
optional .hadoop.common.rpctraceinfoproto traceinfo = 6;  tracing info 
optional bytes clientid = 7;  globally unique client id 
optional .hadoop.common.rpctraceinfoproto traceinfo = 6;  tracing info 
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional .hadoop.common.rpccallercontextproto callercontext = 7;  call context 
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional bytes challenge = 5;
optional .hadoop.common.rpctraceinfoproto traceinfo = 6;  tracing info 
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional .hadoop.common.rpccallercontextproto callercontext = 7;  call context 
optional string protocol = 3;
optional .hadoop.common.rpctraceinfoproto traceinfo = 6;  tracing info 
required string context = 1;
repeated .hadoop.common.rpcsaslproto.saslauth auths = 4;
optional string errormsg = 5;  if request fails, often contains strack trace 
optional .hadoop.common.rpctraceinfoproto traceinfo = 6;  tracing info 
optional .hadoop.common.rpccallercontextproto callercontext = 7;  call context 
required string mechanism = 2;
required string methodname = 1;  name of the rpc method 
required string declaringclassprotocolname = 2;  rpcs for a particular interface (ie protocol) are done using a ipc connection that is setup using rpcproxy. the rpcproxy's has a declared protocol name that is sent form client to server at connection time. each rpc call also sends a protocol name (called declaringclassprotocolname). this name is usually the same as the connection protocol name except in some cases. for example metaprotocols such protocolinfoproto which get metainfo about the protocol reuse the connection but need to indicate that the actual protocol is different (i.e. the protocol is protocolinfoproto) since they reuse the connection; in this case the declaringclassprotocolname field is set to the protocolinfoproto 
required string methodname = 1;  name of the rpc method 
required string methodname = 1;  name of the rpc method 
required string declaringclassprotocolname = 2;  rpcs for a particular interface (ie protocol) are done using a ipc connection that is setup using rpcproxy. the rpcproxy's has a declared protocol name that is sent form client to server at connection time. each rpc call also sends a protocol name (called declaringclassprotocolname). this name is usually the same as the connection protocol name except in some cases. for example metaprotocols such protocolinfoproto which get metainfo about the protocol reuse the connection but need to indicate that the actual protocol is different (i.e. the protocol is protocolinfoproto) since they reuse the connection; in this case the declaringclassprotocolname field is set to the protocolinfoproto 
required string declaringclassprotocolname = 2;  rpcs for a particular interface (ie protocol) are done using a ipc connection that is setup using rpcproxy. the rpcproxy's has a declared protocol name that is sent form client to server at connection time. each rpc call also sends a protocol name (called declaringclassprotocolname). this name is usually the same as the connection protocol name except in some cases. for example metaprotocols such protocolinfoproto which get metainfo about the protocol reuse the connection but need to indicate that the actual protocol is different (i.e. the protocol is protocolinfoproto) since they reuse the connection; in this case the declaringclassprotocolname field is set to the protocolinfoproto 
optional string effectiveuser = 1;
optional string protocol = 3;  protocol name for next rpc layer. the client created a proxy with this protocol name 
optional string realuser = 2;
optional .hadoop.common.userinformationproto userinfo = 2;  userinfo beyond what is determined as part of security handshake at connection time (kerberos, tokens etc). 
optional .hadoop.common.userinformationproto userinfo = 2;  userinfo beyond what is determined as part of security handshake at connection time (kerberos, tokens etc). 
optional string realuser = 2;
optional string effectiveuser = 1;
optional .hadoop.common.userinformationproto userinfo = 2;  userinfo beyond what is determined as part of security handshake at connection time (kerberos, tokens etc). 
optional .hadoop.common.userinformationproto userinfo = 2;  userinfo beyond what is determined as part of security handshake at connection time (kerberos, tokens etc). 
optional .hadoop.common.userinformationproto userinfo = 2;  userinfo beyond what is determined as part of security handshake at connection time (kerberos, tokens etc). 
optional .hadoop.common.userinformationproto userinfo = 2;  userinfo beyond what is determined as part of security handshake at connection time (kerberos, tokens etc). 
optional string protocol = 3;  protocol name for next rpc layer. the client created a proxy with this protocol name 
optional string realuser = 2;
optional .hadoop.common.userinformationproto userinfo = 2;  userinfo beyond what is determined as part of security handshake at connection time (kerberos, tokens etc). 
optional string effectiveuser = 1;
optional string protocol = 3;  protocol name for next rpc layer. the client created a proxy with this protocol name 
optional string sendername = 3;  which handler sent this message 
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
optional string sendername = 3;  which handler sent this message 
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
optional string usermessage = 2;  to be displayed to the user 
optional string identifier = 1;
optional string sendername = 3;  which handler sent this message 
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
optional string identifier = 1;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
optional string identifier = 1;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
repeated string args = 2;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
optional string usermessage = 2;  to be displayed to the user 
repeated string args = 2;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
optional string usermessage = 2;  to be displayed to the user 
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
repeated .hadoop.common.genericrefreshresponseproto responses = 1;
repeated string args = 2;
repeated .hadoop.common.credentialskvproto tokens = 1;
repeated .hadoop.common.credentialskvproto secrets = 2;
required string alias = 1;
repeated .hadoop.common.credentialskvproto secrets = 2;
repeated .hadoop.common.credentialskvproto tokens = 1;
required .hadoop.common.tokenproto token = 1;
required string service = 4;
optional .hadoop.common.tokenproto token = 2;
repeated .hadoop.common.credentialskvproto secrets = 2;
required string renewer = 1;
required .hadoop.common.tokenproto token = 1;
required .hadoop.common.tokenproto token = 1;
required string kind = 3;
repeated .hadoop.common.credentialskvproto secrets = 2;
required string renewer = 1;
repeated .hadoop.common.credentialskvproto tokens = 1;
optional .hadoop.common.tokenproto token = 2;
required bytes identifier = 1;
required .hadoop.common.tokenproto token = 1;
repeated .hadoop.common.credentialskvproto tokens = 1;
repeated .hadoop.common.credentialskvproto secrets = 2;
repeated .hadoop.common.credentialskvproto secrets = 2;
repeated .hadoop.common.credentialskvproto secrets = 2;
required .hadoop.common.tokenproto token = 1;
repeated .hadoop.common.credentialskvproto tokens = 1;
repeated .hadoop.common.credentialskvproto secrets = 2;
repeated .hadoop.common.credentialskvproto secrets = 2;
repeated .hadoop.common.credentialskvproto secrets = 2;
repeated .hadoop.common.credentialskvproto tokens = 1;
optional .hadoop.common.tokenproto token = 1;
required .hadoop.common.tokenproto token = 1;
repeated .hadoop.common.credentialskvproto secrets = 2;
repeated .hadoop.common.credentialskvproto tokens = 1;
optional .hadoop.common.tokenproto token = 1;
required .hadoop.common.tokenproto token = 1;
repeated .hadoop.common.credentialskvproto secrets = 2;
required string alias = 1;
optional .hadoop.common.tokenproto token = 2;
required string kind = 3;
required .hadoop.common.tokenproto token = 1;
optional .hadoop.common.tokenproto token = 2;
optional bytes secret = 3;
optional .hadoop.common.tokenproto token = 2;
repeated .hadoop.common.credentialskvproto tokens = 1;
required string alias = 1;
repeated .hadoop.common.credentialskvproto tokens = 1;
repeated .hadoop.common.credentialskvproto tokens = 1;
required string service = 4;
repeated .hadoop.common.credentialskvproto tokens = 1;
required .hadoop.common.tokenproto token = 1;
optional .hadoop.common.tokenproto token = 2;
optional .hadoop.common.tokenproto token = 1;
repeated .hadoop.common.credentialskvproto secrets = 2;
repeated .hadoop.common.credentialskvproto secrets = 2;
optional .hadoop.common.tokenproto token = 1;
required string renewer = 1;
repeated .hadoop.common.credentialskvproto tokens = 1;
optional .hadoop.common.tokenproto token = 1;
required .hadoop.common.tokenproto token = 1;
optional .hadoop.common.tokenproto token = 1;
repeated .hadoop.common.credentialskvproto tokens = 1;
optional .hadoop.common.tokenproto token = 1;
required .hadoop.common.tokenproto token = 1;
required .hadoop.common.tokenproto token = 1;
optional .hadoop.common.tokenproto token = 2;
required bytes password = 2;
repeated .hadoop.common.credentialskvproto tokens = 1;
required string service = 4;
required string kind = 3;
required .hadoop.common.tokenproto token = 1;
required .hadoop.common.tokenproto token = 1;
optional string notreadyreason = 3;  if not ready to become active, a textual explanation of why not 
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.harequestsource reqsource = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
optional string notreadyreason = 3;  if not ready to become active, a textual explanation of why not 
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.haservicestateproto state = 1;
optional string notreadyreason = 3;  if not ready to become active, a textual explanation of why not 
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
required string key = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
required string classname = 2;
required string classname = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
required string classname = 2;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.configpair config = 2;
required string value = 2;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.configpair config = 2;
required string value = 2;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
required string key = 1;
required string classname = 1;
required string classname = 2;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.configpair config = 2;
required string key = 1;
required string classname = 1;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.spanreceiverlistinfo descriptions = 1;
required string value = 2;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.configpair config = 2;
repeated .hadoop.common.configpair config = 2;
required string user = 1;
repeated string groups = 1;
required string user = 1;
repeated string groups = 1;
repeated string groups = 1;
required string user = 1;
return an output stream that quotes all of the output.
quote all of the active html characters in the given string as they are added to the buffer.
remove html quoting from a string.
does the given string need to be quoted?
quote the given item to make it html-safe.
we cannot use valueof since the allowfrom enum differs from its value allow-from. this is a helper method that does exactly what valueof does, but allows us to handle the allowfrom issue gracefully.
return the set of parameter names, quoting each name.
bind listener by closing and opening the listener.
start the server. does not wait for the server to start.
checks the user has privileges to access to instrumentation servlets.  if hadoop.security.instrumentation.requires.admin is set to false (default value) it always returns true.  if hadoop.security.instrumentation.requires.admin is set to true it will check that if the current user is in the admin acls. if the user is in the admin acls it returns true, otherwise it returns false.
get the admin acls from the given servletcontext and check if the given user is in the acl. the user is not present
add an internal servlet in the server, specifying whether or not to protect with kerberos authentication. note: this method is to be used for adding servlets that facilitate internal communication and not for user facing functionality. for servlets added using this method, filters (except internal kerberos filters) are not enabled.
a wrapper of configuration#getpassword(string). it returns string instead of char[].
bind using single configured port. if findport is true, we will try to bind after incrementing port till a free port is found.
set the min, max number of worker threads (simultaneous connections).
define a filter for a context and set up default url mappings.
does the user sending the httpservletrequest has the administrator acls? if it isn't the case, response will be modified to send an error to the user.
create bind exception by wrapping the bind exception thrown.
quote the url so that users specifying the host http header can't inject attacks.
get the address that corresponds to a particular connector. such connector or the connector is not bounded or was closed.
add the given handler to the front of the list of handlers.
stop the server
load ssl properties from the ssl configuration.
get the pathname to the webapps files. on classpath or in the development location.
bind using port ranges. keep on looking for a free port in the port range and throw a bind exception if no port in the configured range binds.
infer the mime type for the response based on the extension of the request uri. returns null if unknown.
add a jersey resource package.
open the main listener for the server
get an array of filterconfiguration specified in the conf
add the path spec to the filter path mapping.
add an internal servlet in the server, with initialization parameters. note: this method is to be used for adding servlets that facilitate internal communication and not for user facing functionality. for servlets added using this method, filters (except internal kerberos filters) are not enabled.
add default apps.
check whether this instance is the active one.
retrieve the static username from the configuration.
create the named file for values of the named class.
return the nth value in the file.
create the named file for values of the named class.
returns true if o is an enumsetwritable with the same value, or both are null.
reset the enumsetwritable with specified value and elementtype. if the value argument is null or its size is zero, the elementtype argument must not be null. if the argument value's size is bigger than zero, the argument elementtype is not be used.
create a set naming the element comparator and compression type.
must be called by all methods which access fields to ensure that the data has been uncompressed.
used to add "predefined" classes and by writable to copy "new" classes.
constructor.
used by child copy constructors.
add a class to the maps if it is not already present.
returns the unsafe-using comparer, or falls back to the pure-java implementation if unable to do so.
lexicographically compare two arrays.
write a bytebuffer to a filechannel at a given offset, handling short writes.
close the closeable objects and ignore any throwable or null pointers. must only be used for cleanup in exception handlers. instead
takes an ioexception, filedirectory path, and method name and returns an ioexception with the input exception as the cause and also include the file,method details. the new exception provides the stack trace of the place where the exception is thrown and some extra diagnostics information. return instance of same exception if exception class has a public string constructor; otherwise return an pathioexception. interruptedioexception and pathioexception are returned unwrapped.
reads len bytes in a loop. for any reason (including eof)
copies count bytes from one stream to another.
return the complete list of files in a directory as strings. this is better than file#listdir because it does not ignore ioexceptions. this directory.
write a bytebuffer to a writablebytechannel, handling short writes.
ensure that any writes to the given file is written to the storage device that contains it. this method opens channel on given file and closes it once the sync is done. borrowed from uwe schindler in lucene-5588
ensure that any writes to the given file is written to the storage device that contains it. this method opens channel on given file and closes it once the sync is done. borrowed from uwe schindler in lucene-5588 opened for read and ignore ioexceptions, because not all file systems and operating systems allow to fsync on a directory)
close the closeable objects and ignore any throwable or null pointers. must only be used for cleanup in exception handlers.
copies from one stream to another.
closes the socket ignoring ioexception
utility wrapper for reading from inputstream. it catches any errors thrown by the underlying stream (either io or decompression-related), and re-throws as an ioexception.
reads a datainput until eof and returns a byte array. make sure not to pass in an infinite datainput or this will never return.
copies from one stream to another. outputstream at the end. the streams are closed in the finally clause.
similar to readfully(). skips bytes in a loop. for any reason (including eof)
change the capacity of the backing storage. the data is preserved.
get a copy of the bytes that is exactly the length of the data. see #getbytes() for faster access to the underlying array.
generate the stream of bytes as hex pairs separated by ' '.
return the name for a class. default is class#getname().
return the class for a name. default is class#forname(string).
convert text back to string
write a utf8 encoded string to out
write a utf8 encoded string with a maximum size to out
converts the provided string to bytes using the utf-8 encoding. if replace is true, then malformed input is replaced with the substitution character, which is u+fffd. otherwise the method throws a malformedinputexception. and length is bytebuffer.limit()
check to see if a byte array is valid utf-8
read a utf8 encoded string with a maximum size
serialize write this object to out length uses zero-compressed encoding
finds any occurrence of what in the backing buffer, starting as position start. the starting position is measured in bytes and the return value is in terms of byte position in the buffer. the backing buffer is not converted to a string for this operation. string in the utf-8 buffer or -1 if not found
set to contain the contents of a string.
read a text object whose length is already known. this allows creating text from a stream which uses a different serialization format.
get a copy of the bytes that is exactly the length of the data. see #getbytes() for faster access to the underlying array.
returns the next code point at the current position in the buffer. the buffer's position will be incremented. any mark set on this buffer will be changed by this method!
for the given string, returns the number of utf-8 bytes required to encode the string.
force initialization of the static members. as of java 5, referencing a class doesn't force it to initialize. since this class requires that the classes be initialized to declare their comparators, we force that initialization to happen.
reads a zero-compressed encoded long from a byte array and returns it.
get a comparator for a writablecomparable implementation.
optimization hook. override this to make sequencefile.sorter's scream. the default implementation reads the data into two writablecomparables (using writable#readfields(datainput), then calls #compare(writablecomparable,writablecomparable).
reset the limit
checks if this mapfile has the indicated key. the membership test is performed using a bloom filter, so the result has always non-zero probability of false positives.
close the map.
close the map.
create the named map for keys of the named class.
create the named map using the named key comparator.
create the named map using the named key comparator.
reads the final key from the file.
merge all input files to output map file. 1. read first keyvalue from all input files to keysvalues array.  2. select the least key and corresponding value.  3. write the selected key and value to output file.  4. replace the already written keyvalue in keysvalues arrays with the next keyvalue from the selected input  5. repeat step 2-4 till all keys are read. 
create the named map for keys of the named class.
create the named map using the named key comparator.
create the named map for keys of the named class.
override this method to specialize the type of
construct a map reader for the named map using the named comparator.
return the value for the named key, or null if none exists.
create the named map using the named key comparator.
positions the reader at the named key, or if none such exists, at the key that falls just before or just after dependent on how the before parameter is set. file at entry that falls just before key. otherwise, position file at record that sorts just after.  0 - positioned at next record 1 - no more records in file
append a keyvalue pair to the map. the key must be greater or equal to the previous key added to the map.
deletes the named map file.
construct a map reader for the named map.
finds the record that is the closest match to the specified key. the first entry that falls just before the key. otherwise, return the record that sorts just after.
create the named map for keys of the named class.
renames an existing map directory.
this method attempts to fix a corrupt mapfile by re-creating its index.
open the given file for read access, verifying the expected usergroup constraints if security is enabled. note that this function provides no additional checks if hadoop security is disabled, since doing the checks would be too expensive when native libraries are not available. the usergroup does not match
same as openfsdatainputstream except that it will run even if security is off. this is used by unit tests.
open the given file for random read access, verifying the expected user group constraints if security is enabled. note that this function provides no additional security checks if hadoop security is disabled, since doing the checks would be too expensive when native libraries are not available. not match when security is enabled.
same as openforrandomread except that it will run even if security is off. this is used by unit tests.
opens the fsdatainputstream on the requested file on local file system, verifying the expected usergroup constraints if security is enabled. match if security is enabled
same as openforread() except that it will run even if security is off. this is used by unit tests.
overwrite an integer into the internal buffer. note that this call can only be used to overwrite existing data in the buffer, i.e., buffer#count cannot be increased, and dataoutputstream#written cannot be increased.
resets the buffer to empty.
convert to a string.
convert a utf-8 encoded byte array back into a string.
read a utf-8 encoded string.
convert to a string, checking for valid utf8. utf8 data.
set to contain the contents of a string.
write a utf-8 encoded string.
convert a string to a utf-8 encoded byte array.
returns the number of bytes required to write this.
set to contain the contents of a string.
create a new instance of a class with a defined factory.
issue a request to readahead on the given file descriptor. messages (e.g. the file name) (useful if, for example, only some segment of the file is requested by the user). pass long.max_value to allow readahead to the end of the file. of this function on this file descriptor, or null if this is the first call if no readahead was performed
submit a request to readahead on the given file descriptor.
return the singleton instance for the current process.
write a writable, string, primitive type, or an array of the preceding. usages. set false for inter-cluster, file, and other persisted output usages, to preserve the ability to interchange files with other clusters that may not be running the same version of software. sometime in ~2013 we can consider removing this parameter and always using the compact format.
read a writable, string, primitive type, or an array of the preceding.
find and load the class with given name classname by first finding it in the specified conf. if the specified conf is null, try load it directly.
try to instantiate a protocol buffer of the given message class from the given input stream.
a convenient method to create an ioexception.
add the given throwable to the exception list.
construct a hash value for the content from the inputstream.
sets the digest value from a hex string.
construct a hash value for a byte array.
construct a hash value for an array of byte array.
returns a string representation of this object.
constructs, reads and returns an instance.
constructs an md5hash with a specified value.
create a thread local md5 digester
serializes a long to a binary stream with zero-compressed encoding. for -112 <= i <= 127, only one byte is used with the actual value. for other values of i, the first byte value indicates whether the long is positive or negative, and the number of bytes that follow. if the first byte value v is between -113 and -120, the following long is positive, with number of bytes that follow are -(v+112). if the first byte value v is between -121 and -128, the following long is negative, with number of bytes that follow are -(v+120). bytes are stored in the high-non-zero-byte-first order.
reads a zero-compressed encoded integer from input stream and returns it.
skip len number of bytes in input streamin
read a string, but check it for sanity. the format consists of a vint followed by the given number of bytes. is negative or larger than maxsize. only the vint is read.
make a copy of a writable object using serialization to a buffer.
reads a zero-compressed encoded long from input stream and returns it.
convert writables to a byte array
reads an integer from the input stream and returns it. this function validates that the integer is between [lower, upper], inclusive.
read and return the next record length, potentially skipping over a sync block.
append a keyvalue pair.
position valleninvalin to the 'value' corresponding to the 'current' key
merges the contents of files passed in path[] unnecessary
returns the class of values in this file.
perform a file sort from a set of input files and return an iterator.
initialize.
read the next 'compressed' block
read the next key in the file, skipping its value. return null at end of file.
construct a uncompressed writer from a set of options.
merges the list of segments of type segmentdescriptor
compress and flush contents to dfs
perform a file sort from a set of input files into an output file.
read 'raw' records.
create the named file. instead.
close the file.
writes records from rawkeyvalueiterator into a file represented by the passed writer
append a keyvalue pair.
set the current byte position in the input file. the position passed must be a position returned by sequencefile.writer#getlength() when writing this file. to seek to an arbitrary position, use sequencefile.reader#sync(long).
used by mergepass to merge the output of the sort
a queue of file segments to merge
read 'raw' values.
write and flush the file header.
read 'raw' keys.
merges the contents of files passed in path[] unnecessary
sort and merge using an arbitrary rawcomparator.
initialize the reader reader sequencefile.sorter.clonefileattributes, and hence do not initialize every component; false otherwise.
get the compression type for the reduce outputs
seek to the next sync mark past a given position.
close the file.
flush all currently written data to the file system
merge the provided files.
return  remove) the requested number of segment descriptors from the sorted map.
read the next key in the file into key, skipping its value. true if another entry exists, and false at end of file.
common work of the constructors.
this constructor is there primarily to serve the sort routine that generates a single output file with an associated index file
fills up the rawkey object with the key returned by the reader
read the next keyvalue pair in the file into key and val. returns true if such a pair exists and false when at end of file
this is the single level merge that is called multiple times depending on the factor size and the number of segments
the default cleanup. subclasses can override this with a custom cleanup
returns the class of keys in this file.
clones the attributes (like compression of the input file and creates a corresponding writer cloned
create a sync point
read a compressed buffer
get the 'value' corresponding to the last read 'key'.
workhorse to check and write out compressed datalengths
create a new writer with the given options.
append a keyvalue pair.
append a keyvalue pair.
sort calls this to generate the final merged output
the default cleanup. subclasses can override this with a custom cleanup
append a keyvalue pair.
get the 'value' corresponding to the last read 'key'.
stores the item in the configuration with the given keyname.
restores the object from the configuration.
restores the array of objects from the configuration.
stores the array of items in the configuration with the given keyname.
return true if bytes from #getbytes() match.
set the instance that is wrapped.
construct an outputstream from the given dataoutput. if 'out' is already an outputstream, simply returns it. otherwise, wraps it in an outputstream.
create a shared file descriptor which will be both readable and writable. generated descriptor. the descriptor.
create a new sharedfiledescriptorfactory. by this factory. succession, and return a factory using the first usable path.
create a file for write with permissions set to the specified mode. by setting permissions at creation time, we avoid issues related to the user lacking write_dac rights on subsequent chmod calls. one example where this can occur is writing to an smb share where the user does not have full control rights, and therefore write_dac is denied. this method mimics the semantics implemented by the jdk in append, the sharing mode allows other readers and writers, and paths longer than max_path are supported. (see io_util_md.c in the jdk.)
create a filedescriptor that shares delete permission on the file opened at a given offset, i.e. other process can delete the file the filedescriptor is reading. only windows implementation uses the native interface.
a version of renameto that throws a descriptive exception when it fails.
create the specified file for write access, ensuring that it does not exist.
unbuffered file copy from src to dst without tainting os buffer cache in posix platform: it uses filechannel#transferto() which internally attempts unbuffered io on os with native sendfile64() support and falls back to buffered io otherwise. it minimizes the number of filechannel#transferto call by passing the the src file size directly instead of a smaller size as the 3rd parameter. this saves the number of sendfile64() system call when native sendfile64() is supported. in the two fall back cases where sendfile is not supported, filechannle#transferto already has its own batching of size 8 mb and 8 kb, respectively. in windows platform: it uses its own native wrapper of copyfileex with copy_file_no_buffering flag, which is supported on windows server 2008 and above. ideally, we should use filechannel#transferto() across both posix and windows platform. unfortunately, the wrapper(java_sun_nio_ch_filechannelimpl_transferto0) used by filechannel#transferto for unbuffered io is not implemented on windows. based on openjdk 678 source code, java_sun_nio_ch_filechannelimpl_transferto0 on windows simply returns ios_unsupported. note: this simple native wrapper does minimal parameter checking before copy and consistency check (e.g., size) after copy. it is recommended to use wrapper function like the storage#nativecopyfileunbuffered() function in hadoop-hdfs with prepost copy checks.
the windows logon name has two part, netbios domain name and user account name, of the format domain\username. this method will remove the domain part of the full logon name.
return the file stat for a file path. file stat
unmaps the block from memory. see munmap(2). there isn't any portable way to unmap a memory region in java. so we use the sun.nio method here. note that unmapping a memory region could cause crashes if code continues to reference the unmapped code. however, if we don't manually unmap the memory, we are dependent on the finalizer to do it, and we have no idea when the finalizer will run.
returns the file stat for a file descriptor.
locks the provided direct bytebuffer into memory, preventing it from swapping out. after a buffer is locked, future accesses will not incur a page fault. see the mlock(2) man page for more information.
return the decompressor to the pool. pool
return the compressor to the pool.
get a decompressor for the given compressioncodec from the pool or a new one. decompressor compressioncodec the pool or a new one
get a compressor for the given compressioncodec from the pool or a new one. compressor compressioncodec from the pool or a new one
create a compressionoutputstream that will write to the given compressed
create a compressioninputstream that will read from the given stream for uncompressed data.
creates compressioninputstream to be used to read off uncompressed data in one of the two reading modes. i.e. continuous or blocked reading modes only at block boundaries.
write the data provided to the compression codec, compressing no more than the buffer size less the compression overhead as specified during construction for each block. each block contains the uncompressed length for the block, followed by one or more length-prefixed blocks of compressed data.
create a compressioninputstream that will read from the given
create a new decompressor for use by this compressioncodec.
create a new compressor for use by this compressioncodec.
create a compressionoutputstream that will write to the given
create a compression input stream that reads the decompressed bytes from the given stream.
create a new compressor for use by this compressioncodec.
create a compressionoutputstream that will write to the given
are the native snappy libraries loaded  initialized?
create a compressioninputstream that will read from the given
create a new decompressor for use by this compressioncodec.
create an output stream with a codec taken from the global codecpool.
create an input stream with a codec taken from the global codecpool.
sets a list of codec classes in the configuration. in addition to any classes specified using this method, compressioncodec classes on the classpath are discovered using a java serviceloader.
find the relevant compression codec for the codec's canonical class name.
a little test program.
removes a suffix from a filename, if it has it.
get the list of codecs discovered via a java serviceloader, or listed in the configuration. codecs specified in configuration come later in the returned list, and are considered to override those from the serviceloader.
find the relevant compression codec for the codec's canonical class name or by codec alias.  codec aliases are case insensitive.  the code alias is the short class name (without the package name). if the short class name ends with 'codec', then there are two aliases for the codec, the complete short class name and the short class name without the 'codec' ending. for example for the 'gzipcodec' codec class name the alias are 'gzip' and 'gzipcodec'.
find the relevant compression codec for the given file based on its filename suffix.
find the relevant compression codec for the codec's canonical class name or by codec alias and returns its implemetation class.  codec aliases are case insensitive.  the code alias is the short class name (without the package name). if the short class name ends with 'codec', then there are two aliases for the codec, the complete short class name and the short class name without the 'codec' ending. for example for the 'gzipcodec' codec class name the alias are 'gzip' and 'gzipcodec'.
print the extension map out as a string.
find the codecs specified in the config value io.compression.codecs and register them. defaults to gzip and deflate.
get the type of decompressor needed by this compressioncodec.
create a new decompressor for use by this compressioncodec.
create a compressioninputstream that will read from the given
get the type of compressor needed by this compressioncodec.
create a new compressor for use by this compressioncodec.
create a compressionoutputstream that will write to the given
return the appropriate implementation of the zlib compressor.
load native library and set the flag whether to use native library. the method is also used for reset the flag modified by setnativezlibloaded
creates a new (pure java) gzip decompressor.
resets everything, including the input buffer, regardless of whether the current gzip substream is finished.
parse the gzip trailer (assuming we're in the appropriate state). in order to deal with degenerate cases (e.g., user buffer is one byte long), we copy trailer bytes (all 8 of 'em) to a local buffer. see http:www.ietf.orgrfcrfc1952.txt for the gzip spec.
parse the gzip header (assuming we're in the appropriate state). in order to deal with degenerate cases (e.g., user buffer is one byte long), we copy (some) header bytes to another buffer. (filename, comment, and extra-field bytes are simply skipped.) see http:www.ietf.orgrfcrfc1952.txt for the gzip spec. note that no version of gzip to date (at least through 1.4.0, 2010-01-20) supports the fhcrc header-crc16 flagbit; instead, the implementation treats it as a multi-file continuation flag (which it also doesn't support). :-( sun's jdk v6 (1.6) supports the header crc, however, and so do we.
creates a new decompressor.
resets everything including the input buffers (user and direct).
prepare the compressor to be used in a new stream with settings defined in the given configuration. it will reset the compressor's compression level and compression strategy.
creates a new compressor using the specified compression level. compressed data will be generated in zlib format.
reinit the compressor with the given configuration. it will reset the compressor's compression level and compression strategy. different from zlibcompressor, builtinzlibdeflater only support three kind of compression strategy: filtered, huffman_only and default_strategy. it will use default_strategy as default if the configured compression strategy is not supported.
creates a new decompressor.
resets everything including the input buffers (user and direct).
prepare the compressor to be used in a new stream with settings defined in the given configuration. it will reset the compressor's compression level and compression strategy.
sets input data for decompression. this should be called if and only if #needsinput() returns true indicating that more input data is required. (both native and non-native versions of various decompressors require that the data passed in via b[] remain unmodified until the caller is explicitly notified--via #needsinput()--that the buffer may be safely modified. with this requirement, an extra buffer-copy can be avoided.)
if a write would exceed the capacity of the direct buffers, it is set aside to be loaded by this function while the compressed data are consumed.
creates a new compressor.
fills specified buffer with uncompressed data. returns actual number of bytes of uncompressed data. a return value of 0 indicates that input data is required.
returns true if the input data buffer is empty and provide more input. order to provide more input.
resets compressor so that a new set of input data can be processed.
fills specified buffer with compressed data. returns actual number of bytes of compressed data. a return value of 0 indicates that needsinput() should be called in order to determine if more input data is required.
sets input data for compression. this should be called whenever #needsinput() returns true indicating that more input data is required.
returns true if the input data buffer is empty and #setinput() should be called to provide more input. #setinput() should be called in order to provide more input.
creates a new compressor.
returns true if the end of the compressed data output stream has been reached. data output stream has been reached.
resets everything including the input buffers (user and direct).
creates a new decompressor.
this method reads a byte from the compressed stream. whenever we need to read from the underlying compressed stream, this method should be called instead of directly calling the read method of the underlying compressed stream. this method does important record keeping to have the statistic that how many bytes have been read off the compressed stream.
this method tries to find the marker (passed to it as the first parameter) in the stream. it can find bit patterns of length <= 63 bits. specifically this method is used in cbzip2inputstream to find the end of block (eob) delimiter in the stream, starting from the current position of the stream. if marker is found, the stream position will be at the byte containing the starting bit of the marker.
initializes the #tt array. this method is called when the required length of the array is known. i don't initialize it at construction time to avoid unnecessary memory allocation when compressing small files.
in continous reading mode, this read method starts from the start of the compressed stream and end at the end of file by emitting un-compressed data. in this mode stream positioning is not announced and should be ignored. in byblock reading mode, this read method informs about the end of a bzip2 block by returning eob. at this event, the compressed stream position is also announced. this announcement tells that how much of the compressed stream has been de-compressed and read out of this class. in between eob events, the stream position is not updated. if the stream content is malformed or an io error occurs. of -1 means end of stream while -2 represents end of block
constructs a new cbzip2outputstream with specified blocksize.  attention: the caller is resonsible to write the two bzip2 magic bytes "bz" to the specified stream prior to calling this constructor.  the destination stream. the blocksize as 100k units. if an io error occurs in the specified stream. if (blocksize  1) || (blocksize > 9). if out == null.
this method is accessible by subclasses for historical purposes. if you don't know what it does then you don't need it.
creates a new compressor using the specified block size. compressed data will be generated in bzip2 format. an integer from 1 through 9, which is multiplied by 100,000 to obtain the actual block size in bytes. fallback algorithm is used for pathological data. it ranges from 0 to 250.
prepare the compressor to be used in a new stream with settings defined in the given configuration. it will reset the compressor's block size and and work factor.
check if native-bzip2 code is loaded  initialized correctly and can be loaded for this job. and can be loaded for this job, else false
resets compressor so that a new set of input data can be processed.
sets input data for compression. this should be called whenever #needsinput() returns true indicating that more input data is required.
returns true if the input data buffer is empty and #setinput() should be called to provide more input. #setinput() should be called in order to provide more input.
returns true if the end of the compressed data output stream has been reached. data output stream has been reached.
fills specified buffer with compressed data. returns actual number of bytes of compressed data. a return value of 0 indicates that needsinput() should be called in order to determine if more input data is required.
creates a new compressor. which trades cpu for compression ratio.
if a write would exceed the capacity of the direct buffers, it is set aside to be loaded by this function while the compressed data are consumed.
creates a new compressor.
fills specified buffer with uncompressed data. returns actual number of bytes of uncompressed data. a return value of 0 indicates that input data is required.
returns true if the input data buffer is empty and provide more input. order to provide more input.
sets input data for decompression. this should be called if and only if #needsinput() returns true indicating that more input data is required. (both native and non-native versions of various decompressors require that the data passed in via b[] remain unmodified until the caller is explicitly notified--via #needsinput()--that the buffer may be safely modified. with this requirement, an extra buffer-copy can be avoided.)
get a specific coder factory defined by codec name and coder name.
update codermap and codernamemap with iterable type of coder factories.
is the native isa-l library loaded and initialized? throw exception if not.
constructor with key parameters provided. note the extraoptions may contain additional information for the erasure codec to interpret further.
make a meaningful string representation for log output.
constructor with schema name and provided all options. note the options may contain additional information for the erasure codec to interpret further.
create decoder corresponding to given codec.
create encoder corresponding to given codec.
convert to a bytes array, just for test usage.
convert an array of this chunks to an array of bytebuffers
get erased blocks count
the constructor with all the necessary info.
which blocks were erased ? for xor it's simple we only allow and return one erased block, either data or parity.
get erased input blocks from inputblocks
which blocks were erased ?
get indexes of erased blocks from inputblocks
we have all the data blocks and parity blocks as input blocks for recovering by default. it's codec specific
find the valid input from all the inputs.
given a blockgroup, tell if any of the missing blocks can be recovered, to be called by ecmanager recoverable or not
calculating and organizing blockgroup, to be called by ecmanager
check and validate decoding parameters, throw exception accordingly.
convert to a bytebufferencodingstate when it's backed by on-heap arrays.
check and ensure the buffers are of the desired length.
check and ensure the buffers are of the desired length and type, direct buffers or not.
convert to a bytearrayencodingstate when it's backed by on-heap arrays.
encode with inputs and generates outputs. note, for both inputs and outputs, no mixing of on-heap buffers and direct buffers are allowed. if the coder option allow_change_inputs is set true (false by default), the content of input buffers may change after the call, subject to concrete implementation. anyway the positions of input buffers will move forward. be 0 after encoding after the call
encode with inputs and generates outputs. more see above. after the call
decode with inputs and erasedindexes, generates outputs. more see above. erasedindexes, ready for read after the call
decode with inputs and erasedindexes, generates outputs. how to prepare for inputs: 1. create an array containing data units + parity units. please note the data units should be first or before the parity units. 2. set null in the array locations specified via erasedindexes to indicate they're erased and no data are to read from; 3. set null in the array locations for extra redundant items, as they're not necessary to read when decoding. for example in rs-6-3, if only 1 unit is really erased, then we have 2 extra items as redundant. they can be set as null to indicate no data will be used from them. for an example using rs (6, 3), assuming sources (d0, d1, d2, d3, d4, d5) and parities (p0, p1, p2), d2 being erased. we can and may want to use only 6 units like (d1, d3, d4, d5, p0, p2) to recover d2. we will have: inputs = [null(d0), d1, null(d2), d3, d4, d5, p0, null(p1), p2] erasedindexes = [2] index of d2 into inputs array outputs = [a-writable-buffer] note, for both inputs and outputs, no mixing of on-heap buffers and direct buffers are allowed. if the coder option allow_change_inputs is set true (false by default), the content of input buffers may change after the call, subject to concrete implementation. be 0 after decoding erasedindexes, ready for read after the call
check and ensure the buffers are of the desired length and type, direct buffers or not.
convert to a bytearraydecodingstate when it's backed by on-heap arrays.
check and ensure the buffers are of the desired length and type, direct buffers or not.
make sure to return an empty chunk buffer for the desired length.
picking up indexes of valid inputs.
get indexes array for items marked as null, either erased or not to read.
find the valid input from all the inputs.
clone an input bytes array as direct bytebuffer.
ensure a buffer filled with zero bytes from current readablewritable position. are not changed after the call
convert an array of this chunks to an array of bytebuffers
convert to a bytebufferdecodingstate when it's backed by on-heap arrays.
check and ensure the buffers are of the desired length.
check and ensure the buffers are of the desired length.
check and validate decoding parameters, throw exception accordingly. the checking assumes it's a mds code. other code can override this.
the "bulk" version of the remainder, using bytebuffer. warning: this function will modify the "dividend" inputs.
compute the sum of two polynomials. the index in the array corresponds to the power of the entry. for example p[0] is the constant term of the polynomial p.
get the object performs galois field arithmetics
a "bulk" version of the substitute, using bytebuffer. tends to be 2x faster than the "int" substitute in a loop.
compute the multiplication of two polynomials. the index in the array corresponds to the power of the entry. for example p[0] is the constant term of the polynomial p.
a "bulk" version of the solvevandermondesystem, using bytebuffer.
see above. try to use the byte[] version when possible.
invert a matrix assuming it's invertible. ported from intel isa-l library.
print data in hex format in a chunk.
convert bytes into format like 0x02 02 00 80. if limit is negative or too large, then all bytes will be converted.
 serializations are found by reading the io.serializations property from conf, which is a comma-delimited list of classnames. 
the compression algorithm to be used to for compression.
finishing reading the block. release all resources.
constructor fs input stream. length of the corresponding file
signaling the end of write to the block. the block register will be called for registering the finished block.
close the bcfile writer. attempting to use the writer after calling close is not allowed and may lead to undetermined results.
finishing up the current block.
find the smallest block index whose starting offset is greater than or equal to the specified offset. user-specific offset. otherwise.
stream access to a data block. 0-based data block index.
stream access to a meta block. meta block name the meta block with the given name does not exist.
create a data block and obtain an output stream for adding data into the block. there can only be one blockappender stream active at any time. data blocks may not be created after the first meta blocks. the caller must call blockappender.close() to conclude the block creation.
constructor fs output stream. name of the compression algorithm, which will be used for all data blocks.
internal api. comparing the key at cursor to user-specified key. user-specified key. and positive if key at cursor greater than user key.
constructor output stream for writing. must be at position 0. minimum compressed block size in bytes. a compression block will not be closed until it reaches this size except for the last block. name of the compression algorithm. must be one of the strings returned by tfile#getsupportedcompressionalgorithms(). leave comparator as null or empty string if tfile is not sorted. otherwise, provide the string name for the comparison algorithm for keys. two kinds of comparators are supported.  algorithmic comparator: binary comparators that is language independent. currently, only "memcmp" is supported. language-specific comparator: binary comparators that can only be constructed in specific language. for java, the syntax is "jclass:", followed by the class name of the rawcomparator. currently, we only support rawcomparators that can be constructed through the default constructor (with no parameters). parameterized rawcomparators such as one should write a wrapper class that inherits from such classes and use its default constructor to perform proper initialization.  the configuration object.
input key. if no such block exists.
lazily loading the tfile index.
stream access to value. the value part of the key-value pair pointed by the current cursor is not cached and can only be examined once. calling any of the following functions more than once without moving the cursor will result in exception: #getvalue(byte[]),
get the length of the value. isvaluelengthknown() must be tested true.
move the cursor to the next key-value pair. the entry returned by the previous entry() call will be invalid. already at the end location and cannot be advanced.
writing the value to the output stream. this method avoids copying value data from scanner into user buffer, then writing to the output stream. it does not require the value length to be known. the output stream
close the current data block if necessary. force the closure regardless of the block size.
obtain an output stream for creating a meta block. this function may not be called when there is a key append stream or value append stream active. no more key-value insertion is allowed after a meta data block has been added to tfile. name of the meta block. name of the compression algorithm to be used. must be one of the strings returned by closing the stream would signal the ending of the block. the meta block with the same name already exists.
get a scanner that covers a portion of tfile based on keys. begin key of the scan (inclusive). if null, scan from the first key-value entry of the tfile. end key of the scan (exclusive). if null, scan up to the last key-value entry of the tfile. greater than or equal to the beginkey and less than the endkey.
advance cursor by n positions within the block. number of key-value pairs to skip in block.
if greater is true then returns the beginning location of the block containing the key strictly greater than input key. if greater is false then returns the beginning location of the block greater than equal to the input key the input key boolean flag
get the location pointing to the beginning of the first key-value pair in a compressed block whose byte offset in the tfile is greater than or equal to the specified offset. the user supplied offset. entry exists.
obtain an output stream for writing a key into tfile. this may only be called when there is no active key appending stream or value appending stream. the expected length of the key. if length of the key is not known, set length = -1. otherwise, the application must write exactly as many bytes as specified here before calling close on the returned output stream.
close the writer. resources will be released regardless of the exceptions being thrown. future close calls will have no effect. the underlying fsdataoutputstream is not closed.
check whether we have already successfully obtained the key. it also initializes the valueinputstream.
constructor fs input stream of the tfile. the length of tfile. this is required because we have no easy way of knowing the actual size of the input file through the file input stream.
copy value into user-supplied buffer. user supplied buffer must be large enough to hold the whole value (starting from the offset). the value part of the key-value pair pointed by the current cursor is not cached and can only be examined once. calling any of the following functions more than once without moving the cursor will result in exception: #getvalue(byte[]), #getvalue(byte[], int), isvaluelengthknown() to be true.
constructor the tfile reader object. begin key of the scan. if null, scan from the first  entry of the tfile. end key of the scan. if null, scan up to the last  v> entry of the tfile.
for writing to file.
obtain an output stream for creating a meta block. this function may not be called when there is a key append stream or value append stream active. no more key-value insertion is allowed after a meta data block has been added to tfile. data will be compressed using the default compressor as defined in writer's constructor. name of the meta block. closing the stream would signal the ending of the block. the meta block with the same name already exists.
advance cursor in block until we find a key that is greater than or equal to the input key. key to compare. advance until we find a key greater than the input key.
get a sample key that is within a block whose starting offset is greater than or equal to the specified offset. the file offset. (which could happen if the offset is close to the end of the tfile).
adding a new key-value pair to tfile. buffer for key. offset in key buffer. length of key. buffer for value. offset in value buffer. length of value. upon io errors.  if an exception is thrown, the tfile will be in an inconsistent state. the only legitimate call after that would be close
for reading from file.
copy the value into byteswritable. the input byteswritable will be automatically resized to the actual value size. the implementation directly uses the buffer inside byteswritable for storing the value. the call does not require the value length to be known.
get a scanner that covers a specific key range. begin key of the scan (inclusive). if null, scan from the first key-value entry of the tfile. end key of the scan (exclusive). if null, scan up to the last key-value entry of the tfile. greater than or equal to the beginkey and less than the endkey.
get a comparator object to compare entries. it is useful when you want stores the entries in a collection (such as priorityqueue) and perform sorting or comparison among entries based on the keys without copying out the key.
obtain an output stream for writing a value into tfile. this may only be called right after a key appending operation (the key append stream must be closed). the expected length of the value. if length of the value is not known, set length = -1. otherwise, the application must write exactly as many bytes as specified here before calling close on the returned output stream. advertising the value size up-front guarantees that the value is encoded in one chunk, and avoids intermediate chunk buffering.
dumping the tfile information. a list of tfile paths.
compare an entry with a rawcomparable object. this is useful when entries are stored in a collection, and we want to compare a user supplied key.
load a compressed block for reading. expecting blockindex is valid.
check if we need to start a new data block.
move the cursor to the new location. the entry returned by the previous entry() call will be invalid. new cursor location. it must fall between the begin and end location of the scanner.
constructor the tfile reader object. begin location of the scan. end location of the scan.
input key. if no such block exists.
copy the key into user supplied buffer. the buffer supplied by user. the starting offset of the user buffer where we should copy the key into. requiring the key-length + offset no greater than the buffer length.
provide a customized comparator for entries. this is useful if we have a collection of entry objects. however, if the entry objects come from different tfiles, users must ensure that those tfiles share the same rawcomparator.
create a scanner that covers a range of records. the recordnum for the first record (inclusive). the recordnum for the last record (exclusive). to scan the whole file, either specify endrecnum==-1 or endrecnum==getentrycount().
copy the key into byteswritable. the input byteswritable will be automatically resized to the actual key size. byteswritable to hold the key.
dump information about tfile. path string of the tfile printstream to output the information. the configuration object.
wrap a partial byte array as a rawcomparable. the byte array buffer. the starting offset the length of the consecutive bytes to be wrapped.
constructor the fsdatainputstream we connect to. beginning offset of the region. length of the region. the actual length of the region may be smaller if (off_begin + length) goes beyond the end of fs input stream.
lower bound binary search. find the index to the first element in the list that compares greater than or equal to key. type of the input key. the list the input key. otherwise.
decoding the variable-length integer. synonymous to (int)utils#readvlong(in). input stream
read a string as a vint n, followed by n bytes in text format. the input stream.
encoding a long integer into a variable-length encoding format.  if n in [-32, 127): encode in one byte with the actual value. otherwise, if n in [-202^8, 202^8): encode in two bytes: byte[0] = n256 - 52;  otherwise, if n in [-162^16, 162^16): encode in three bytes: byte[0]=n2^16 - 88; byte[1]=(n>>8)&0xff;  otherwise, if n in [-82^24, 82^24): encode in four bytes: byte[0]=n2^24 - 112; byte[1] = (n>>16)&0xff; byte[2] = (n>>8)&0xff;  otherwise: if n in [-2^31, 2^31): encode in five bytes: byte[0]=-125; byte[1] = (n>>24)&0xff; byte[2]=(n>>16)&0xff; byte[3]=(n>>8)&0xff; byte[4]=n&0xff; if n in [-2^39, 2^39): encode in six bytes: byte[0]=-124; byte[1] = (n>>32)&0xff; byte[2]=(n>>24)&0xff; byte[3]=(n>>16)&0xff; byte[4]=(n>>8)&0xff;  if n in [-2^47, 2^47): encode in seven bytes: byte[0]=-123; byte[1] = (n>>40)&0xff; byte[2]=(n>>32)&0xff; byte[3]=(n>>24)&0xff; byte[4]=(n>>16)&0xff; byte[5]=(n>>8)&0xff; byte[6]=n&0xff; if n in [-2^55, 2^55): encode in eight bytes: byte[0]=-122; byte[1] = (n>>48)&0xff; byte[2] = (n>>40)&0xff; byte[3]=(n>>32)&0xff; byte[4]=(n>>24)&0xff; byte[5]=(n>>16)&0xff; byte[6]=(n>>8)&0xff; byte[7]=n&0xff; if n in [-2^63, 2^63): encode in nine bytes: byte[0]=-121; byte[1] = (n>>54)&0xff; byte[2] = (n>>48)&0xff; byte[3] = (n>>40)&0xff; byte[4]=(n>>32)&0xff; byte[5]=(n>>24)&0xff; byte[6]=(n>>16)&0xff; byte[7]=(n>>8)&0xff; byte[8]=n&0xff;  output stream the integer number
lower bound binary search. find the index to the first element in the list that compares greater than or equal to key. type of the input key. the list the input key. comparator for the key. otherwise.
decoding the variable-length integer. suppose the value of the first byte is fb, and the following bytes are nb[].  if (fb >= -32), return (long)fb; if (fb in [-72, -33]), return (fb+52)<<8 + nb[0]&0xff; if (fb in [-104, -73]), return (fb+88)<<16 + (nb[0]&0xff)<<8 + nb[1]&0xff; if (fb in [-120, -105]), return (fb+112)<<24 + (nb[0]&0xff)<<16 + (nb[1]&0xff)<<8 + nb[2]&0xff; if (fb in [-128, -121]), return interpret nb[fb+129] as a signed big-endian integer. input stream
write a string as a vint n, followed by n bytes as in text format.
upper bound binary search. find the index to the first element in the list that compares greater than the input key. type of the input key. the list the input key. comparator for the key. otherwise.
return a string representation of the version.
upper bound binary search. find the index to the first element in the list that compares greater than the input key. type of the input key. the list the input key. otherwise.
write out a chunk. the chunk buffer. offset to chunk buffer for the beginning of chunk. is this the last call to flushbuffer?
write out a chunk that is a concatenation of the internal buffer plus user supplied data. this will never be the last block. user supplied data buffer. offset to user data buffer. user data buffer size.
create a proxy for an interface of implementations of that interface using the given failoverproxyprovider and the a set of retry policies specified by method name. if no retry policy is defined for a method then a default of retrypolicies#try_once_then_fail is used.
create a proxy for an interface of an implementation class using the a set of retry policies specified by method name. if no retry policy is defined for a method then a default of
create a proxy for an interface of implementations of that interface using the given failoverproxyprovider and the same retry policy for each method in the interface.
 create a proxy for an interface of an implementation class using the same retry policy for each method in the interface. 
return the default retry policy set in conf. if the value retrypolicyenabledkey is set to false in conf, use try_once_then_fail. otherwise, get the multiplelinearrandomretry policy specified in the conf and then (1) use multiplelinearrandomretry for - remoteexceptiontoretry, or - ioexception other than remoteexception, or - serviceexception; and (2) use try_once_then_fail for - non-remoteexceptiontoretry remoteexception, or - non-ioexception.
return the multiplelinearrandomretry policy specified in the conf, or null if the feature is disabled. if the policy is specified in the conf but the policy cannot be parsed, the default policy is returned. retry policy spec: n pairs of sleep-time and number-of-retries "s1,n1,s2,n2,..." or null if the feature is disabled.
for the lower rpc layers to set the async return value.
invoke the call once without retrying.
it first processes the wait time, if there is any, and then invokes #processretryinfo(). if the wait time is positive, it either sleeps for synchronous calls or immediately returns for asynchronous calls. otherwise, return callreturn#wait_retry.
 a retry policy for remoteexception set a default policy with some explicit handlers for specific exceptions. 
a retry policy for exceptions other than remoteexception.
parse the i-th element as an integer. otherwise, return the parsed value.
parse the given string as a multiplelinearrandomretry object. the format of the string is "t_1, n_1, t_2, n_2, ...", where t_i and n_i are the i-th pair of sleep time and number of retries. note that the white spaces in the string are ignored.
 keep trying forever with a fixed time between attempts. 
given the current number of retry, search the corresponding pair. or null if the current number of retry > maximum number of retry.
resets existing buffer with a new one of the specified size.
generate checksums for the given data chunks and output chunks  checksums to the underlying output stream.
writes len bytes from the specified byte array starting at offset off and generate a checksum for each data chunk.  this method stores bytes from the given array into this stream's buffer before it gets checksumed. the buffer gets checksumed and flushed to the underlying output stream when all data in a checksum chunk are in the buffer. if the buffer is empty and requested length is at least as large as the size of next checksum chunk size, this method will checksum and write the chunk directly to the underlying output stream. thus it avoids unnecessary data copy. @exception ioexception if an io error occurs.
retrieves the number of links to the specified file.
creates a hardlink
creates hardlinks from multiple existing files within one parent directory, into one target directory. parentdir.list()
validate the createflag for the append operation. the flag must contain append, and cannot contain overwrite.
validate the createflag for create operation
validate the createflag and throw exception if it is invalid
set mandatory int option.
constructor.
construct from a filecontext.
set optional float parameter for the builder.
set mandatory boolean option.
set a string array as mandatory option.
set mandatory float option.
set optional int parameter for the builder.
set mandatory double option.
set mandatory option to the builder. if the option is not supported or unavailable on the filesystem, the client should expect #build() throws illegalargumentexception.
set optional boolean parameter for the builder.
set an array of string values as optional parameter for the builder.
set to true to overwrite the existing file. set it to false, an exception will be thrown when calling #build() if the file exists.
set optional double parameter for the builder.
set optional builder parameter.
format: cmd: operation `path' to `target': error string
the src file is on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed
list files and its block locations in a directory.
returns a local file that the user can write output to. the caller provides both the eventual fs target name and the local working file. if the fs is local, we write directly into the target. if the fs is remote, we write into the tmp local area.
the src files are on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed
make sure that a path specifies a filesystem.
the src file is on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed
return a remote iterator for listing in a directory
the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. delsrc indicates if the src will be removed or not.
called when we're all done writing to the target. a local fs will do nothing, because we've written to exactly the right place. a remote fs will copy the contents of tmplocalfile to the correct target at fsoutputfile.
called after a new filesystem instance is constructed. for this filesystem
attempt calling overridden #docall(path) method with specified filesystem and path. if the call fails with an unresolvedlinkexception, it will try to resolve the path and retry the call by calling #next(filesystem, path).
get a path from the local fs. pass size as size_unknown if not known apriori. we round-robin over the set of disks (via the configured dirs) and return the first complete path which has enough space available disk)
get all of the paths that currently exist in the working directories.
this method must be used to obtain the dir allocation context for a particular value of the context name. the context name must be an item defined in the configuration object for which we want to control the dir allocations (e.g., mapred.local.dir). the method will create a context for that name if it doesn't already exist.
get a path from the local fs for reading. we search through all the configured dirs for the file's existence and return the complete path to the file when we find one
get a path from the local fs. if size is known, we go round-robin over the set of disks (via the configured dirs) and return the first complete path which has enough space. if size is not known, use roulette selection -- pick directories with probability proportional to their available space.
removes the context from the context config items
get a path from the local fs for reading. we search through all the configured dirs for the file's existence and return the complete path to the file when we find one
we search through all the configured dirs for the file's existence and return true when we find one
this method gets called everytime before any readwrite to make sure that any change to localdirs is reflected immediately.
creates a file on the local fs. pass size as round-robin over the set of disks (via the configured dirs) and return a file on the first path which has enough space. the file is guaranteed to go away when the jvm exits.
creates a temporary file in the local fs. pass size as -1 if not known apriori. we round-robin over the set of disks (via the configured dirs) and select the first complete path which has enough space. a file is created on this directory. the file is guaranteed to go away when the jvm exits.
get all of the paths that currently exist in the working directories.
method to check whether a context is valid
create an allocator object
utility function for partial evaluation of filestatus instances to a fixed set of handle options. produce pathhandle instances.
set an option
get an option of desired type returns null if there isn't any
a helper method for processing user input and default value to create a combined checksum option. this is a bit complicated because bytesperchecksum is kept for backward compatibility. ignored if  0.
utility method to extract a handleopt from the set provided. opt is null or a suitable match is not found.
return an array containing hostnames, offset and size of portions of the given file. for nonexistent file or regions, null is returned.  if f == null : result = null elif f.getlen() <= start: result = [] else result = [ locations(fs, b) for b in blocks(fs, p, s, s+l)]  this call is most helpful with and distributed filesystem where the hostnames of machines that contain blocks of the given file can be determined. the default implementation returns an array containing one element:  blocklocation( "localhost:9866" , "localhost" , 0, file.getlen())  in hdfs, if file is three-replicated, the returned array contains elements like:  blocklocation(offset: 0, length: block_size, hosts: "host1:9866", "host2:9866, host3:9866") blocklocation(offset: block_size, length: block_size, hosts: "host2:9866", "host3:9866, host4:9866")  and if a file is erasure-coded, the returned blocklocation are logical block groups. suppose we have a rs_3_2 coded file (3 data units and 2 parity units). 1. if the file size is less than one stripe size, say 2 cell_size, then there will be one blocklocation returned, with 0 offset, actual file size and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks. 3. if the file size is less than one group size but greater than one stripe size, then there will be one blocklocation returned, with 0 offset, actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting the actual blocks. 4. if the file size is greater than one group size, 3 block_size + 123 for example, then the result will be like:  blocklocation(offset: 0, length: 3 block_size, hosts: "host1:9866", "host2:9866","host3:9866","host4:9866","host5:9866") blocklocation(offset: 3 block_size, length: 123, hosts: "host1:9866", "host4:9866", "host5:9866") 
update old-format filesystem names, for back-compatibility. this should eventually be replaced with a checkname() method that throws an exception for old-format names.
get the number of large file system read operations such as list files under a large directory.
list corrupted file blocks. (may contain duplicates if a file has more than one corrupt block) (default).
load the filesystem declarations from service resources. this is a synchronized operation.
get the number of file system write operations such as create, append rename etc.
create and initialize a new instance of a filesystem. and to pass to the filesystem#initialize(uri, configuration).
cancel the scheduled deletion of the path when the filesystem is closed.
filter filesdirectories in the given path using the user-supplied path filter.  does not guarantee to return the list of filesdirectories status in a sorted order. a path name the user-supplied path filter after applying the filter
initialize a filesystem. called after the new filesystem instance is constructed, and before it is ready for use. filesystem implementations overriding this method must forward it to their superclass, though the order in which it is done, and whether to alter the configuration before the invocation are options of the subclass. for this filesystem
copy constructor.
delete all paths that were marked as delete-on-exit. this recursively deletes all files and directories in the specified paths. the time to process this operation is o(paths), with the actual time dependent on the time for existence and deletion operations to complete, successfully or not.
close all filesystem instances in the cache.
see filecontext#fixrelativepart.
get the map of statistics object indexed by uri scheme.
mark a path to be deleted when its filesystem is closed. when the jvm shuts down cleanly, all cached filesystem objects will be closed automatically. these the marked paths will be deleted as a result. if a filesystem instance is not cached, i.e. has been created with be deleted in when #close() is called on that instance. the path must exist in the filesystem at the time of the method call; it does not have to exist at the time of jvm shutdown. notes  clean shutdown of the jvm cannot be guaranteed. the time to shut down a filesystem will depends on the number of files to delete. for filesystems where the cost of checking for the existence of a filedirectory and the actual delete operation (for example: object stores) is high, the time to shutdown the jvm can be significantly extended by over-use of this feature. connectivity problems with a remote filesystem may delay shutdown further, and may cause the files to not be deleted. 
set the storage policy for a given file or directory. of supported storage policies can be retrieved via #getallstoragepolicies. (default outcome).
list a directory. the returned results include its block location if it is a file the results are filtered by the given path filter in the given path
get a filesystem for this uri's scheme and authority.   if the configuration has the property a new instance will be created, initialized with the supplied uri and configuration, then returned without being cached.   if the there is a cached fs instance matching the same uri, it will be returned.   otherwise: a new fs instance will be created, initialized with the configuration and uri, cached and returned to the caller.  
removes acl entries from files and directories. other acl entries are retained. (default outcome).
process the input stat. if it is a file, return the file stat. if it is a directory, traverse the directory if recursive is true; ignore it if recursive is false.
close this filesystem instance. will release any held locks, delete all files queued for deletion through calls to #deleteonexit(path), and remove this fs instance from the cache, if cached. after this operation, the outcome of any method call on this filesystem instance, or any inputoutput stream created by it is undefined.
filter filesdirectories in the given list of paths using user-supplied path filter.  does not guarantee to return the list of filesdirectories status in a sorted order. a list of paths the user-supplied path filter applying the filter
see filecontext#createsymlink(path, path, boolean).
modifies acl entries of files and directories. this method can add new acl entries or modify the permissions on existing acl entries. all existing acl entries that are not specified in this call are retained without changes. (modifications are merged into the current acl.) (default outcome).
get the filesystem implementation class of a filesystem. this triggers a scan and load of all filesystem implementations listed as services and discovered via the serviceloader a filesystem binding declaration in the configuration is skipped. for the scheme.
removes all default acl entries from files and directories. (default outcome).
query the effective storage policy id for the given file or directory. (default outcome).
renames path src to path dst  fails if src is a file and dst is a directory. fails if src is a directory and dst is a file. fails if the parent of dst does not exist or is a file.   if overwrite option is not passed as an argument, rename fails if the dst already exists.  if overwrite option is passed as an argument, rename overwrites the dst if it is a file or an empty directory. rename fails if dst is a non-empty directory.  note that atomicity of rename is dependent on the file system implementation. please refer to the file system documentation for details. this default implementation is non atomic.  this method is deprecated since it is a temporary method added to support the transition from filesystem to filecontext for user applications. path of dst does not exist. a directory
rename a snapshot. (default outcome).
removes all but the base acl entries of files and directories. the entries for user, group, and others are retained for compatibility with permission bits. (default outcome).
retrieve all the storage policies supported by this file system. (default outcome).
get the statistics for a particular file system.
get all of the xattrs namevalue pairs for a file or directory. only those xattrs which the logged-in user has permissions to view are returned.  refer to the hdfs extended attributes user documentation for details. (default outcome).
see filecontext#getlinktarget(path). (default outcome).
apply the given aggregator to all statisticsdata objects associated with this statistics object. for each statisticsdata object, we will call accept on the visitor. finally, at the end, we will call aggregate to get the final total.
canonicalize the given uri. this is implementation-dependent, and may for example consist of canonicalizing the hostname using dns and adding the default port if not specified. the default implementation simply fills in the default port if not specified and if #getdefaultport() returns a default port.
return the contentsummary of a given path.
see abstractfilesystem#getlinktarget(path). (default outcome).
performs clean-up action when the associated thread is garbage collected.
get the total number of bytes read.
get all statistics data. mr or other frameworks can use the method to get all statistics at once.
this method adds a filesystem instance to the cache so that it can be retrieved later. it is only for testing.
create a file with the provided permission. the permission of the file is set to be the provided permission as in setpermission, not  the hdfs implementation is implemented using two rpcs. it is understood that it is inefficient, but the implementation is thread-safe. the other option is to change the value of umask in configuration to be 0, but it is not thread-safe.
list the statuses and block locations of the files in the given path. does not guarantee to return the iterator that traverses statuses of the files in a sorted order.  if the path is a directory, if recursive is false, returns files in the directory; if recursive is true, return files in the subtree rooted at the path. if the path is a file, return the file's status and block locations. 
return an array containing hostnames, offset and size of portions of the given file. for a nonexistent file or regions, null is returned. this call is most helpful with location-aware distributed filesystems, where it returns hostnames of machines that contain the given file. a filesystem will normally return the equivalent result of passing the filestatus of the path to another fs that it could be delegating the call to
get all the trash roots for current user or all users. default filesystem returns .trash under users' home directories if
this create has been added to support the filecontext that processes the permission with umask before calling this method. this a temporary method added to support the transition from filesystem to filecontext for user applications.
get a filesystem instance based on the uri, the passed in configuration and the user. somehow interrupted.
get the total number of bytes read on erasure-coded files.
return a set of server default configuration values.
returns the filesystem for this uri's scheme and authority and the given user. internally invokes #newinstance(uri, configuration) somehow interrupted.
create a directory with the provided permission. the permission of the directory is set to be the provided permission as in setpermission, not 
filter filesdirectories in the given path using the user-supplied path filter. results are added to the given array results.
print all statistics for all file systems to system.out
get the total number of bytes written.
given an opaque iteration token, return the next batch of entries in a directory. this is a private api not meant for use by end users.  this method should be overridden by filesystem subclasses that want to use the generic filesystem#liststatusiterator(path) implementation. if this is the first call.
set an xattr of a file or directory. the name must be prefixed with the namespace followed by ".". for example, "user.attr".  refer to the hdfs extended attributes user documentation for details. (default outcome).
opens an fsdataoutputstream at the indicated path with write-progress reporting. same as create(), except fails if parent directory doesn't already exist.
check that a path belongs to this filesystem. the base implementation performs case insensitive equality checks of the uris' schemes and authorities. subclasses may implement slightly different checks. part of this filesystem.
get or create the thread-local data associated with the current thread.
get the fs instance if the key maps to an instance, creating and initializing the fs if it is not found. if this is the first entry in the map and the jvm is not shutting down, this registers a shutdown hook to close filesystems, and adds this fs to the toautoclose set if "fs.automatic.close" is set in the configuration (default: true).
this method provides the default implementation of
delete a snapshot of a directory. (default outcome).
get the number of file system read operations such as list files.
returns the filesystem for this uri's scheme and authority. the entire uri is passed to the filesystem instance's initialize method. this always returns a new filesystem object.
get the default filesystem uri from a configuration.
this version of the mkdirs method assumes that the permission is absolute. it has been added to support the filecontext that processes the permission with umask before calling this method. this a temporary method added to support the transition from filesystem to filecontext for user applications.
create a snapshot.
resets all statistics to 0. in order to reset, we add up all the thread-local statistics data, and set rootdata to the negative of that. this may seem like a counterintuitive way to reset the statistics. why can't we just zero out all the thread-local data? well, thread-local data can only be modified by the thread that owns it. if we tried to modify the thread-local data from this thread, our modification might get interleaved with a read-modify-write operation done by the thread that owns the data. that would result in our update getting lost. the approach used here avoids this problem because it only ever reads (not writes) the thread-local data. both reads and writes to rootdata are done under the lock, so we're free to modify rootdata from any thread that holds the lock.
in case of the symlinks or mount points, one has to move the appropriate trashbin in the actual volume of the path p being deleted. hence we get the file system of the fully-qualified resolved-path and then move the path p to the trashbin in that volume,
rename filesdirs
the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. if src and dst are directories, the copycrc parameter determines whether to copy crc files.
set replication for an existing file. implement the abstract setreplication of filesystem false if file does not exist or is a directory
opens an fsdatainputstream at the indicated path.
implement the delete(path, boolean) in checksum file system.
seek to the given position in the stream. the next read() will be from that position. this method does not allow seek past the end of the file. this produces ioexception. @exception ioexception if an io error occurs or seeks after eof checksumexception if the chunk to seek to is corrupted
perform a fallback read.
construct given a filecontext and a path.
rename a snapshot. exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
list the statuses of the filesdirectories in the given path if the path is a directory. return the file's status and block locations if the path is a file. if a returned status is a file, it contains the file's block locations. in the given path if any io exception (for example the input directory gets deleted while listing is being executed), next() or hasnext() of the returned iterator may throw a runtimeexception with the io exception as the cause. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
make(create) a directory and all the non-existent parents. then parent must exist exists and createparent is false directory is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server runtimeexceptions:
return a file status object that represents the path. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
unset the storage policy set for a given file or directory.
copy from src to dst, optionally deleting src and overwriting dst. and overwrite is false. a directory src or dst is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server runtimeexceptions:
get all of the xattr names for a file or directory. only those xattr names which the logged-in user has permissions to view are returned.  refer to the hdfs extended attributes user documentation for details.
removes acl entries from files and directories. other acl entries are retained.
resolves all symbolic links in the specified path leading up to, but not including the final path component.
set an xattr of a file or directory. the name must be prefixed with the namespace followed by ".". for example, "user.attr".  refer to the hdfs extended attributes user documentation for details.
create or overwrite file on indicated path and returns an output stream for writing into the file.  progress - to report progress on the operation - default null permission - umask is applied against permission: default is fspermissions:getdefault() createparent - create missing parent path; default is to not to create parents the defaults for the following are ss defaults of the file server implementing the target path. not all parameters make sense for all kinds of file system - eg. localfs ignores blocksize, replication, checksum  buffersize - buffersize used in fsdataoutputstream blocksize - block size for file blocks replicationfactor - replication for blocks checksumparam - checksum parameters. server default is used if not specified.   and createparent is false directory. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server runtimeexceptions:
create a filecontext for specified default uri using the specified config. not supported could not be instantiated, or if login fails.
set access time of a file. the number of milliseconds since epoch (jan 1, 1970). a value of -1 means that this call should not set modification time. the number of milliseconds since jan 1, 1970. a value of -1 means that this call should not set access time. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
list the statuses and block locations of the files in the given path. if the path is a directory, if recursive is false, returns files in the directory; if recursive is true, return files in the subtree rooted at the path. the subtree is traversed in the depth-first order. if the path is a file, return the file's status and block locations. files across symbolic links are also returned. if any io exception (for example a sub-directory gets deleted while listing is being executed), next() or hasnext() of the returned iterator may throw a runtimeexception with the io exception as the cause. is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
fully replaces acl of files and directories, discarding all existing entries. for user, group, and others for compatibility with permission bits.
delete all the paths that were marked as delete-on-exit.
set the working directory for wd-relative names (such a "foobar"). working directory feature is provided by simply prefixing relative names with the working dir. note this is different from unix where the wd is actually set to the inode. hence setworkingdir does not follow symlinks etc. this works better in a distributed environment that has multiple independent roots.  newwdir can be one of:  relative path: "foobar"; absolute without scheme: "foobar" fully qualified with scheme: "xx:authfoobar"   illegal wds:  relative with scheme: "xx:foobar" non existent directory 
set permission of a path. is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
list the statuses of the filesdirectories in the given path if the path is a directory. in the given path not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
check if copying srcname to dst would overwrite an existing file or directory. existing file and the overwrite option is not passed.
returns a status object describing the use and capacity of the file system denoted by the parh argument p. if the file system has multiple partitions, the use and capacity of the partition pointed to by the specified path is reflected. root partition of the default file system. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
return the contentsummary of path f. f is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
get delegation tokens for the file systems accessed for a given path.
opens an fsdatainputstream at the indicated path. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
are qualsrc and qualdst of the same file system?
returns the next file's status with its block locations file's status or locations fs is unsupported for example, namenode is not avaialbe or namenode throws ioexception due to an error while getting the status or block locations
get all of the xattrs for a file or directory. only those xattrs for which the logged-in user has permissions to view are returned.  refer to the hdfs extended attributes user documentation for details.
delete a snapshot of a directory. exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
returns the target of the given symbolic link as it was specified when the link was created. links in the path leading up to the final path component are resolved transparently. not supported or an io error occurred
mark a path to be deleted on jvm shutdown. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
returns true if the iterator has more files. file's status or locations fs is unsupported for example, namenode is not avaialbe or namenode throws ioexception due to an error while getting the status or block locations
returns the list of abstractfilesystems accessed in the path. the list may contain more than one abstractfilesystems objects in case of symlinks. path which needs to be resolved
renames path src to path dst   fails if src is a file and dst is a directory. fails if src is a directory and dst is a file. fails if the parent of dst does not exist or is a file.   if overwrite option is not passed as an argument, rename fails if the dst already exists.  if overwrite option is passed as an argument, rename overwrites the dst if it is a file or an empty directory. rename fails if dst is a non-empty directory.  note that atomicity of rename is dependent on the file system implementation. please refer to the file system documentation for details  options has options.rename#overwrite option false. directory and dst is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
removes all default acl entries from files and directories.
set replication for an existing file. exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
checks if the user can access a path. the mode specifies which access checks to perform. if the requested permissions are granted, then the method returns normally. if access is denied, then the method throws an  the default implementation of this method calls #getfilestatus(path) and checks the returned permissions against the requested permissions. note that the getfilestatus call will be subject to authorization checks. typically, this requires search (execute) permissions on each directory in the path's prefix, but this is implementation-defined. any file system that provides a richer authorization model (such as acls) may override the default implementation so that it checks against that model instead.  in general, applications should avoid using this method, due to the risk of time-of-checktime-of-use race conditions. the permissions on a file may change immediately after the access call returns. most applications should prefer running specific file system actions as the desired user represented by a usergroupinformation. is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
gets the acls of files and directories.
get the checksum of a file. which indicates that no checksum algorithm is implemented in the corresponding filesystem. exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
delete a file. true, the directory is deleted else throws an exception. in case of a file the recursive can be set to either true or false. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server runtimeexceptions:
return all the files that match filepattern and are not checksum files. results are sorted by their names.  a filename pattern is composed of regular characters and special pattern matching characters, which are:       ?   matches any single character.      matches zero or more characters.    [abc]   matches a single character from character set a,b,c.    [a-b]   matches a single character from the character range a...b. note: character a must be lexicographically less than or equal to character b.    [^a]   matches a single char that is not from character set or range a. note that the ^ character must occur immediately to the right of the opening bracket.    \c   removes (escapes) any special meaning of character c.    ab,cd   matches a string from the string set ab, cd     ab,cde,fh   matches a string from string set ab, cde, cfh    pathpattern is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
create a snapshot. exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
set owner of a path (i.e. a file or a directory). the parameters username and groupname cannot both be null. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server runtimeexceptions: groupname is invalid.
filter filesdirectories in the given path using the user-supplied path filter. after applying the filter pathpattern is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
(may contain duplicates if a file has more than one corrupt block)
truncate the file in the indicated path to the indicated size.  fails if path is a directory. fails if path does not exist. fails if path is not closed. fails if new size is greater than current size.  newlength and is immediately available to be reused for write operations such as append, or false if a background process of adjusting the length of the last block has been started, and clients should wait for it to complete before proceeding with further file updates. not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
create a filecontext using the passed config. generally it is better to use is not supported
modifies acl entries of files and directories. this method can add new acl entries or modify the permissions on existing acl entries. all existing acl entries that are not specified in this call are retained without changes. (modifications are merged into the current acl.)
set the storage policy for a given file or directory. of supported storage policies can be retrieved via #getallstoragepolicies.
list the statuses of the filesdirectories in the given path if the path is a directory. in the given path not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
process the input stat. if it is a file, return the file stat. if it is a directory, traverse the directory if recursive is true; ignore it if recursive is false. if it is a symlink, resolve the symlink first and then process it depending on if it is a file or directory.
creates a symbolic link to an existing file. an exception is thrown if the symlink exits, the user does not have permission to create symlink, or the underlying file system does not support symlinks. symlink permissions are ignored, access to a symlink is determined by the permissions of the symlink target. symlinks in paths leading up to the final path component are resolved transparently. if the final path component refers to a symlink some functions operate on the symlink itself, these are: - delete(f) and deleteonexit(f) - deletes the symlink. - rename(src, dst) - if src refers to a symlink, the symlink is renamed. if dst refers to a symlink, the symlink is over-written. - getlinktarget(f) - returns the target of the symlink. - getfilelinkstatus(f) - returns a filestatus object describing the symlink. some functions, create() and mkdir(), expect the final path component does not exist. if they are given a path that refers to a symlink that does exist they behave as if the path referred to an existing file or directory. all other functions fully resolve, ie follow, the symlink. these are: open, setreplication, setowner, settimes, setworkingdirectory, setpermission, getfilechecksum, setverifychecksum, getfileblocklocations, getfsstatus, getfilestatus, exists, and liststatus. symlink targets are stored as given to createsymlink, assuming the underlying file system is capable of storing a fully qualified uri. dangling symlinks are permitted. filecontext supports four types of symlink targets, and resolves them as follows  given a path referring to a symlink of form: <---x---> fs:hostablink <-----y-----> in this path x is the scheme and authority that identify the file system, and y is the path leading up to the final path component "link". if y is a symlink itself then let y' be the target of y and x' be the scheme and authority of y'. symlink targets may: 1. fully qualified uris fs:hostxabfile resolved according to the target file system. 2. partially qualified uris (eg scheme but no host) fs:abfile resolved according to the target file system. eg resolving a symlink to hdfs:a results in an exception because hdfs uris must be fully qualified, while a symlink to file:a will not since hadoop's local file systems require partially qualified uris. 3. relative paths path resolves to [y'][path]. eg if y resolves to hdfs:hosta and path is "..bfile" then [y'][path] is hdfs:hostbfile 4. absolute paths path resolves to [x'][path]. eg if y resolves hdfs:hostab and path is "file" then [x][path] is hdfs:hostfile  false then parent must exist directory. target or link is not supported
remove an xattr of a file or directory. the name must be prefixed with the namespace followed by ".". for example, "user.attr".  refer to the hdfs extended attributes user documentation for details.
get the file system of supplied path. absorfqpath is not supported. not be instantiated.
removes all but the base acl entries of files and directories. the entries for user, group, and others are retained for compatibility with permission bits.
resolves all symbolic links in the specified path. returns the new path object.
get an xattr for a file or directory. the name must be prefixed with the namespace followed by ".". for example, "user.attr".  refer to the hdfs extended attributes user documentation for details.
query the effective storage policy id for the given file or directory.
return an array of filestatus objects whose path names match pathpattern and is accepted by the user-supplied path filter. results are sorted by their path names. return null if pathpattern has no glob and the path does not exist. return an empty array if pathpattern has a glob and no path matches it. pathpattern is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
return a file status object that represents the path. if the path refers to a symlink then the filestatus of the symlink is returned. the behavior is equivalent to #getfilestatus() if the underlying file system does not support symbolic links. not supported
get all of the xattrs for a file or directory. only those xattrs for which the logged-in user has permissions to view are returned.  refer to the hdfs extended attributes user documentation for details.
filter filesdirectories in the given list of paths using user-supplied path filter. applying the filter exist exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
opens an fsdatainputstream at the indicated path using default buffersize. is not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server
return blocklocation of the given file for the given offset and len. for a nonexistent file or regions, null will be returned. this call is most helpful with dfs, where it returns hostnames of machines that contain the given file. in hdfs, if file is three-replicated, the returned array contains elements like:  blocklocation(offset: 0, length: block_size, hosts: "host1:9866", "host2:9866, host3:9866") blocklocation(offset: block_size, length: block_size, hosts: "host2:9866", "host3:9866, host4:9866")  and if a file is erasure-coded, the returned blocklocation are logical block groups. suppose we have a rs_3_2 coded file (3 data units and 2 parity units). 1. if the file size is less than one stripe size, say 2 cell_size, then there will be one blocklocation returned, with 0 offset, actual file size and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks. 3. if the file size is less than one group size but greater than one stripe size, then there will be one blocklocation returned, with 0 offset, actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting the actual blocks. 4. if the file size is greater than one group size, 3 block_size + 123 for example, then the result will be like:  blocklocation(offset: 0, length: 3 block_size, hosts: "host1:9866", "host2:9866","host3:9866","host4:9866","host5:9866") blocklocation(offset: 3 block_size, length: 123, hosts: "host1:9866", "host4:9866", "host5:9866")  not supported exceptions applicable to file systems accessed over rpc: undeclared exception to rpc server runtimeexceptions:
return true if a statistic is being tracked.
encode byte[] value to string representation with encoding. values encoded as text strings are enclosed in double quotes (\"), while strings encoded as hexadecimal and base64 are prefixed with 0x and 0s, respectively.
decode string representation of a value and check whether it's encoded. if the given string begins with 0x or 0x, it expresses a hexadecimal number. if the given string begins with 0s or 0s, base64 encoding is expected. if the given string is enclosed in double quotes, the inner string is treated as text. otherwise the given string is treated as text.
return the header of with the storagetypes.
return true if any storage type quota has been set.
return true if any storage type consumption information is available.
convert boolean attributes to a set of flags.
write instance encoded as protobuf to stream.
read instance encoded as protobuf from stream.
expand the leftmost outer curly bracket pair containing a slash character ("") in filepattern.
expand globs in the given filepattern into a collection of file patterns so that in the expanded set no file pattern has a slash character ("") in a curly bracket pair.  some examples of how the filepattern is expanded:   filepattern - expanded file pattern  ab - ab ab - ab pab,cds - pabs, pcds ab,cd,e,f - ab, cd, e,f ab,cde,f - abe,f, cde,f a,bb,cd,ef - a,bb, a,bcd, a,bef a,bc\d - a,bcd 
finds the index of the leftmost opening curly bracket containing a slash character ("") in filepattern. slash character (""), or -1 if there is no such bracket
set and compile a glob pattern
get the delay until this event should happen.
renew or replace the delegation token for this file system. it can only be called when the action is not in the queue.
add a renew action to the queue.
remove the associated renew action from the queue
return true if a statistic is being tracked.
keeps track of disk usage. the actual interval will be chosen uniformly between
delete the given path to a file or directory.
(note: returned list is not sorted in any given order, due to reliance on java's file#list() api.)
convert a path to a file.
sets the path's last modified time and last access time to the given valid times.
return a filestatus representing the given path. if the path refers to a symlink return a filestatus representing the link rather than the object the link refers to.
deprecated. remains for legacy support. should be removed when stat gains support for windows and other operating systems.
calls out to platform's native stat(1) implementation to get file metadata (permissions, user, group, atime, mtime, etc). this works around the lack of lstat(2) in java 6. currently, the stat class used to do this only supports linux and freebsd, so the old #deprecatedgetfilelinkstatusinternal(path) implementation (deprecated) remains further os support is added.
hook to implement support for pathhandle operations.
parse the first argument into an owner and group
register the permission related commands with the factory
returns all jars that are in the directory. it is useful in expanding a wildcard path to return all jars from the directory to use in a classpath. the directory does not exist
returns the target of the given symlink. returns the empty string if the given path does not refer to a symlink or there is an error accessing the symlink. a symlink.
a wrapper for file#list(). this java.io api returns null when a dir is not a directory or for any io error. instead of having null check everywhere file#list() is used, we will add utility api to get around this problem. for the majority of cases where we prefer an ioexception to be thrown. @exception accessdeniedexception for unreadable directory @exception ioexception for invalid directory or for bad disk
platform independent implementation for file#setwritable(boolean) file#setwritable does not work as expected on windows.
delete the contents of a directory, not the directory itself. if we return false, the directory may be partially-deleted. if dir is a symlink to a directory, all the contents of the actual directory pointed to by dir will be deleted. and all the underlying directories before trying to delete their contents.
a wrapper for file#listfiles(). this java.io api returns null when a dir is not a directory or for any io error. instead of having null check everywhere file#listfiles() is used, we will add utility api to get around this problem. for the majority of cases where we prefer an ioexception to be thrown. @exception ioexception for invalid directory or for a bad disk.
change the permissions on a file directory, recursively, if needed.
run a command and send the contents of an input stream to it.
recursively delete a directory.
takes an input dir and returns the du on that local directory. very basic implementation. the input dir to get the disk space of this local dir
given a tar file as input it will untar the file in a the untar directory passed as the second parameter this utility will untar ".tar" files and ".tar.gz","tgz" files.
set permissions to the required value. uses the java primitives instead of forking if group == other.
given a stream input it will unzip the it in the unzip directory. passed as the second parameter
given a file input it will unzip it in the unzip directory. passed as the second parameter
given a tar file as input it will untar the file in a the untar directory passed as the second parameter this utility will untar ".tar" files and ".tar.gz","tgz" files. todo use magic number and pusbackinputstream to identify
create a soft link between a src and destination only on a local disk. hdfs does not support this. on windows, when symlink creation fails due to security setting, we will log a warning. the return code in this case is 2.
convert an array of filestatus to an array of path. if stats if null, return path an array of filestatus objects default path to return in stats is null
copy filesystem files to local files.
platform independent implementation for file#canexecute() on windows, true if process has execute access on the path
copy local files to a filesystem.
copy filesystem files to local files.
create a jar file at the given path, containing a manifest with a classpath that references all specified entries. some platforms may have an upper limit on command line length. for example, the maximum command line length on windows is 8191 characters, but the length of the classpath may exceed this. to work around this limitation, use this method to create a small intermediate jar with a manifest that contains the full classpath. it returns the absolute path to the new jar, which the caller may set as the classpath for a new process. environment variable evaluation is not supported within a jar manifest, so this method expands environment variables before inserting classpath entries to the manifest. the method parses environment variables according to platform-specific syntax (%var% on windows, or $var otherwise). on windows, environment variables are case-insensitive. for example, %var% and %var% evaluate to the same value. specifying the classpath in a jar manifest does not support wildcards, so this method expands wildcards internally. any classpath entry that ends with is translated to all files at that path with extension .jar or .jar. for expansion unexpanded wild card entry path in position 1
register all files recursively to be deleted on exit.
convert a os-native filename to a path that works for the shell and avoids script injection attacks.
delete a directory and all its contents. if we return false, the directory may be partially-deleted. (1) if dir is symlink to a file, the symlink is deleted. the file pointed to by the symlink is not deleted. (2) if dir is symlink to a directory, symlink is deleted. the directory pointed to by symlink is not deleted. (3) if dir is a normal file, it is deleted. (4) if dir is a normal directory, then dir and all its contents recursively are deleted.
platform independent implementation for file#setexecutable(boolean) file#setexecutable does not work as expected on windows. note: revoking execute permission on folders does not have the same behavior on windows as on unix platforms. creating, deleting or renaming a file within that folder will still succeed on windows.
create a tmp file for a base file. @exception ioexception if a tmp file cannot created
copy files between filesystems.
platform independent implementation for file#canread() on windows, true if process has read access on the path
platform independent implementation for file#setreadable(boolean) file#setreadable does not work as expected on windows.
move the src file to the name specified by target. @exception ioexception if this operation fails
copy files between filesystems.
convert an array of filestatus to an array of path an array of filestatus objects
set the ownership on a file directory. user name and group name cannot both be null.
platform independent implementation for file#canwrite() on windows, true if process has write access on the path
implement the delete(path, boolean) in checksum file system.
opens an fsdatainputstream at the indicated path.
seek to the given position in the stream. the next read() will be from that position. this method does not allow seek past the end of the file. this produces ioexception. @exception ioexception if an io error occurs or seeks after eof checksumexception if the chunk to seek to is corrupted
main() has some simple utility methods
run
performs any necessary cleanup
returns true if the specified string is considered valid in the path part of a uri by this file system. the default implementation enforces the rules of hdfs, but subclasses may override this method to implement specific validation rules for specific file systems.
check that the uri's scheme matches
get all of the xattr names for a file or directory. only the xattr names for which the logged-in user has permissions to view are returned.  refer to the hdfs extended attributes user documentation for details.
get all of the xattrs for a file or directory. only those xattrs for which the logged-in user has permissions to view are returned.  refer to the hdfs extended attributes user documentation for details.
the specification of this method matches that of
the specification of this method matches that of must be for this file system. in hdfs implementation, the blocklocation of returned locatedfilestatus will have different formats for replicated and erasure coded file. please refer to filesystem#getfileblocklocations(filestatus, long, long) for more details.
the specification of this method matches that of file system.
create a file system instance for the specified uri using the conf. the conf is used to find the class name that implements the file system. the conf is also passed to the file system for its configuration. not found
removes all default acl entries from files and directories.
retrieve the storage policy for a given file or directory.
the specification of this method matches that of this file system.
check that a path belongs to this filesystem. if the path is fully qualified uri, then its scheme and authority matches that of this file system. otherwise the path must be slash-relative name.
removes all but the base acl entries of files and directories. the entries for user, group, and others are retained for compatibility with permission bits.
the specification of this method matches that of
(may contain duplicates if a file has more than one corrupt block)
retrieve all the storage policies supported by this file system.
modifies acl entries of files and directories. this method can add new acl entries or modify the permissions on existing acl entries. all existing acl entries that are not specified in this call are retained without changes. (modifications are merged into the current acl.)
set the storage policy for a given file or directory. of supported storage policies can be retrieved via #getallstoragepolicies.
prints statistics for all file systems.
the specification of this method matches that of
the specification of this method matches that of that the path f must be fully qualified and the permission is absolute (i.e. umask has been applied).
create an object for the given class and initialize it from conf.
get the uri for the file system based on the given uri. the path, query part of the given uri is stripped out and default file system port is used to form the uri. false authority must be null.
get the statistics for a particular file system. used as key to lookup statistics_table. only scheme and authority part of the uri are used.
set an xattr of a file or directory. the name must be prefixed with the namespace followed by ".". for example, "user.attr".  refer to the hdfs extended attributes user documentation for details.
get the path-part of a pathname. checks that uri matches this file system and that the path-part is a valid name.
the specification of this method matches that of f must be for this file system.
partially resolves the path. this is used during symlink resolution in
removes acl entries from files and directories. other acl entries are retained.
seek to the given position in the stream. the next read() will be from that position. this method may seek past the end of the file. this produces no exception and an attempt to read from the stream will result in -1 indicating the end of the file. @exception ioexception if an io error occurs. checksumexception if the chunk to seek to is corrupted
a utility function that tries to read up to len bytes from stm
read checksum verified bytes from this byte-input stream into the specified byte array, starting at the given offset.  this method implements the general contract of the corresponding  inputstream#read(byte[], int, int) read method of the  inputstream class. as an additional convenience, it attempts to read as many bytes as possible by repeatedly invoking the read method of the underlying stream. this iterated read continues until one of the following conditions becomes true:   the specified number of bytes have been read,  the read method of the underlying stream returns -1, indicating end-of-file.  if the first read on the underlying stream returns -1 to indicate end-of-file then this method returns -1. otherwise this method returns the number of bytes actually read. the stream has been reached. @exception ioexception if an io error occurs. checksumexception if any checksum error occurs
set the checksum related parameters
validation code, available for use in subclasses. data requested.
initialize using a copy of bytes from the serialized handle.
store a reference to the given bytes as the serialized form.
moves files to a bad file directory on the same device, so that their storage will not be reused.
translate an absolute path into a list of path components. we merge double slashes into a single slash here. posix root path, i.e. '', does not get an entry in the list.
initialize a har filesystem per har archive. the archive home directory is the top level directory in the filesystem that contains the har archive. be careful with this method, you do not want to go on creating new filesystem instances per call to path.getfilesystem(). the uri of har is har:underlyingfsscheme-host:portarchivepath. or har:archivepath. this assumes the underlying filesystem to be used in case not specified.
not implemented.
get filestatuses of all the children of a given directory. this just reads through index file and reads line by line to get all statuses for children of a directory. its a brute force way of getting all such filestatuses the parent path directory the list to add the children filestatuses to
not implemented.
combine the status stored in the index and the underlying status.
decode the raw uri to get the underlying uri
get block locations from the underlying fs and fix their offsets and lengths.
create a har specific auth har-underlyingfs:port filesystem
implementing position readable.
not implemented.
not implemented.
returns a har input stream which fakes end of file. it reads the index files to get the part file name and the size and start of the file.
position readable again.
not implemented.
this method returns the path inside the har filesystem. this is relative path inside the har filesystem.
liststatus returns the children of a directory after looking up the index files.
fix offset and length of block locations. note that this method modifies the original array.
implement the policy for canunbuffer#unbuffer().
constructor
return a fully-qualified version of the given symlink target if it has no scheme and authority. partially and fully-qualified paths are returned unmodified.
performs the operation specified by the next function, calling it repeatedly until all symlinks in the given path are resolved.
reset all global storage statistics.
create or return the storagestatistics object with the given name. object if needed. object or a new storagestatistics object with the wrong name.
get an instance of the configured trashpolicy based on the value of the configuration parameter fs.trash.classname.
get an instance of the configured trashpolicy based on the value of the configuration parameter fs.trash.classname.
a frozen version of #tostring() to be backward compatible. when backward compatibility is not needed, use #tostring(), which provides more info and is supposed to evolve. don't change this method except for major revisions. note: currently this method is used by cli for backward compatibility.
returns a qualified path object. components, borrow them from this uri working directory a new path that includes a path and authority and is fully qualified
returns the number of elements in this path.
returns the final component of this path.
construct a path from a string. path strings are uris, but with unescaped elements and some additional normalization.
test whether this path uses a scheme and is relative. pathnames with scheme and relative path are illegal.
validate the contents of a deserialized path, so as to defend against malicious object streams.
merge 2 paths such that the second path is appended relative to the first. the returned path has the scheme and authority of the first path. on windows, the drive specification in the second path is discarded.
construct a path from components.
returns the parent of a path or null if at root.
return a version of the given path without the scheme information.
determine whether a given path string represents an absolute path on windows. e.g. "c:ab" is an absolute path. "c:ab" is not. drive-specifier
normalize a path string to use non-duplicated forward slashes as the path separator and remove any trailing path separators. should replace backslashes or not
create a new path based on the child path resolved against the parent path.
strip out the root from the path.
construct a simple link (i.e. not a mergelink).
create inode tree from the specified mount-table specified in config fsconstants.config_viewfs_prefix
resolve the pathname p relative to root inodedir
get the target of the link. if a merge link then it returned as "," separated uri list.
get the value of the home dir conf value for specified mount table
add config variable for homedir the specified mount table
add a linkmerge to the config for the specified mount table.
add a linkfallback to the config for the specified mount table.
add a linkmergeslash to the config for the specified mount table.
add a link to the config for the specified mount table
category: read.
initializes an nfly mountpoint in viewfs.
creates a new nfly instance.
returns the closest non-failing destination's result.
iterate all available nodes in the proximity order to attempt repair of all filenotfound nodes.
get fsstatus for all viewfsmountpoints matching path for the given viewfilesystem. say viewfilesystem has following mount points configured (1) hdfs:nn0_host:portsales mounted on deptsales (2) hdfs:nn1_host:portmarketing mounted on deptmarketing (3) hdfs:nn2_host:porteng_usa mounted on deptengusa (4) hdfs:nn3_host:porteng_asia mounted on deptengasia for the above config, here is a sample list of paths and their matching mount points while getting fsstatus path description matching mountpoint "" root viewfilesystem lists all (1), (2), (3), (4) mount points. "dept" not a mount point, but a valid (1), (2), (3), (4) internal dir in the mount tree and resolved down to "" path. "deptsales" matches a mount point (1) "deptsalesindia" path is over a valid mount point (1) and resolved down to "deptsales" "depteng" not a mount point, but a valid (1), (2), (3), (4) internal dir in the mount tree and resolved down to "" path. "erp" doesn't match or leads to or over any valid mount points none
update fsstatus for the given the mount point.
this constructor has the signature needed by
retrieve the storage policy for a given file or directory.
strip out the root from the path.
constructor
called after a new filesystem instance is constructed. this filesystem
return the total size of all files under "", if constants#config_viewfs_link_merge_slash is supported and is a valid mount point. else, throw notinmountpointexception.
get the trash root directory for current user when the path specified is deleted.
get all the trash roots for current user or all users.
this constructor has the signature needed by
set the ftpclient's data connection mode based on configuration. valid values are active_local_data_connection_mode, passive_local_data_connection_mode and passive_remote_data_connection_mode.  defaults to active_local_data_connection_mode.
probe for a path being a parent of another
this optional operation is not yet supported.
convert the file information in ftpfile to a filestatus object.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
connect to the ftp server using configuration parameters
resolve against given working directory.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
a stream obtained via this call must be closed before using other apis of this class or else the invocation will block.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
logout and disconnect the given ftpclient.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
set ftp's transfer mode based on configuration. valid values are stream_transfer_mode, block_transfer_mode and compressed_transfer_mode.  defaults to block_transfer_mode.
returns a list of pathdata objects of the items contained in the given directory.
ensure that the file exists and if it is or is not a directory what was specified in typerequirement.
check if the path represents a directory as determined by the basename being "." or "..", or the path ending with a directory separator
creates an object to wrap the given parameters as fields. the string used to create the path will be recorded since the path object does not return exactly the same string used to initialize it.
get the filestatus info
normalize the given windows path string. this does the following: 1. adds "file:" scheme for absolute paths. 2. ensures the scheme-specific part starts with '' per rfc2396. 3. replaces backslash path separators with forward slashes. if it is not a windows absolute path.
returns a remoteiterator for pathdata objects of the items contained in the given directory.
construct a uri from a string with unescaped special characters that have non-standard semantics. e.g. , ?, #. a custom parsing is needed to prevent misbehavior.
validates the given windows path. inferred; false, otherwise.
get the path to a local file
expand the given path as a glob pattern. non-existent paths do not throw an exception because creation commands like touch and mkdir need to create them. the "stat" field will be null if the path does not exist. and does not exist, the list will contain a single pathdata with a null stat
given a child of this directory, use the directory's path and the child's basename to construct the string to the child. this preserves relative paths since path will fully qualify.
get a public static class field
display an exception prefaced with the command name. also increments the error count for the command which will result in a non-zero exit code.
expands a list of arguments into pathdata objects. the default behavior is to call #expandargument(string) on each element which by default globs the argument. the loop catches ioexceptions, increments the error count, and displays the exception.
the short usage suitable for the synopsis
iterates over the given expanded paths and invokes will do a post-visit dfs on directories.
invokes the command handler. the default behavior is to process options, expand arguments, and then process each argument.  run |-> #processoptions(linkedlist) \-> #processrawarguments(linkedlist) |-> #expandarguments(linkedlist) | \-> #expandargument(string) \-> #processarguments(linkedlist) |-> #processargument(pathdata) | |-> #processpathargument(pathdata) | \-> #processpaths(pathdata, pathdata...) | \-> #processpath(pathdata) \-> #processnonexistentpath(pathdata)  most commands will chose to implement just
expand the given argument into a list of pathdata objects. the default behavior is to expand globs. commands may override to perform other expansions on an argument.
the name of the command. will first try to use the assigned name else fallback to the command's preferred name
register the command classes used by the fs subcommand
compute column widths and rebuild the format string
initialise the comparator to be used for sorting files. if multiple options are selected then the order is chosen in the following precedence: - modification time (or access time if requested) - file size - file name
wait for all files in waitlist to have length equal to newlength.
wait for all files in waitlist to have replication number equal to rep.
close the stream.
read a single byte from the stream.
add a row of objects to the table
create a table wo headers
render the table to a stream.
add a new row to the usages table for the given filesystem uri.
prints a single extended acl entry. if the mask restricts the permissions of the entry, then also prints the restricted version as the effective permissions. the mask applies to all named entries and also the unnamed group entry.
prints all the acl entries in a single scope.
check the source and target paths to ensure that they are either both in .reservedraw or neither in .reservedraw. if neither src nor target are in .reservedraw, then return false, indicating not to preserve raw. xattrs. if both srctarget are in .reservedraw, then return true, indicating raw. xattrs should be preserved. if only one of srctarget is in .reservedraw then throw an exception. path, not relative. path, not relative. .reservedraw.
if direct write is disabled ,copies the stream contents to a temporary file "._copying_". if the copy is successful, the temporary file will be renamed to the real path, else the temporary file will be deleted. if direct write is enabled , then creation temporary file is skipped.
add file attributes that need to be preserved. this method may be called multiple times to add attributes.
preserve the attributes of the source to the target. the method calls #shouldpreserve(fileattribute) to check what attribute to preserve.
the last arg is expected to be a remote path, if only one argument is given then the destination will be the remote user's directory
if true, the last modified time, last access time, owner, group and permission information of the source file will be preserved as far as target filesystem implementation allows.
called with a source and target destination pair
the last arg is expected to be a local path, if only one argument is given then the destination will be the current directory
gets all of the registered commands
get an instance of the requested command
invokes "static void registercommands(commandfactory)" on the given class. this method abstracts the contract between the factory and the command class. do not assume that directly invoking registercommands on the given class will have the same effect.
register the given object as handling the given list of command names. avoid calling this method and use startup overhead from excessive command object instantiations. this method is intended only for handling nested non-static classes that are re-usable. namely -help-usage.
parse parameters starting from the given position consider using the variant that directly takes a list
add option with value
parse parameters from the given list of args. the list is destructively modified to remove the options.
simple parsing of command line arguments
returns all the options that are set
get an instance of the requested expression name of the command to lookup the hadoop configuration
creates an instance of the requested expression class. name of the expression class to be instantiated the hadoop configuration
register the given class as handling the given list of expression names. the class implementing the expression names one or more command names that will invoke this class if the expression is not of an expected type
invokes "static void registerexpression(findexpressionfactory)" on the given class. this method abstracts the contract between the factory and the expression class. do not assume that directly invoking registerexpression on the given class will have the same effect. class to allow an opportunity to register
registers this expression with the specified factory.
applies child expressions to the pathdata item. if all pass then returns result#pass else returns the result of the first non-passing expression.
registers this expression with the specified factory.
registers this expression with the specified factory.
returns true if the target is an ancestor of the source.
parse a list of arguments to to extract the expression elements. the input deque will be modified to remove the used elements.
register the expressions with the expression factory.
build the description used by the help command.
create a new set of find options.
returns the argument at the given position (starting from 1). argument to be returned if the argument doesn't exist or is null
returns the filestatus from the pathdata item. if the current options require links to be followed then the returned file status is that of the linked file. pathdata current depth in the process directories
shutdown the connection pool and close all open connections.
add the channel into pool.
set configuration from ui.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
a stream obtained via this call must be closed before using other apis of this class or else the invocation will block.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
connecting by using configuration parameters.
resolve against given working directory.
convert the file information in lsentry to a filestatus object.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of openingclosing a tcp connection.
creates a new scopedaclentries from the given list. it is assumed that the list is already sorted such that all access entries precede all default entries.
returns the pivot point in the list between the access entries and the default entries. this is the index of the first element in the list that is a default entry.
create an immutable permissionstatus object.
create and initialize a permissionstatus from datainput.
serialize a permissionstatus from its base components.
begin parsing permission stored in modestr
return true if this action implies that action.
get the effective permission for the aclentry.  recommended to use this api only if client communicates with the old namenode, needs to pass the permission for the path to get effective permission, else use aclstatus#geteffectivepermission(aclentry). communicating with old namenode, then this argument will not have any preference. namenode and permission is not passed as an argument.
create from unmasked mode and umask. if the mode is already an fscreatemodes object, return it.
create from masked and unmasked modes.
create a fspermission from a unix symbolic permission string
encode the object to a short.
apply a umask to this permission and return a new one. the umask is used by create, mkdir, and other hadoop filesystem operations. the mode argument for these operations is modified by removing the bits which are set in the umask. thus, the umask limits the permissions which newly created files and directories get.
create and initialize a fspermission from datainput.
get the user file creation mask (umask) or octal. symbolic umask is applied relative to file mode creation mask; the permission op characters '+' clears the corresponding bit in the mask, '-' sets bits in the mask. octal umask, the specified bits are set in the file mode creation mask.
convert a list of aclentries into a string - the reverse of parseaclspec.
parses a string representation of an acl spec into a list of aclentry objects. example: "user::rwx,user:foo:rw-,group::r--,other::---" the expected format of acl entries in the string parameter is the same format produced by the #tostringstable() method. string representation of an acl spec. for setacl operations this will be true. i.e. aclspec should include permissions. but for removeacl operation it will be false. i.e. aclspec should not contain permissions. example: "user:foo,group:bar"
returns a string representation guaranteed to be stable across versions to satisfy backward compatibility requirements, such as for shell command output or serialization. the format of this string representation matches what is expected by the #parseaclspec(string, boolean) and
parses a string representation of an acl into a aclentry object. the expected format of acl entries in the string parameter is the same format produced by the #tostringstable() method. string representation of an acl. example: "user:foo:rw-" for setacl operations this will be true. i.e. acl should include permissions. but for removeacl operation it will be false. i.e. acl should not contain permissions. example: "user:foo,group:bar,mask::"
sets the optional acl entry name.
apply permission against specified file and determine what the new mode would be
translates the given permission bits to the equivalent minimal acl. group and other permissions
given permissions and extended acl entries, returns the full logical acl.
record some set of values at the specified time into this helper. this can be useful to avoid fetching the current time twice if the caller has already done so for other purposes. this additionally allows the caller to specify a name for this recorder. when multiple names are used, one is denoted as the primary recorder. only recorders named as the primary will trigger logging; other names not matching the primary can only be triggered by following the primary. this is used to coordinate multiple logging points. a primary can be set via the is set in the constructor, then the first recorder name used becomes the primary. if multiple names are used, they maintain entirely different sets of values and summary information. for example:  initialize "pre" as the primary recorder name logthrottlinghelper helper = new logthrottlinghelper(1000, "pre"); logaction prelog = helper.record("pre", time.monotonicnow()); if (prelog.shouldlog()) ... double eventsprocessed = ... perform some action logaction postlog = helper.record("post", time.monotonicnow(), eventsprocessed); if (postlog.shouldlog()) ... can use postlog.getstats(0) to access eventsprocessed information  since "pre" is the primary recorder name, logging to "pre" will trigger a log action if enough time has elapsed. this will indicate that "post" should log as well. this ensures that "post" is always logged in the same iteration as "pre", yet each one is able to maintain its own summary information. other behavior is the same as #record(double...). current recorder is the primary. other names are arbitrary and are only used to differentiate between distinct recorders.
convert an event to json
build a json entry from the parameters. this is public for testing.
for use in tests
convert an event to json
send httphttps request to the daemon.
configures the client to send httphttps request to the url. supports spengo for authentication.
connect to the url. supports httphttps and supports spnego authentication. it falls back to simple authentication if it fails to initiate spnego.
registers an object as a handler for a given identity. note: will prevent handler from being gc'd, object should unregister itself when done such as org.apache.hadoop.blacklist
lookup the responsible handler and return its result. this should be called by the rpc server when it gets a refresh request.
create a decay scheduler. in a single instance.
given the number of occurrences, compute a scheduling decision.
update the schedulecache to match current conditions in callcounts.
returns the priority level for a given identity by first trying the cache, then computing it.
generate default thresholds if user did not specify. strategy is to halve each time, since queue usage tends to be exponential. so if numlevels is 4, we would generate: double[]0.125, 0.25, 0.5 which specifies the boundaries between each queue's usage.
decay the stored counts for each user and clean as necessary. this method should be called periodically in order to keep counts current.
get the number of occurrences and increment atomically.
add a new cache entry into the retry cache. the cache entry consists of clientid and callid extracted from editlog.
static method that provides null check for retrycache
constructor
this method handles the following conditions:  if retry is not to be processed, return null if there is no cache entry, add a new entry newentry and return it. if there is an existing entry, wait for its completion. if the completion state is cacheentry#failed, the expectation is that the thread that waited for completion, retries the request. the if the completion state is cacheentry#success, the entry is returned so that the thread that waits for it can can return previous response. 
static method that provides null check for retrycache
try to set up the response to indicate that the client version is incompatible with the server. this can contain special-case code to speak enough of past ipc protocols to pass back an exception to the caller.
this is a wrapper around readablebytechannel#read(bytebuffer). if the amount of data is large, it writes to channel in smaller chunks. this is to avoid jdk from creating many direct buffers as the size of bytebuffer increases. there should not be any performance degredation.
verify rpc header is valid
authorize proxy users to access this server
process an rpc request - the connection headers and context must have been already read. - based on the rpckind, decode the rpcrequest. - a successfully decoded rpccall will be deposited in rpc-q and its response will be sent later when the request is processed. such as invalid header or deserialization error. the call queue may also throw a fatal or non-fatal exception on overflow. be sent to client.
process a saslmessge. or sasl protocol mixup is not supported or trying to re-attempt negotiation.
send a deferred response, ignoring errors.
stops the service. no new calls will be handled after this is called.
setup response for the ipc call.
logs a slow rpc request. if this request took too much time relative to other requests we consider that as a slow rpc. 3 is a magic number that comes from 3 sigma deviation. a very simple explanation can be found by searching for 68-95-99.7 rule. we flag an rpc as slow rpc if and only if it falls above 99.7% of requests. we start this logic only once we have enough sample size.
this is a wrapper around writablebytechannel#write(bytebuffer). if the amount of data is large, it writes to channel in smaller chunks. this is to avoid jdk from creating many direct buffers as the size of buffer increases. this also minimizes extra copies in nio layer as a result of multiple write operations required to write a large buffer.
reads the connection context following the connection header deserialized, or the user is not authorized
register a rpc kind and the class to deserialize the rpc request. called by static initializers of rpckind engines the rpc request.
starts the service. must be called before any calls will be handled.
process a wrapped rpc request - unwrap the sasl packet and process each embedded rpc request
authorize the incoming client connection.
refresh the service authorization acl for the service handled by this server using the specified configuration.
process the sasl's negotiate request, including the optimization of accelerating token negotiation. authmethods and challenge if the tokens are supported.
decode the a protobuf from the given input stream
process one rpc request from buffer read from socket stream - decode rpc in a rpc-call - handle out-of-band rpc requests such as the initial connectioncontext - a successfully decoded rpccall will be deposited in rpc-q and its response will be sent later when the request is processed. prior to this call the connectionheader ("hrpc...") has been handled and if sasl then sasl has been established and the buf we are passed has been unwrapped from sasl. client, typically failure to respond to client
get the numopenconnectionsuser.
errorinfo
helper for #channelread(readablebytechannel, bytebuffer) and #channelwrite(writablebytechannel, bytebuffer). only one of readch or writech should be non-null.
allow a ipc response to be postponed instead of sent immediately after the handler returns from the proxy method. the intended use case is freeing up the handler thread when the response is known, but an expensive pre-condition must be satisfied before it's sent to the client.
get from config if client backoff is enabled on that port.
setup response for the ipc call on fatal error from a client that is using old version of hadoop. the response is serialized using the previous protocol's response layout.
return a new set containing all the exceptions in exceptionsset and exceptionclass.
process saslmessage and send saslresponse back failure, premature or invalid connection context, or other state errors. this exception needs to be sent to the client. this exception will wrap retriableexception,
constructs a server listening on the named port and address. parameters passed must be of the named class. the handlercount determines the number of handler threads that will be used to process calls. if queuesizeperhandler or numreaders are not -1 they will be used instead of parameters from configuration. otherwise the configuration will be picked up. if rpcrequestclass is null then the rpcrequestclass must have been registered via #registerprotocolengine(rpc.rpckind, class, rpc.rpcinvoker) this parameter has been retained for compatibility with existing tests and usage.
establish rpc connection setup by negotiating sasl if required, then reading and authorizing the connection header negotiation failure, premature or invalid connection context, or other state errors. this exception needs to be sent to the client.
some exceptions ( retriableexception and standbyexception) that are wrapped as a cause of parameter e are unwrapped so that they can be sent as the true cause to the client side. in case of
this method reads in a non-blocking fashion from the channel: this method is called repeatedly when data is present in the channel; when it has enough data to process one rpc it processes that rpc. on the first pass, it processes the connectionheader, connectioncontext (an outofband rpc) and at most one rpc request that follows that. on future passes it will process at most one rpc request. quirky things: datalengthbuffer (4 bytes) is used to read "hrpc" or rpc request length. client, typically failure to respond to client
stop the proxy. proxy must either implement closeable or must have associated rpcinvocationhandler. the rpc proxy object to be stopped if the proxy does not implement closeable interface or does not have closeable invocationhandler
get the protocol name. if the protocol class has a protocolannotation, then get the protocol name from the annotation; otherwise the class name is the protocol name.
return the connection id of the given object. if the provided object is in fact a protocol translator, we'll get the connection id of the underlying proxy object.
build the rpc server.
get all superinterfaces that extend versionedprotocol
get the protocol version from protocol class. if the protocol class has a protocolannotation, then get the protocol version from the annotation; otherwise get it from the versionid field of the protocol class.
get a meaningful and short name for a server based on a java class. the rules are defined to support the current naming schema of the generated protobuf classes where the final class usually an anonymous inner class of an inner class. 1. for simple classes it returns with the simple name of the classes (with the name without package name) 2. for inner classes, this is the simple name of the inner class. 3. if it is an object created from a class factory e.g., org.apache.hadoop.ipc.testrpc$testclass$2 this method returns parent class testclass. 4. if it is an anonymous class e.g., 'org.apache.hadoop.ipc.testrpc$10' servernamefromclass returns parent class testrpc.
get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server
calculate a method's hash code considering its method name, returning type, and its parameter types
get a server protocol's signature
return a protocol's signature and finger print from cache
get a server protocol's signature
convert an array of method into an array of hash codes
checks if queue is empty by checking at checkpoint_num points with checkpoint_interval_ms interval. this doesn't mean the queue might not fill up at some point later, but it should decrease the probability that we lose a call this way.
read the number of levels from the configuration. this will affect the faircallqueue's overall capacity.
replaces active queue with the newly requested one and transfers all calls to the newq before returning.
insert e into the backing queue or block until we can. if client backoff is enabled this method behaves like add which throws if the queue overflows. if we block and the queue changes on us, we will insert while the queue is drained.
retrieve an e from the backing queue or block until we can. guaranteed to return an element from the current queue.
check if a method is supported by the server or not
move to the next queue.
creates default weights for each queue. the weights are 2^n.
put the element in a queue of a specific priority.
returns maximum remaining capacity. this does not reflect how much you can ideally fit in this faircallqueue, as that would depend on the scheduler's decisions.
returns an element first non-empty queue equal to the priority returned by the multiplexer or scans from highest to lowest priority queue. caller must always acquire a semaphore permit before invoking. everything was empty
create a faircallqueue. notes: each sub-queue has a capacity of `capacity numsubqueues`. the first or the highest priority sub-queue has an excess capacity of `capacity % numsubqueues`
add, put, and offer follow the same pattern: 1. get the assigned prioritylevel from the call by scheduler 2. get the nth sub-queue matching this prioritylevel 3. delegate the call to this sub-queue. but differ in how they handle overflow: - add will move on to the next queue, throw on last queue overflow - put will move on to the next queue, block on last queue overflow - offer does not attempt other queues on overflow
offer the element to queue of the given or lower priority.
offer the element to queue of a specific priority.
drainto defers to each sub-queue. note that draining from a faircallqueue to another faircallqueue will likely fail, since the incoming calls may be scheduled differently in the new faircallqueue. nonetheless this method is provided for completeness.
peek, like poll, provides no strict consistency.
write the connection header - this is sent when connection is established +----------------------------------+ | "hrpc" 4 bytes | +----------------------------------+ | version (1 byte) | +----------------------------------+ | service class (1 byte) | +----------------------------------+ | authprotocol (1 byte) | +----------------------------------+
make a call, passing rpcrequest, to the ipc server defined by remoteid, returning the rpc response. indicate if a secure client falls back to simple auth s the rpc response throws exceptions if there are network problems or if the remote code threw an exception.
returns a connectionid object.
set call id and retry count for the next call.
the time after which a rpc will timeout. if ping is not enabled (via ipc.client.ping), then the timeout value is the same as the pinginterval. if ping is enabled, then there is no timeout value.
add a call to this connection's call queue and notify a listener; synchronized. returns false if called during shutdown.
initiates a rpc call by sending the rpc request to the remote server. note: this is not called from the connection thread, but by other threads.
cleanup executor on which ipc calls' parameters are sent. if reference counter is zero, this method discards the instance of the executor. if not, this method just decrements the internal reference counter. null is returned if not.
the time after which a rpc will timeout.
connect to the server and set up the io streams. it then sends a header to the server and starts the connection thread that waits for responses.
get executor on which ipc calls' parameters are sent. if the internal reference counter is zero, this method creates the instance of executor. if not, this method just returns the reference of clientexecutor.
check the rpc response header.
stop all threads related to this client. no further calls may be made using this client.
indicate when the call is complete and the value or error are available. notifies by default.
close the connection.
get a connection from the pool, or create a new one and add it to the pool. connections to a given connectionid are reused.
if multiple clients with the same principal try to connect to the same server at the same time, the server assumes a replay attack is in progress. this is a feature of kerberos. in order to work around this, what is done is that the client backs off randomly and tries to initiate the connection again. the other problem is to do with ticket expiry. to handle that, a relogin is attempted.
construct an ipc client whose values are of the given writable class.
update the server address if the address corresponding to the host name has changed.
convert an rpc method to a string. the format we want is 'methodouterclassshortname#methodname'. for example, if the method is: org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos. clientnamenodeprotocol.blockinginterface.getserverdefaults the format we want is: clientnamenodeprotocol#getserverdefaults
returns whether the given method is supported or not. the protocol signatures are fetched and cached. the connection id for the proxy provided is re-used.
convert an rpc class method to a string. the format we want is 'secondoutermostclassshortname#outermostclassshortname'. for example, if the full class name is: org.apache.hadoop.hdfs.protocol.clientprotocol.getblocklocations the format we want is: clientprotocol#getblocklocations
return the ioexception thrown by the remote server wrapped in serviceexception as cause. a new ioexception that wraps the unexpected serviceexception.
construct  cache an ipc client with the user-provided socketfactory if no cached client exists.
stop a rpc client connection a rpc client is closed only when its reference count becomes zero.
return clientid as byte[]
convert from clientid string byte[] representation of clientid
convert a clientid byte[] to string
construct a client-side proxy object that implements the named protocol, talking to a server at the named address.
construct an rpc server. can be null for compatibility with old usage (see below for details)
register the rpcrequest deserializer for writablerpcengine
this constructor takes a connectionid, instead of creating a new one.
this is the client side invoker of rpc method. it only throws serviceexception, since the invocation proxy expects only serviceexception to be thrown by the method in case protobuf service. serviceexception has the following causes:  exceptions encountered on the client side in this method are set as cause in serviceexception as is. exceptions from the server are wrapped in remoteexception and are set as cause in serviceexception  note that the client calling protobuf rpc methods, must handle serviceexception by getting the cause from the serviceexception. if the cause is remoteexception, then unwrap it to get the exception thrown by the server.
if this remote exception wraps up one of the lookuptypes then return this exception.  unwraps any ioexception.
process a get request for the specified resource. the servlet request we are processing the servlet response we are creating
disable trace method to avoid trace vulnerability.
initialize this servlet.
get the remote server's principal. the value will be obtained from the config and cross-checked against the server's advertised principal.
get sasl wrapped outputstream if sasl qop requires wrapping, otherwise return original stream. can be called only after saslconnect() has been called.
do client side sasl authentication with server via the given inputstream and outputstream inputstream to use outputstream to use
get sasl wrapped inputstream if sasl qop requires unwrapping, otherwise return original stream. can be called only after saslconnect() has been called.
instantiate a sasl client for the first supported auth type in the given list. the auth type must be defined, enabled, and the user must possess the required credentials, else the next auth is tried.
try to create a saslclient for an authentication type. may return null if the type isn't supported or the client lacks the required credentials.
release resources used by wrapped saslclient
evaluate the server provided challenge. the server must send a token if it's not done. if the server is done, the challenge token is optional because not all mechanisms send a final token for the client to update its internal state. the client must also be done after evaluating the optional token to ensure a malicious server doesn't prematurely end the negotiation with a phony success.
try to locate the required token for the server.
add group to cache
get netgroups for a given user
identify the sasl properties to be used for a connection with a client.
get the group memberships of a given user. if the user's group is not cached, this method may block.
get the groups being used to map user-to-groups.
create new groups used to map user-to-groups with loaded configuration.
this method will block if a cache entry doesn't exist, and any subsequent requests for the same user will wait on this request to return. if a user already exists in the cache, and when the key expires, the first call to reload the key will block, but subsequent requests will return the old value until the blocking thread returns. if reloadgroupsinbackground is true, then the thread that needs to refresh an expired key will not block either. instead it will return the old cache value and schedule a background refresh
override the reload method to provide an asynchronous implementation. if reloadgroupsinbackground is false, then this method defers to the super implementation, otherwise is arranges for the cache to be updated later
refresh all user-to-groups mappings.
queries impl for groups belonging to the user. this could involve io and take awhile.
add groups to cache
gets unix groups and netgroups for the user. it gets all unix groups as returned by id -gn but it only returns netgroups that are used in acls (there is no way to get all netgroups for a given user, see documentation for getent netgroup)
add a group to cache, only netgroups are cached
calls jni function to get users for a netgroup, since c functions are not reentrant we need to make this synchronized (see documentation for setnetgrent, getnetgrent and endnetgrent)
check if the executor had a timeout and logs the event.
get the current user's group list from unix by running the command 'groups' note. for non-existing user it will return empty list. group is returned first.
attempt to parse group names given that some names are not resolvable. use the group id list to identify those that are not resolved.
split group names into a linked list.
attempt to partially resolve group names.
returns the number of bytes that can be read from this input stream without blocking. the available method of inputstream returns 0. this method should be overridden by subclasses. blocking. @exception ioexception if an io error occurs.
reads the next byte of data from this input stream. the value byte is returned as an int in the range 0 to 255. if no byte is available because the end of the stream has been reached, the value -1 is returned. this method blocks until input data is available, the end of the stream is detected, or an exception is thrown.  is reached. @exception ioexception if an io error occurs.
constructs a saslinputstream from an inputstream and a saslserver  note: if the specified inputstream or saslserver is null, a nullpointerexception may be thrown later when they are used. the inputstream to be processed an initialized saslserver object
read more data and get them processed  entry condition: ostart = ofinish  exit condition: ostart <= ofinish  return (ofinish-ostart) (we have this many bytes for you), 0 (no data now, but could have more later), or -1 (absolutely no more data)
skips n bytes of input from the bytes that can be read from this input stream without blocking.  fewer bytes than requested might be skipped. the actual number of bytes skipped is equal to n or the result of a call to n is less than zero, no bytes are skipped.  the actual number of bytes skipped is returned. the number of bytes to be skipped. @exception ioexception if an io error occurs.
closes this input stream and releases any system resources associated with the stream.  the close method of saslinputstream calls the close method of its underlying input stream. @exception ioexception if an io error occurs.
reads up to len bytes of data from this input stream into an array of bytes. this method blocks until some input is available. if the first argument is null, up to len bytes are read and discarded. the buffer into which the data is read. the start offset of the data. the maximum number of bytes read. if there is no more data because the end of the stream has been reached. @exception ioexception if an io error occurs.
disposes of any system resources or security-sensitive information sasl might be using. @exception saslexception if a sasl error occurs.
constructs a saslinputstream from an inputstream and a saslclient  note: if the specified inputstream or saslclient is null, a nullpointerexception may be thrown later when they are used. the inputstream to be processed an initialized saslclient object
convenience method for reading a token storage file and loading its tokens.
convenience method for reading a token storage file and loading its tokens.
populates keysvalues from proto buffer storage.
stores all the keys to dataoutput.
write contents of this instance as credentialsproto message to dataoutput.
return all the secret key entries in the in-memory map.
add a token in the storage (in memory).
loads all the keys.
convenience method for reading a token from a datainputstream.
run the given action as the user, potentially throwing an exception.
create a proxy user ugi for testing hdfs and mapreduce the full user principal name for effective user ugi of the real user the names of the groups that the user belongs to
log a user in from a keytab file. loads a user identity from a keytab file and logs them in. they become the currently logged-in user.
log current ugi and token information into specified log.
initialize ugi and related classes.
get time for next login retry. this will allow the thread to retry with exponential back-off, until tgt endtime. last retry is #kerberosminsecondsbeforerelogin before endtime.
login a subject with the given parameters. if the subject is null, the login context used to create the subject will be attached.
create a usergroupinformation from a subject with kerberos principal. the creator of subject is responsible for renewing credentials.
returns the authentication method of a ugi. if the authentication method is proxy, returns the authentication method of the real user.
return the current user, including any doas in the current stack.
get the user's full principal name.
obtain the collection of tokens associated with this user.
get the group names for this user. fails, it returns an empty list.
create a ugi for testing hdfs and mapreduce
log the current user out who previously logged in using keytab. this method assumes that the user logged in by calling or if the user did not log in by invoking loginuserfromkeytab() before.
create a usergroupinformation for the given subject. this does not change the subject or acquire new credentials. the creator of subject is responsible for renewing credentials.
get realuser (vs. effectiveuser)
create a proxy user using username of the effective user and the ugi of the real user.
create a user from a login name. it is intended to be used for remote users in rpc, since it won't have any credentials.
create a usergroupinformation from a kerberos ticket cache. cache
log a user in from a keytab file. loads a user identity from a keytab file and login them in. this new user does not affect the currently logged-in user.
a test method to print out the current user's ugi. and print it out.
a method to initialize the fields that depend on a configuration. must be called before usekerberos or groups is used.
remove the login method that is followed by a space from the username e.g. "jack (auth:simple)" -> "jack"
compare the subjects to see if they are equal to each other.
return the username.
get the currently logged in user. if no explicit login has occurred, the user will automatically be logged in with either kerberos credentials if available, or as the local os user, based on security settings.
get the kerberos tgt
get the authentication method from the real user's subject. if there is no real user, return the given user's authentication method.
log all (current, real, login) ugi and token info into specified log.
obtain the tokens in credentials form associated with this user.
re-login a user in from the ticket cache. this method assumes that login had happened already. the subject field of this usergroupinformation object is updated to have the new credentials.
calls shell to get users for a netgroup by calling getent netgroup, this is a low level function that just returns string that
get unix groups (parent) and netgroups for given user
add a group to cache, only netgroups are cached
gets users for a netgroup
returns list of groups for a user. the ldapctx which underlies the dircontext object is not thread-safe, so we need to block around this whole method. the caching infrastructure will ensure that performance stays in an acceptable range.
perform ldap queries to get group names of a user. perform the first ldap query to get the user object using the user's name. if one-query is enabled, retrieve the group names from the user object. if one-query is disabled, or if it failed, perform the second query to get the groups. return an empty string array.
a helper method to get the relative distinguished name (rdn) from distinguished name (dn). according to active directory documentation, a group object's rdn is a cn.
perform the second query to get the groups of the user. if posixgroups is enabled, use use posix giduid to find. otherwise, use the general group member attribute to find it.
passwords should not be stored in configuration. use configuration, string, string) to avoid reading passwords from a configuration file.
look up groups using posixgroups semantics. use posix giduid to find groups of the user. semantics.
returns an instance of saslpropertiesresolver. looks up the configuration to see if there is custom class specified. constructs the instance by passing the configuration directly to the constructor to achieve thread safety using final fields.
a util function to retrieve specific additional sasl property from config. used by subclasses to read sasl properties used by themselves.
closes this output stream and releases any system resources associated with this stream. @exception ioexception if an io error occurs.
constructs a sasloutputstream from an outputstream and a saslclient  note: if the specified outputstream or saslclient is null, a nullpointerexception may be thrown later when they are used. the outputstream to be processed an initialized saslclient object
disposes of any system resources or security-sensitive information sasl might be using. @exception saslexception if a sasl error occurs.
constructs a sasloutputstream from an outputstream and a saslserver  note: if the specified outputstream or saslserver is null, a nullpointerexception may be thrown later when they are used. the outputstream to be processed an initialized saslserver object
writes len bytes from the specified byte array starting at offset off to this output stream. the data. the start offset in the data. the number of bytes to write. @exception ioexception if an io error occurs.
writes the specified byte to this output stream. the byte. @exception ioexception if an io error occurs.
resolves a host subject to the security requirements determined by hadoop.security.token.service.use_ip. optionally logs slow resolutions.
login as a principal specified in config. substitute $host in user's kerberos principal name with hostname. if non-secure mode - return. if no keytab available - bail out with an exception conf to use the key to look for keytab file in conf the key to look for user's kerberos principal name in conf hostname to use for substitution
look up the tokeninfo for a given protocol. it searches all known securityinfo providers.
perform the given action as the daemon's login user. if the login user cannot be determined, this will log a fatal error and exit the whole jvm.
look up the kerberosinfo for a given protocol. it searches all known securityinfo providers.
for use only by tests and initialization
utility method to fetch zk auth info from the configuration. cannot be read
set the given token's service to the format expected by the rpc client
tgs must have the server principal of the form "krbtgtfoo@foo".
create an inetaddress with a fully qualified hostname of the given hostname. inetaddress does not qualify an incomplete hostname that is resolved via the domain search list. hostname, but it always return the a record whereas the given hostname may be a cname.
convert kerberos principal name pattern to valid kerberos principal names. this method is similar to #getserverprincipal(string, string), except 1) the reverse dns lookup from addr to hostname is done only when necessary, 2) param addr can't be null (no default behavior of using local hostname when addr is null). kerberos principal name pattern to convert inetaddress of the host used for substitution
create the service name for a delegation token
retrieve the name of the current host. multihomed hosts may restrict the hostname lookup to a specific interface and nameserver with org.apache.hadoop.fs.commonconfigurationkeyspublic#hadoop_security_dns_interface_key and org.apache.hadoop.fs.commonconfigurationkeyspublic#hadoop_security_dns_nameserver_key
construct the service key for a token hadoop.security.token.service.use_ip
mangle given local java keystore file uri to allow use as a localjavakeystoreprovider. i.e. return of file.touri, e.g. file:homelarrycreds.jceks has an authority component.
convert a nested uri to decode the underlying path. the translation takes the authority and parses it into the underlying scheme and authority. for example, "myscheme:hdfs@nnmypath" is converted to "hdfs:nnmypath".
there are certain integrations of the credential provider api in which a recursive dependency between the provider and the hadoop filesystem abstraction causes a problem. these integration points need to leverage this utility method to remove problematic provider types from the existing provider path within the configuration.
the password is either found in the environment or in a file. this routine implements the logic for locating the password in these locations. contain the password. must not be null. can be null. nonexistent file or a file that fails to open and be read properly.
format and raise a failure.
execute diagnostics.  things it would be nice if ugi made accessible  a way to enable jaas debug programatically access to the tgt 
get the default realm.  not having a default realm may be harmless, so is noted at info. all other invocation failures are downgraded to warn, as follow-on actions may still work. failure to invoke the method via introspection is considered a failure, as it's a sign of jvm compatibility issues that may have other consequences
a cursory look at the kinit executable. if it is an absolute path: it must exist with a size > 0. if it is just a command, it has to be on the path. there's no check for that -but the path is printed out.
dump a ugi.
main entry point.
validate any jaas entry referenced in the #sun_security_jaas_file property.
inner entry point, with no logging or system exits.
dump a keytab: list all principals.
log in from a keytab, dump the ugi, validate it, then try and log in again. that second-time login catches jvmhadoop compatibility problems.
locate the krb5.conf file and dump it. no-op on windows.
dump any file to standard out.
print a line of output. this goes to any output file, or is logged at info. the output is flushed before and after, to try and stay in sync with jre logging.
flush all active output channels, including system.err, so as to stay in sync with any jre log messages.
try to load the sasl resolver.
dump all tokens of a ugi.
verify whether auth_to_local rules transform a principal name  having a local user name "bar@foo.com" may be harmless, so it is noted at info. however if what was intended is a transformation to "bar" it can be difficult to debug, hence this check.
validate that hadoop.token.files (if specified) exist and are valid.
identify the sasl properties to be used for a connection with a client.
set the static configuration to get and evaluate the rules.  important: this method does a nop if the rules have been set already. if there is a need to reset the rules, the kerberosname#setrules(string) method should be invoked directly.
get the list of users or groups returned by the specified command, and save them in the corresponding map.
initializes hadoop-auth authenticationfilter.  propagates to hadoop-auth authenticationfilter configuration all hadoop configuration properties prefixed with "hadoop.http.authentication."
returns list of groups for a user.
returns list of groups for a user. this calls ldapgroupsmapping's getgroups and applies the configured rules on group names before returning.
handles an httpinteraction by applying the filtering logic. and a servlet api call has failed
this method interrogates the user-agent string and returns whether it refers to a browser. if its not a browser, then the requirement for the csrf header will not be enforced; if it is a browser, the requirement will be enforced.  a user-agent string is considered to be a browser if it matches any of the regex patterns from browser-useragent-regex; the default behavior is to consider everything a browser that matches the following: "^mozilla.,^opera.". subclasses can optionally override this method to use different behavior.
authorize the user to access the protocol being used.
refreshes configuration using the specified proxy user prefix for properties.
authorize the superuser which is doing doas
returns an instance of impersonationprovider. looks up the configuration to see if there is custom class specified.
returns the access control list as a string that can be used for building a new instance by sending it to the constructor of accesscontrollist.
remove group from the names of groups allowed for this service. the group name
add group to the names of groups allowed for this service. the group name
add user to the names of users allowed for this service. the user name
remove user from the names of users allowed for this service. the user name
build acl from the given two strings. the strings contain comma separated values.
checks whether acl string contains wildcard
returns descriptive way of users and groups that are part of this acl. use #getaclstring() to get the exact string that can be given to the constructor of accesscontrollist to create a new instance.
checks if a user represented by the provided usergroupinformation is a member of the access control list
returns comma-separated concatenated single string of all strings of the given set
remove a token from a file in the local filesystem, matching alias.
fetch a token from a service and save to file in the local filesystem.
print out a credentials file from the local filesystem.
append tokens from list of files in local filesystem, saving to last file.
print out a credentials object.
renew a token from a file in the local filesystem, matching alias.
alias a token from a file and save back to file in the local filesystem.
match fetcher's service name to the service text andor url prefix.
write out a credentials object as a local file.
format a long integer type into a date string.
get the bytes for the token identifier
compute hmac of the identifier using the secret key and return the output as password
generate a new random secret key.
generate a string with the url-quoted base64 encoded serialized form of the writable.
clone a token.
default constructor.
modify the writable to the value from the newvalue.
construct a token given a token identifier and a secret manager for the type of the token identifier.
get the token identifier object, or null if it could not be constructed (because the class could not be loaded, for example).
construct a token from the components.
construct a token from a tokenproto.
never call this method directly.
given a renewer, add delegation tokens for issuer and it's child issuers to the credentials object if it is not already present.  note: this method is not intended to be overridden. issuers should implement getcanonicalservice and getdelegationtoken to ensure consistent token acquisition behavior.
parse the command line arguments and initialize subcommand. also will attempt to perform kerberos login if both -principal and -keytab flags are passed in args array.
parse arguments looking for kerberos keytabprincipal. if both are found: remove both from the argument list and attempt login. if only one of the two is found: remove it from argument list, log warning and do not attempt login. if neither is found: return original args array, doing nothing. return the pruned args array if either flag is present.
load the pathchildrencache into the in-memory map. possible caches to be loaded are keycache and tokencache.
add an entry to the jaas configuration with the passed in name, principal, and keytab. the other necessary options will be set for you. the name of the entry (e.g. "client") the principal of the user the location of the keytab
this method synchronizes the state of a delegation token information in local cache with its actual value in zookeeper.
get the username encoded in the token identifier
reset all data structures and mutable state.
should be called before this object is used
for subclasses externalizing the storage, for example zookeeper based implementations
update the current master key for generating delegation tokens it should be called only by tokenremoverthread.
remove expired delegation tokens from cache
find the delegationtokeninformation for the given token id, and verify that if the token is expired. note that this method should be called with acquiring the secret manager's monitor.
for subclasses externalizing the storage, for example zookeeper based implementations
verifies that the given identifier and password are valid and match.
cancel a token by removing it from cache.
renew a delegation token.
update the current master key this is called once by startthreads before tokenremoverthread is created, and only by tokenremoverthread afterwards.
add a previously used master key to cache (when nn restarts), should be called before activate().
this method is intended to be used for recovering persisted delegation tokens this method must be called before this secret manager is activated (before startthreads() is called)
extract a query string parameter without triggering http parameters processing by the servlet container. defined.
cancels a delegation token from the server end-point. it does not require being authenticated by the configured authenticator. are supported.
requests a delegation token using the configured authenticator for authentication. supported. delegation token will be stored.
renews a delegation token from the server end-point using the configured authenticator for authentication. supported.
this method checks if the given http request corresponds to a management operation. operation false otherwise
authenticates a request looking for the delegation query-string parameter and verifying it is a valid token. if there is not delegation query-string parameter, it delegates the authentication to the kerberosauthenticationhandler unless it is disabled.
returns the proxyuser configuration. all returned properties must start with proxyuser.'  subclasses may override this method if the proxyuser configuration is read from other place than the filter init parameters.
set auth_type property to the name of the corresponding authentication handler class based on the input properties.
this method is overridden to restrict http authentication schemes available for delegation token management functionality. the authentication schemes to be used for delegation token management are configured using delegation_token_schemes_property the basic logic here is to check if the current request is for delegation token management. if yes then check if the request contains an "authorization" header. if it is missing, then return the http 401 response with www-authenticate header for each scheme configured for delegation token management. it is also possible for a client to preemptively send authorization header for a scheme not configured for delegation token management. we detect this case and return the http 401 response with www-authenticate header for each scheme configured for delegation token management. if a client has sent a request with "authorization" header for a scheme configured for delegation token management, then it is forwarded to underlying multischemeauthenticationhandler for actual authentication. finally all other requests (excluding delegation token management) are forwarded to underlying multischemeauthenticationhandler for actual authentication.
returns an authenticated httpurlconnection. if the delegation token is present, it will be used taking precedence over the configured authenticator. if the doas parameter is not null, the request will be done on behalf of the specified doas user. as self.
select a delegation token from all tokens in credentials, based on url.
returns a configured sslserversocketfactory. be initialized. the server keystore.
returns a configured sslengine. be initialized. the server keystore.
if the given httpurlconnection is an httpsurlconnection configures the connection with the sslsocketfactory and
initializes the factory. happened. configuration.
creates an sslfactory. will be read.
returns the hostname verifier it should be used in httpsurlconnections.
returns a configured sslsocketfactory. be initialized. the server keystore.
extracts the array of subjectalt dns names from an x509certificate. returns null if there aren't any.  note: java doesn't appear able to extract international characters from the subjectalts. it can only extract international characters from the cn field.  (or maybe the version of openssl i'm using to test isn't storing the international characters correctly in the subjectalts?).
the javax.net.ssl.hostnameverifier contract.
counts the number of dots "." in a string.
starts the reloader thread.
creates a reloadable trustmanager. the trustmanager reloads itself if the underlying trustore file has changed. changed, in milliseconds. to an io error. initialized due to a security error.
releases any resources being used.
initializes the keystores of the factory. to an io error. initialized due to a security error.
parse the command line arguments and initialize the data.  % hadoop credential create alias [-provider providerpath] % hadoop credential list [-provider providerpath] % hadoop credential delete alias [-provider providerpath] [-f] 
open up and initialize the keystore. or a problem reading the keystore.
build a powershell script to kill a java.exe process in a remote machine. commandline.
attempt to use evil reflection tricks to determine the pid of a launched process. this is helpful to ops if debugging a fencing process that might have gone wrong. if running on a system or jvm where this doesn't work, it will simply return null.
set the environment of the subprocess to be the configuration, with '.'s replaced by '_'s.
abbreviate a string by putting '...' in the middle of it, in an attempt to keep logs from getting too messy.
add information about the target to the the environment of the subprocess.
verify that the argument, if given, in the conf is parseable.
execute a command through the ssh session, pumping its stderr and stdout to our own logs.
store the results of the last attempt to become active. this is used so that, during manually initiated failover, we can report back the results of the attempt to become active to the initiator of the failover.
ensure that the local node is in a healthy state, and thus eligible for graceful failover.
wait until one of the following events:  another thread publishes the results of an attempt to become active using #recordactiveattempt(activeattemptrecord) the node enters bad health status the specified timeout elapses  service becomes unhealthy
coordinate a graceful failover to this node.
ask the remote zkfc to cede its active status and wait for the specified timeout before attempting to claim leader status.
coordinate a graceful failover. this proceeds in several phases: 1) pre-flight checks: ensure that the local node is healthy, and thus a candidate for failover. 2a) determine the current active node. if it is the local node, no need to failover - return success. 2b) get the other nodes 3a) ask the other nodes to yield from election for a number of seconds 3b) ask the active node to yield from the election for a number of seconds. 4) allow the normal election path to run in other threads. wait until we either become unhealthy or we see an election attempt recorded by the normal code path. 5) allow the old active to rejoin the election, so a future failback is possible.
check the current state of the service, and join the election if it should be in the election.
in the cluster, or null if no node is active.
request from graceful failover to cede active role. causes this zkfc to transition its local node to standby, then quit the election for the specified period of time, after which it will rejoin iff it is healthy.
schedule a call to #recheckelectability() in the future.
perform pre-failover checks on the given service we plan to failover to, eg to prevent failing over to a service (eg due to it being inaccessible, already active, not healthy, etc). an option to ignore tosvc if it claims it is not ready to become active is provided in case performing a failover will allow it to become active, eg because it triggers a log roll so the standby can learn about new blocks and leave safemode.
failover from service 1 to service 2. if the failover fails then try to failback.
try to get the ha state of the node at the given address. this function is guaranteed to be "quick" -- ie it has a short timeout and no retries. its only purpose is to avoid fencing a node that has already restarted.
hook to allow subclasses to add any parameters they would like to expose to fencing implementationsscripts. fencing methods are free to use this map as they see fit -- notably, the shell script implementation takes each entry, prepends 'target_', substitutes '_' for '.', and adds it to the environment of the script. subclass implementations should be sure to delegate to the superclass implementation as well as adding their own keys.
interface implementation of zookeeper callback for monitor (exists)
get a new zookeeper client instance. protected so that test class can inherit and mock out the zookeeper instance
to participate in election, the app will call joinelection. the result will be notified by a callback on either the becomeactive or becomestandby app interfaces.  after this the elector will automatically monitor the leader status and perform re-election if necessary the app could potentially start off in standby mode and ignore the becomestandby call. to be set by the app. non-null data must be set. if valid data is not supplied
try to delete the "activebreadcrumb" node when gracefully giving up active status. if this fails, it will simply warn, since the graceful release behavior is only an optimization.
clear all of the state held within the parent znode. this recursively deletes everything within the znode as well as the parent znode itself. it should only be used when it's certain that no electors are currently participating in the election.
the callbacks and watchers pass a reference to the zk client which made the original call. we don't want to take action based on any callbacks from prior clients after we quit the election.
waits for the next event from zookeeper to arrive. be a zookeeper connectionloss exception code.
interface implementation of zookeeper watch events (connection and node), proxied by watcherwithclientref.
interface implementation of zookeeper callback for create
any service instance can drop out of the election by calling quitelection.  this will lose any leader status, if held, and stop monitoring of the lock node.  if the instance wants to participate in election again, then it needs to call joinelection().  this allows service instances to take themselves out of rotation for known impending unavailable states (e.g. long gc pause or software upgrade). if a failover occurs due to dropping out of the election.
if there is a breadcrumb node indicating that another node may need fencing, try to fence that node. if no breadcrumb node existed
create a new activestandbyelector object  the elector is created by providing to it the zookeeper configuration, the parent znode under which to create the znode and a reference to the callback interface.  the parent znode name must be the same for all service instances and different across services.  after the leader has been lost, a new leader will be elected after the session timeout expires. hence, the app must set this parameter based on its needs for failure response time. the session timeout must be greater than the zookeeper disconnect timeout and is recommended to be 3x that value to enable zookeeper to retry transient disconnections. setting a very short session timeout may result in frequent transitions between active and standby states during issues like network outagesgs pauses. zookeeper hostport for all zookeeper servers zookeeper session timeout znode under which to create the lock zookeeper acl's zk connection reference to callback interface object whether need to add the retry when establishing zk connection.
get data set by the active leader when there is no active leader other zookeeper operation errors when zookeeper connection could not be established
utility function to ensure that the configured base znode exists. this recursively creates the znode as well as all of its parents.
write the "activebreadcrumb" node, indicating that this node may need to be fenced on failover.
checks whether other target node is active or not occurred and forceactive was set otherwise false
ensure that we are allowed to manually manage the ha state of the target service. if automatic failover is configured, then the automatic failover controllers should be doing state management, and it is generally an error to use the haadmin command line to do so.
initiate a graceful failover by talking to the target node's zkfc. this sends an rpc to the zkfc, which coordinates the failover.
add cli options which are specific to the failover command and no others.
is a state transition valid? there are no checks for current==proposed as that is considered a non-transition. using an array kills off all branch misprediction costs, at the expense of cache line misses.
check that a state tansition is valid and throw an exception if not
verify that that a service is in a given state. the desired state
return the state text as the tostring() value
callback for a state change event: log it
log events to the given log
stop the services in reverse order started, not those that are notinited or inited. stop process -after all services are stopped
get a cloned list of services added services will not be picked up.
add the passed service to the list of services managed by this
this action
this invokes #serviceinit the state change not permitted, or something else went wrong
all initialization code needed by a service. this method will only ever be called once during the lifecycle of a specific service instance. implementations do not need to be synchronized as the logic in #init(configuration) prevents re-entrancy. the base implementation checks to see if the subclass has created a new configuration instance, and if so, updates the base class value possibly wrapped, and wil; trigger a service stop
notify local and global listeners of state changes. exceptions raised by listeners are not passed up.
failure handling: record the exception that triggered it -if there was not one already. services are free to call this themselves.
add a state change event to the lifecycle history
enter a state; record this via #recordlifecycleevent and log at the info level. it wasn't already in that state, and the state model permits state re-entrancy.
put a blocker to the blocker map -replacing any with the same name.
remove a blocker from the blocker map - this is a no-op if the blocker is not present
change to a new state and notify all listeners. this method will block until all notifications have been issued. it caches the list of listeners before the notification begins, so additions or removal of listeners will not be visible.
thread-safe addition of a new listener to the end of a list. attempts to re-register a listener that is already registered will be ignored.
stop a service; if it is null do nothing. exceptions are caught and logged at warn level. (but not throwables). this operation is intended to be used in cleanup operations
stop a service; if it is null do nothing. exceptions are caught and logged at warn level. (but not throwables). this operation is intended to be used in cleanup operations
stop a service. do nothing if the service is null or not in a state in which it can beneeds to be stopped.  the service state is checked before the operation begins. this process is not thread safe.
convert any exception into a runtimeexception. all other exception types are wrapped in a new instance of
convert any exception into a runtimeexception. if the caught exception is already of that type, it is typecast to a all other exception types are wrapped in a new instance of
bind to the interrupt handler.
handler for the jvm api for signal handling.
uncaught exception handler. if an error is raised: shutdown the state of the system is unknown at this point -attempting a clean shutdown is dangerous. instead: exit
create an instance of the launcher.
print a warning message.  this tries to log to the log's warn() operation. if the log at that level is disabled it logs to system error
build a log message for starting up and shutting down.
handler for uncaught exceptions: terminate the service.
get the usage message, ideally dynamically.
convert an exception to an exitexception. this process may just be a simple pass through, otherwise a new exception is created with an exit code, the text of the supplied exception, and the supplied exception as an inner cause.  if is already the right type, pass it through. if it implements exitcodeprovider#getexitcode(), the exit code is extracted and used in the new exception. otherwise, the exit code 
launch the service. all exceptions that occur are propagated upwards. if the method returns a status code, it means that it got as far starting the service, and if it implements launchableservice, that the method launchableservice#execute() has completed. after this method returns, the service can be retrieved returned by have been stripped out. this service on shutdown. tests should set this to false. containing exception
override point: create an options instance to combine with the standard options set. important. synchronize uses of optionbuilder with optionbuilder.class
get the service name via service#getname(). if the service is not instantiated, the classname is returned instead.
report an error.  this tries to log to log.error().  if that log level is disabled disabled the message is logged to system error along with thrown.tostring()
verify that all the specified filenames exist.
launch a service catching all exceptions and downgrading them to exit codes after logging. sets #serviceexception to this value. have been stripped out. this service on shutdown. tests should set this to false.
extract the command options and apply them to the configuration, building an array of processed arguments to hand down to the service. the service classname and is skipped.
override point: register this class as the handler for the control-c and sigint interrupts. subclasses can extend this with extra operations, such as an exception handler:  thread.setdefaultuncaughtexceptionhandler( new yarnuncaughtexceptionhandler()); 
record that an exit exception has been raised. save it to #serviceexception, with its exit code in
launch the service and exit.  parse the command line. build the service configuration from it. start the service.. if it is a launchableservice: execute it otherwise: wait for it to finish. exit passing the status code to the #exit(int, string) method.  assumed to be the service classname.
instantiate the service defined in serviceclassname. sets the configuration field to the the value of conf, and the service field to the service created.
this creates all the configurations defined by the resources have been pushed in. if one cannot be loaded it is logged and the operation continues except in the case that the class does load but it isn't actually a subclass of configuration.
parse the command arguments, extracting the service class as the last element of the list (after extracting all the rest). the field #commandoptions field must already have been set.
unregister the hook.
shutdown operation.  subclasses may extend it, but it is primarily made available for testing.
 the base implementation logs all arguments at the debug level, then returns the passed in config unchanged.
look up the handler for a signal.
register an interrupt handler.
shutdown callback: stop the service and set an atomic boolean if it stopped within the shutdown time.
this method makes the change to this objects configuration and calls reconfigurepropertyimpl to update internal data structures. this method cannot be overridden, subclasses should instead override reconfigurepropertyimpl.
start a reconfiguration task to reload configuration in background.
using bigdecimal so we can throw if we are overflowing the long.max.
using bigdecimal to avoid issues with overflow and underflow.
apply configuratio changes after admin has approved them.
print configuration options that can be changed.
matches given config key against patterns and determines whether or not it should be considered sensitive enough to redact in logs and other plaintext displays.
get the set of parameters marked final.
fallback to clear text passwords in configuration.
get all properties belonging to tag.
get the credential entry by name from a credential provider. handle key deprecation.
set the socket address a client can use to connect for the name property as a host:port. the wildcard address is replaced with the local host's address. if the host and address properties are configured the host component of the address will be combined with the port component of the addr to generate the address. this is to allow optional control over which host name is used in multi-home bind-host cases where a host can have multiple names
writes out all properties and their attributes (final and resource) to the given writer, the format of the output would be,  "properties" : [ key : "key1", value : "value1", isfinal : "key1.isfinal", resource : "key1.resource" , key : "key2", value : "value2", isfinal : "ke2.isfinal", resource : "key2.resource" ]  it does not output the properties of the configuration object which is loaded from an input stream. 
get the value of the name property as a class implementing the interface specified by xface. if no such property is specified, then defaultvalue is returned. an exception is thrown if the returned class does not implement the named interface. or defaultvalue.
get the value of the name property as an array of class. the value of the property specifies a list of comma separated class names. if no such property is specified, then defaultvalue is returned. or defaultvalue.
get an iterator to go through the list of string key-value pairs in the configuration.
return time duration in the given time unit. valid units are encoded in properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds (ms), seconds (s), minutes (m), hours (h), and days (d).
gets information about why a property was set. typically this is the path to the resource objects (file, url, etc.) the property came from, but it can also indicate that it was set programmatically, or because of the command line. returns a list of the sources of the resource. the older sources are the first ones in the list. so for example if a configuration is set from the command line, and then written out to a file that is read back in the first entry would indicate that it was set from the command line, while the second one would indicate the file that the new configuration was read in from.
adds a set of deprecated keys to the global deprecations. this method is lockless. it works by means of creating a new deprecationcontext based on the old one, and then atomically swapping in the new context. if someone else updated the context in between us reading the old context and swapping in the new one, we try again until we win the race.
set the value of the name property to the name of a theclass implementing the given interface xface. an exception is thrown if theclass does not implement the interface xface.
get an input stream attached to the configuration resource with the given name.
reload existing configuration instances.
a new configuration with the same settings cloned from another.
get the value of the name property as a set of comma-delimited int values. if no such property exists, an empty array is returned. int values
get keys matching the the regex
get the value of the name property as a pattern. if no such property is specified, or if the specified value is not a valid pattern, then defaultvalue is returned. note that the returned value is not trimmed by this method.
append a property with its attributes to a given #link document if the property is found in configuration.
sets all deprecated properties that are not currently set but have a corresponding new property that is set. useful for iterating the properties when all deprecated properties for currently set properties need to be present.
try and resolve the provided element name as a credential provider alias.
get all properties belonging to list of input tags. calls getallpropertiesbytag internally.
convert the value from one storage unit to another.
get the socket address for hostproperty as a inetsocketaddress. if hostproperty is null, addressproperty will be used. this is useful for cases where we want to differentiate between host bind address and address clients should use to establish connection.
get the value of the name property as a trimmed string, null if no such property exists. if the key is deprecated, it returns the value of the first key which replaces the deprecated key and is not null values are processed for  href="#variableexpansion">variable expansion before being returned. or null if no such property exists.
get a local file under a directory named by dirsprop with the given path. if dirsprop contains multiple directories, then one is chosen based on path's hash code. if the selected directory does not exist, an attempt is made to create it.
write property and its attributes as json format to given
set the value of the name property. if name is deprecated, it also sets the value to the keys that replace the deprecated key. name will be trimmed before put into configuration. (for debugging).
read the values passed as tags and store them in a map for later retrieval.
add a default resource. resources are loaded in the order of the resources added.
get a local file name under a directory named in dirsprop with the given path. if dirsprop contains multiple directories, then one is chosen based on path's hash code. if the selected directory does not exist, an attempt is made to create it.
writes properties and their attributes (final and resource) to the given writer.  when propertyname is not empty, and the property exists in the configuration, the format of the output would be,  "property": "key" : "key1", "value" : "value1", "isfinal" : "key1.isfinal", "resource" : "key1.resource"    when propertyname is null or empty, it behaves same as output would be,  "properties" : [ key : "key1", value : "value1", isfinal : "key1.isfinal", resource : "key1.resource" , key : "key2", value : "value2", isfinal : "ke2.isfinal", resource : "key2.resource" ]    when propertyname is not empty, and the property is not found in the configuration, this method will throw an   empty and the property is not found in configuration
print a warning if a property with a given name already exists with a different value
returns alternative names (non-deprecated keys or previously-set deprecated keys) for a given non-deprecated key. if the given key is deprecated, return null.
this is a manual implementation of the following regex "\\$\\[^\\\\$\u0020]+\\". it can be 15x more efficient than a regex matcher as demonstrated by hadoop-11506. this is noticeable with hadoop apps building on the assumption configuration#get is an o(1) hash table lookup, especially when the eval is a long string. eval.substring(res[0], res[1]) is "var" for the left-most occurrence of $var in eval. if no variable is found -1, -1 is returned.
get the comma delimited values of the name property as a collection of strings, trimmed of the leading and trailing whitespace. if no such property is specified then empty collection is returned.
get range start for the first integer range.
create a new deprecationcontext by copying a previous deprecationcontext and adding some deltas. from nothing.
get a reader attached to the configuration resource with the given name.
attempts to repeatedly expand the value expr by replacing the left-most substring of the form "$var" in the following precedence order  by the value of the environment variable "var" if defined by the value of the java system property "var" if defined by the value of the configuration key "var" if defined  if var is unbounded the current state of expansion "prefix$varsuffix" is returned.  this function also detects self-referential substitutions, i.e.  foo.bar = $foo.bar  if a cycle is detected then the original expr is returned. loops involving multiple substitutions are not detected. expr using the algorithm above.
return the xml dom corresponding to this configuration.
constructs a mapping of configuration and includes all properties that start with the specified configuration prefix. property names in the mapping are trimmed to remove the configuration prefix.
get the value of the name property as a list of objects implementing the interface specified by xface. an exception is thrown if any of the classes does not exist, or if it does not implement the named interface. name.
convert a string to an int treating empty strings as the default value.
unset a previously set property.
load a class by name.
get the value of the name property as a class. if no such property is specified, then defaultvalue is returned. or defaultvalue.
add tags defined in hadoop_tags_system, hadoop_tags_custom.
adds the deprecated key to the global deprecation map. it does not override any existing entries in the deprecation map. this is to be used only by the developers in order to add deprecation of keys, and attempts to call this method after loading resources once, would lead to unsupportedoperationexception if a key is deprecated in favor of multiple keys, they are all treated as aliases of each other, and setting any one of them resets all the others to the new value. if you have multiple deprecation entries to add, it is more efficient to use #adddeprecations(deprecationdelta[] deltas) instead. string custommessage) instead
gets storage size from a config file. not present. for the return value.
returns whether or not a deprecated name has been warned. if the name is not deprecated then always return false
get the value of the name property as a boolean. if no such property is specified, or if the specified value is not a valid boolean, then defaultvalue is returned. or defaultvalue.
method to provide the warning message. it gives the custom message if non-null, and default message otherwise.
checks for the presence of the property name in the deprecation map. returns the first of the list of new keys if present in the deprecation map or the name itself. if the property is not presently set but the property map contains an entry for the deprecated key, the value of the deprecated key is set as the value for the provided property name. the name or the name itself.
write out the non-default properties in this configuration to the given writer.  when property name is not empty and the property exists in the configuration, this method writes the property and its attributes to the writer.    when property name is null or empty, this method writes all the configuration properties and their attributes to the writer.    when property name is not empty but the property doesn't exist in the configuration, this method throws an illegalargumentexception.  
a new configuration where the behavior of reading from the default resources can be turned off. if the parameter loaddefaults is false, the new instance will not load resources from the default files.
load a class by name, returning null rather than throwing an exception if it couldn't be loaded. this is to avoid the overhead of creating an exception.
guts of the servlet - extracted for easy testing.
set the configuration and extract the configuration parameters of interest
build and execute the resolution command. the command is executed in the directory specified by the system property "user.dir" if set; otherwise the current working directory is used or the output of the command.
caches the resolved host:rack mappings. the two list parameters must be of equal size. at index(i) is the resolved value for the entry in uncachedhosts[i]
get the (host x switch) map.
or null if any of the names are not currently in the cache
return a free port number. there is no guarantee it will remain free, so it should be used immediately. s a free port for binding a local socket
using cidr notation, false otherwise
adds a static resolution for host. this can be used for setting up hostnames with names that are fake to point to a well known host. for e.g. in some testcases we require to have daemons with different hostnames running on the same machine. in order to create connections to these daemons, one can set up mappings from those hostnames to "localhost". the actual hostname.
return a socketinputwrapper for the socket and set the given timeout. if the socket does not have an associated channel, then its socket timeout will be set to the specified value. otherwise, a timeout. any socket created using socket factories returned by #netutils, must use this interface instead of socket#getinputstream(). in general, this should be called only once on each socket: see the note in socketinputwrapper#settimeout(long) for more information. long as necessary.
returns an inetsocketaddress that a client can use to connect to the given listening address.
like netutils#connect(socket, socketaddress, int) but also takes a local address and port to bind the socket to.
return an inetaddress for each interface that matches the given subnet specified using cidr notation. whether to return ips associated with subinterfaces
create a socket address with the given host and port. the hostname might be replaced with another host that was set via hadoop.security.token.service.use_ip will determine whether the standard java host resolver is used, or if the fully qualified resolver is used.
retrieves the resolved name for the passed host. the resolved name must have been set earlier using
take an ioexception , the local host port and remote host port details and return an ioexception with the input exception as the cause and also include the host details. the new exception provides the stack trace of the place where the exception is thrown and some extra diagnostics information. if the exception is of type bindexception, connectexception, unknownhostexception, sockettimeoutexception or has a string constructor, return a new one of the same type; otherwise return an ioexception.
get the default socket factory as specified by the configuration parameter hadoop.rpc.socket.factory.default the jvm default socket factory if the configuration does not contain a default socket factory property.
resolve the uri's hostname and add the default port if not in the uri
attempt to obtain the host name of the given string which contains an ip address and an optional port.
given an inetaddress, checks to see if the address is a local address, by comparing the address with all the interfaces on the node.
get the socket factory corresponding to the given proxy uri. if the given proxy uri corresponds to an absence of configuration parameter, returns null. if the uri is malformed raises an exception. socketfactory to instantiate; assumed non null and non empty.
create an inetsocketaddress from the given target string and default port. if the string cannot be parsed correctly, the configname parameter is used as part of the exception message, allowing the user to better diagnose the misconfiguration. include a port number target was loaded. this is used in the exception message in the case that parsing fails.
returns outputstream for the socket. if the socket has an associated socketchannel then it returns a have a channel, socket#getoutputstream() is returned. in the later case, the timeout argument is ignored and the write will wait until data is available. any socket created using socket factories returned by netutils, must use this interface instead of socket#getoutputstream(). for waiting as long as necessary.
this is used to get all the resolutions that were added using value is a list each element of which contains an array of string of the form string[0]=hostname, string[1]=resolved-hostname
get the socket factory for the given class according to its configuration parameter hadoop.rpc.socket.factory.class.<classname>. when no such parameter exists then fall back on the default socket factory as configured by hadoop.rpc.socket.factory.class.default. if this default socket factory is not configured, then fall back on the jvm default socket factory.
add all addresses associated with the given nif in the given subnet to the given list.
given a collection of string representation of hosts, return a list of corresponding ip addresses in the textual representation.
get the host details as a string
performs a sanity check on the list of hostnamesips to verify they at least appear to be valid.
normalize a path by stripping off any trailing #path_separator if pathis null or empty #root is returned is not #path_separator
construct a node from its path a concatenation of this node's location, the path separator, and its name
set this node's name and location
sort nodes array by their distances to reader.  this is the same as networktopology#sortbydistance(node, node[], int) except with a four-level network topology which contains the additional network distance of a "node group" which is between local and same rack.
remove a node update node counter and rack counter if necessary
add a leaf node update node counter  rack counter if necessary @exception illegalargumentexception if add a node to a leave or node to be added is not a leaf
given a string representation of a node group for a specific network location a path-like string representation of a network location
set the timeout for reads from this stream. note: the behavior here can differ subtly depending on whether the underlying socket has an associated channel. in particular, if there is no channel, then this call will affect the socket timeout for all readers of this socket. if there is a channel, then this call will affect the timeout only for this stream. as such, it is recommended to only create one socketinputwrapper instance per socket. the new timeout, 0 for no timeout if the timeout cannot be set
set the proxy of this socket factory as described in the string parameter
convert a network tree to a string.
return the number of leaves in scope but not in excludednodes if scope starts with ~, return the number of nodes that are not in scope and excludednodes;
check if two nodes are on the same rack @exception illegalargumentexception when either node1 or node2 is null, or node1 or node2 do not belong to the cluster
sort nodes array by network distance to reader.  as an additional twist, we also randomize the nodes at each network distance. this helps with load balancing when there is data skew.
returns an integer weight which specifies how far away node is from reader. a lower value signifies that a node is closer. it uses network location to calculate the weight
given a string representation of a node, return its reference a path-like string representation of a node
check if the tree contains node node
randomly choose one node from scope. if scope starts with ~, choose one from the all nodes except for the ones in scope; otherwise, choose one from scope. if excludednodes is given, choose a node that's not in excludednodes.
returns an integer weight which specifies how far away node is away from reader. a lower value signifies that a node is closer.
add a leaf node update node counter  rack counter if necessary @exception illegalargumentexception if add a node to a leave or node to be added is not a leaf
return the distance between two nodes it is assumed that the distance from one node to its parent is 1 the distance between two nodes is calculated by summing up their distances to their closest common ancestor. or integer#max_value if node1 or node2 do not belong to the cluster
given a string representation of a rack, return its children
remove a node update node counter and rack counter if necessary
return the distance between two nodes by comparing their network paths without checking if they belong to the same ancestor node by reference. it is assumed that the distance from one node to its parent is 1 the distance between two nodes is calculated by summing up their distances to their closest common ancestor.
normalize a path by stripping off any trailing #path_separator. if pathis null or empty #root is returned is not #path_separator
randomly choose one node under parentnode, considering the exclude nodes and scope. should be called with #netlock's readlock held. the excludedscopenode could be chosen, excluding excludednodes
return leaves in scope
takes one selector from end of lru list of free selectors. if there are no selectors awailable, it creates a new selector. also invokes trimidleselectors().
performs one io and returns number of bytes read or written. it waits up to the specified timeout. if the channel is not read before the timeout, sockettimeoutexception is thrown. selectionkey.op_read while reading and selectionkey.op_write while writing.
closes selectors that are idle for idle_timeout (10 sec). it does not traverse the whole list, just over the one that have crossed the timeout.
the contract is similar to socketchannel#connect(socketaddress) with a timeout.
this is similar to #doio(bytebuffer, int) except that it does not perform any io. it just waits for the channel to be ready for io as specified in ops. if select on the channel times out. if any other io error occurs.
utility function to check if channel is ok. mainly to throw ioexception instead of runtime exception in case of mismatch. this mismatch can occur for many runtime reasons.
waits on the channel with the given timeout using one of the cached selectors. it also removes any cached selectors that are idle for a few seconds.
create a new ouput stream with the given timeout. if the timeout is zero, it will be treated as infinite timeout. the socket's channel will be configured to be non-blocking. channel for writing, should also be a selectablechannel. the channel will be configured to be non-blocking.
transfers data from filechannel using updates waitforwritabletime and transfertotime with the time spent blocked on the network and the time spent transferring data from disk to network respectively. similar to readfully(), this waits till requested amount of data is transfered. to become writable if end of input file is reached before requested number of bytes are transfered. if this channel blocks transfer longer than timeout for this stream.
generate a string listing the switch mapping implementation, the mapping for every known node and the number of nodes and unique switches known about -each entry to a separate line. debug messages.
create a new input stream with the given timeout. if the timeout is zero, it will be treated as infinite timeout. the socket's channel will be configured to be non-blocking. channel for reading, should also be a selectablechannel. the channel will be configured to be non-blocking.
get dependencies in the topology for a given host
set the configuration and extract the configuration parameters of interest
returns the hostname associated with the specified ip address by the provided nameserver. loopback addresses
returns all the host names associated by the provided nameserver with the address bound to the specified network interface the name of the network interface or subinterface to query (e.g. eth0 or eth0:0) the dns host name if true and if reverse dns resolution fails then attempt to resolve the hostname with hosts file resolution. the specified interface
get the ipaddress of the local host as a string. this will be a loop back value if the local host address cannot be determined. if the loopback address of "localhost" does not resolve, then the system's network is in such a state that nothing is going to work. a message is logged at the error level and a null pointer returned, a pointer which will trigger failures later on the application
or null if no interface with the given name can be found
determine the local hostname; retrieving it from cache if it is known if we cannot determine our host name, return "localhost"
returns all the ips associated with the provided interface, if any, as a list of inetaddress objects. the name of the network interface or sub-interface to query (eg eth0 or eth0:0) or the string "default" whether to return ips associated with subinterfaces of the given interface interface. the local host ip is returned if the interface name "default" is specified or there is an io error looking for the given interface. if the given interface is invalid
returns all the ips associated with the provided interface, if any, in textual form. the name of the network interface or sub-interface to query (eg eth0 or eth0:0) or the string "default" whether to return ips associated with subinterfaces of the given interface interface. the local host ip is returned if the interface name "default" is specified or there is an io error looking for the given interface. if the given interface is invalid
see below for the rationale for using an ordered set
judge if this node represents a rack
send callback and return whether or not the domain socket was closed as a result of processing.
close the domainsocketwatcher and wait for its thread to terminate. if there is more than one close, all but the first will be ignored.
remove a socket. its handler will be called.
add a socket. we are already watching. called any time after this function is called.
send callback, and if the domain socket was closed as a result of processing, then also remove the entry for the file descriptor.
wake up the domainsocketwatcher thread.
create a pair of unix domain sockets which are connected to each other by calling socketpair(2). each other.
call shutdown(shut_rdwr) on the unix domain socket.
accept a new unix domain connection. this method can only be used on sockets that were bound with bind(). such as the socket being closed from under us. particularly when the accept is timed out, it throws sockettimeoutexception.
send some filedescriptor objects to the process on the other side of this socket. one byte.
create a new domainsocket connected to the given path.
close the socket.
receive some filedescriptor objects from the process on the other side of this socket, and wrap them in fileinputstream objects.
create a new domainsocket listening on the given path.
add any key,val pair to the string, between the prefix and suffix, separated by the separator.
throw a metricsexception if the given property is not set.
if the sink isn't set to ignore errors, throw a metricsexception if the stream encountered an exception. the message parameter will be used as the new exception's message with the current file name ( #currentfilepath) appended to it. the current file name ( #currentfilepath) appended to it. ignoring errors
return the property value if it's non-negative and throw an exception if it's not.
initialize the connection to hdfs and create the base directory. also launch the flush thread.
use the given time to determine the current directory. the current directory will be based on the #rollintervalminutes.
return the next id suffix to use when creating the log file. this method will look at the files in the directory, find the one with the highest id suffix, and 1 to that suffix, and return it. this approach saves a full linear probe, which matters in the case where there are a large number of log files. directory
extract the id from the suffix of the given file name.
turn a security property into a nicely formatted set of name=value strings, allowing for either the property or the configuration not to be set.
return the supplied file system for testing or otherwise get a new file system.
schedule the current interval's directory to be flushed. if this ends up running after the top of the next interval, it will execute immediately.
create a new directory based on the current interval and a new log file in that directory. new directory or new log file
check the current directory against the time stamp. if they're not the same, create a new directory and a new log file in that directory. new directory or new log file
test whether the file system supports append and return the answer.
update the #nextflush variable to the next flush time. add an integer number of flush intervals, preserving the initial random offset.
if the sink isn't set to ignore errors, wrap the throwable in a as the new exception's message with the current file name ( #currentfilepath) and the throwable's string representation appended to it. current file name ( #currentfilepath), and the throwable's string representation (wrapped in square brackets) appended to it.
return the supplied configuration for testing or otherwise load a new configuration.
set the #nextflush variable to the initial flush time. the initial flush will be an integer number of flush intervals past the beginning of the current hour and will have a random offset added, up to past that can be used from which to calculate future flush times.
if the sink isn't set to ignore errors, throw a new new exception's message with the current file name ( #currentfilepath) appended to it. the current file name ( #currentfilepath) appended to it.
create a new log file and return the fsdataoutputstream. if a file with the specified path already exists, add a suffix, starting with 1 and try again. keep incrementing the suffix until a nonexistent target path is found. once the file is open, update #currentfsoutstream, appropriately.
create a new log file and return the fsdataoutputstream. if a file with the specified path already exists, open the file for append instead. once the file is open, update #currentfsoutstream,
extract the roll interval from the configuration and return it in milliseconds.
sends ganglia metrics to the configured hosts
lookup gangliaconf from cache. if not found, return default values
puts a string into the buffer by first writing the size of the string as an int, followed by the bytes of the string, padded if necessary to a multiple of 4.
the method sends metrics to ganglia servers. the method has been taken from org.apache.hadoop.metrics.ganglia.gangliacontext31 with minimal changes in order to keep it in sync.
the method sends metrics to ganglia servers. the method has been taken from org.apache.hadoop.metrics.ganglia.gangliacontext30 with minimal changes in order to keep it in sync.
publish a metrics snapshot to all the sinks instead of using a separate thread.
requests an immediate publish of all metrics from sources to sinks.
initialized the metrics system with a prefix.
sample all the sources for a snapshot of metricstags
construct the metrics system
consume all the elements, will block if queue is empty
consume one element, will block if queue is empty only one consumer at a time is allowed
load configuration from a list of files until the first successful load
return sub configs for instance specified in the config. assuming format specified as follows: [type].[instance].[option] = [value] note, '' is a special default instance, which is excluded in the result.
will poke parents for defaults
add a snapshot to the metric
aggregates the thread's local samples into the global metrics. the caller should ensure its thread safety.
add a rate sample for a rate metric.
initialize the registry with all the methods in a protocol so they all show up in the first snapshot. convenient for jmx implementations.
collects states maintained in threadlocal, if any.
initialize the registry with all the methods in a protocol so they all show up in the first snapshot. convenient for jmx implementations.
create a mutable long integer gauge
create a mutable integer gauge
create a mutable metric with stats
add sample to a stat metric by name.
sample all the mutable metrics and put the snapshot in the builder
create a mutable integer counter
create a mutable float gauge
add a tag to the metrics
create a mutable metric that estimates quantiles of a stream of values
create a mutable long integer counter
remove the prefix "get", if any, from the method name. return the capacitalized method name."
instantiates a new mutablequantiles for a metric that rolls itself over on the specified time interval. of the metric long-form textual description of the metric type of items in the stream (e.g., "ops") type of the values rollover interval (in seconds) of the estimator
add mutablemetric for a method annotated with metric
change the declared field field in source object to
this method is for testing only to replace the scheduledtask.
retrieve a map of metric name -> (aggregate). filter out entries that don't have at least minsamples. node seen over the measurement period.
constructor for mutablerollingaverages.
iterates over snapshot to capture all avg metrics into rolling structure
parses a space andor comma separated sequence of server specifications of the form hostname or hostname:port. if the specs string is null, defaults to localhost:defaultport.
update the cache and return the current cached record
get the cached record
register the mbean using our standard mbeanname format "hadoop:service=,name=" where the  and  are the supplied parameters.
register the mbean using our standard mbeanname format "hadoop:service=,name=" where the  and  are the supplied parameters. properties.
check an argument for false conditions
check an argument for false conditions
check an argument for false conditions
check an argument for false conditions
check an argument for false conditions
try to remove extraneous items from the set of sampled items. this checks if an item is unnecessary based on the desired error bounds, and merges it with the adjacent item if it is.
get a snapshot of the current values of all the tracked quantiles. to the estimator, returns null.
resets the estimator, clearing out all previously inserted items
get the estimated value at the specified quantile.
specifies the allowable error for this rank, depending on which quantiles are being targeted. this is the f(r_i, n) function from the ckms paper. it's basically how wide the range of this rank can be. the index in the list of samples
merges items from buffer into the samples array in one pass. this is more efficient than doing an insert on every item.
encryption is buffer based. if there is enough room in #inbuffer, then write to this buffer. if #inbuffer is full, then do encryption and write data to the underlying stream.
update the #encryptor: calculate counter and #padding.
do the encryption, input is #inbuffer and output is
get crypto codec for specified algorithmmodepadding. the configuration algorithmmodepadding crypto codec classes with cipher suite configured.
get crypto codec for algorithmmodepadding in config value hadoop.security.crypto.cipher.suite the configuration crypto codec classes with cipher suite configured.
returns if a given protocol version is supported.
return an opensslcipher object that implements the specified transformation. aesctrnopadding. empty, in an invalid format, or if openssl doesn't implement the specified algorithm. a padding scheme that is not available.
finishes a multiple-part operation. the data is encrypted or decrypted, depending on how this cipher was initialized.  the result is stored in the output buffer. upon return, the output buffer's position will have advanced by n, where n is the value returned by this method; the output buffer's limit will not have changed.  if output.remaining() bytes are insufficient to hold the result, a shortbufferexception is thrown.  upon finishing, this method resets this cipher object to the state it was in when previously initialized. that is, the object is available to encrypt or decrypt more data.  if any exception is thrown, this cipher object need to be reset before it can be used again.
continues a multiple-part encryption or decryption operation. the data is encrypted or decrypted, depending on how this cipher was initialized.  all input.remaining() bytes starting at input.position() are processed. the result is stored in the output buffer.  upon return, the input buffer's position will be equal to its limit; its limit will not have changed. the output buffer's position will have advanced by n, when n is the value returned by this method; the output buffer's limit will not have changed.  if output.remaining() bytes are insufficient to hold the result, a shortbufferexception is thrown. output buffer
check and floor buffer size
forcibly free the direct buffer.
aesctrnopadding is required
convert to ciphersuite from name, #algoblocksize is fixed for certain cipher suite, just need to compare the name.
returns suffix of cipher suite configuration.
get decryptor from pool
return direct buffer to pool
clean direct buffer pool
calculate the counter and iv, update the decryptor.
seek to a position.
get underlying stream position.
positioned read fully. it is thread-safe
decryption is buffer based. if there is data in #outbuffer, then read it out of this buffer. if there is no data in #outbuffer, then read more from the underlying stream and do the decryption.
bytebuffer read.
positioned read. it is thread-safe
get direct buffer from pool
read data from underlying stream.
decrypt length bytes in buffer starting at offset. output is also put into buffer starting at offset. it is thread-safe.
reset the underlying stream offset; clear #inbuffer and or #skip(long).
this method is executed immediately after decryption. check whether decryptor should be updated and recalculate padding if needed.
do the decryption using inbuffer as input and outbuffer as output. upon return, inbuffer is cleared; the decrypted data starts at outbuffer.position() and ends at outbuffer.limit();
skip n bytes
return decryptor to pool
decrypt all data in buf: total n bytes from given start position. output is also buf and same start position. buf.position() and buf.limit() should be unchanged after decryption.
generates an integer containing the user-specified number of random bits (right justified, with leading zeros). 0 <= numbits <= 32. of random bits (right justified, with leading zeros).
generates a user-specified number of random bytes. it's thread-safe.
the keystore might have gone down during a flush, in which case either the _new or _old files might exists. this method tries to load the keystore from one of these intermediate files.
try loading from the user specified path, else load from the backup path in case exception is not due to badwrong password.
open up and initialize the keystore. or a problem reading the keystore.
factory method to create a new encryptedkeyversion that can then be passed into #decryptencryptedkey. note that the fields of the returned encryptedkeyversion will only partially be populated; it is not necessarily suitable for operations besides decryption. encrypted key. to encrypt the encrypted key. key. the iv of the encryption key used to encrypt the encrypted key is derived from this iv.
derive the initialization vector (iv) for the encryption key from the iv of the encrypted key. this derived iv is used with the encryption key to decrypt the encrypted key.  the alternative to this is using the same iv for both the encryption key and the encrypted key. even a simple symmetric transformation like this improves security by avoiding iv re-use. ivs will also be fairly unique among different eeks. #getencryptedkeyiv())
creates a keyprovidercryptoextension using a given  if the given keyprovider implements the will provide the extension functionality. if the given keyprovider implements the extended by the keyprovider implements the provide the extension functionality. otherwise, a default extension implementation will be used. keyprovidercryptoextension extension. given keyprovider.
create a new encryptedkeyversion. encrypt the encrypted key. to encrypt the encrypted key. key. the iv of the encryption key used to encrypt the encrypted key is derived from this iv.
creates a keyproviderdelegationtokenextension using a given  if the given keyprovider implements the itself will provide the extension functionality, otherwise a default extension implementation will be used. keyproviderdelegationtokenextension extension. using the given keyprovider.
parse the command line arguments and initialize the data.  % hadoop key create keyname [-size size] [-cipher algorithm] [-provider providerpath] % hadoop key roll keyname [-provider providerpath] % hadoop key list [-provider providerpath] % hadoop key delete keyname [-provider providerpath] [-i] % hadoop key invalidatecache keyname [-provider providerpath] 
serialize the metadata to a set of bytes.
get key metadata in bulk.
generates a key material.
get the algorithm from the cipher.
get the algorithm from the cipher.
roll a new version of the given key generating the material for it.  this implementation generates the key material and calls the
deserialize a new metadata object from a set of bytes.
split the versionname in to a base name. converts "aaabbb3" to "aaabbb".
constructor.
find the provider with the given key.
create a keyprovider based on a provided uri. a provider for the specified uri scheme could not be found.
get size of the queue for keyname. this is only used in unit tests.
initializes the value queues for the provided keys by calling the fill method with "numinitvalues" values
this removes the "num" values currently at the head of the queue for the provided key. will immediately fire the queue filler function if key does not exist how many values are actually returned is governed by the syncgenerationpolicy specified by the user.
drains the queue for the provided key.
constructor takes the following tunable configuration parameters particular key. below which the fillqueueforkey() funciton will be invoked to fill the queue. evicted from the cache. calls "getatmost"
shutdown valuequeue executor threads
calls the wrapped configure() method, then sets timeouts
generate a delegationtokenauthenticatedurl.token from the given generic typed delegation token. token set to the delegation token passed in.
get the doas user name. 'actualugi' is the ugi of the user creating the client it is possible that the creator of the kmsclientprovier calls this method on behalf of a proxyuser (the doasuser). in which case this call has to be made as the proxy user.
this provider expects uris in the following form : kms:@ where : - proto = http or https - authority = [:] - hosts = [;] - hostname = string - port = integer this will always create a loadbalancingkmsclientprovider if the uri is correct.
transform input xml given a stylesheet.
completes this node, moving the parent node to its next child.
adds a new phase. caller needs to set progress weightage
called during execution on a leaf node to set its progress.
computes progress in this node.
adds a node to the tree. gives equal weightage to all phases
adds a named node with a specified progress weightage to the tree.
adds n nodes to the tree. gives equal weightage to all phases
returns progress weightage of the given phase progress weightage
adds a node with a specified progress weightage to the tree.
adds a named node to the tree.
find a jar that contains a class of the same name, if any. it will return a jar file, even if that is not the first thing on the class path that has a class with the same name.
checks if a class should be included as a system class. a class is a system class if and only if it matches one of the positive patterns and none of the negative ones.
create an instance of a class
dump a resource to out
log that a class has been loaded, and where from.
main entry point. runs the class via the toolrunner, then exits with an appropriate exit code.
run the classresource find or load operation
create an instance of gctimemonitor. once it's started, it will stay alive and monitor gc time percentage until shutdown() is called. if you don't put a limit on the number of gctimemonitor instances that you create, and alerthandler != null, you should necessarily call shutdown() once the given instance is not needed. otherwise, you may create a memory leak, because each running gctimemonitor will keep its alerthandler object in memory, which in turn may reference and keep in memory many more other objects. of gc time should be calculated. a practical value would be somewhere between 30 sec and several minutes. gc timings. this is also a frequency with which alerthandler will be invoked if gc time percentage exceeds the specified limit. a practical value would likely be 500..1000 ms. observationwindowms. once this is exceeded, alerthandler will be invoked every sleepintervalms milliseconds until gc time percentage falls below this limit. time percentage exceeds the specified limit.
remove the element corresponding to the key, given key.hashcode() == index. otherwise, return null.
print detailed information of this object.
return default os instance.
return a buffer into the pool. after being returned, the buffer may be recycled, so the user must not continue to use it in any way.
return the number of available buffers of a given size. this is used only for tests.
allocate a direct buffer of the specified size, in bytes. if a pooled buffer is available, returns that. otherwise allocates a new one.
ensure the existence of a given directory.
unpack matching files from a jar. entries inside the jar that do not match the given pattern will be skipped. cannot be created and does not already exist
unpack matching files from a jar. entries inside the jar that do not match the given pattern will be skipped. keep also a copy of the entire jar in the same directory for backward compatibility. todo remove this feature in a new release and do only unjar cannot be created and does not already exist
unpack matching files from a jar. entries inside the jar that do not match the given pattern will be skipped. cannot be created and does not already exist
creates a classloader based on the environment that was specified by the user. if hadoop_use_client_classloader is specified, it creates an application classloader that provides the isolation of the user class space from the hadoop classes and their dependencies. it forms a class space for the user jar as well as the hadoop_classpath. otherwise, it creates a classloader that simply adds the user jar to the classpath.
find the first option of the required class.
returns a java.util.concurrent.threadfactory that names each created thread uniquely, with a common prefix.
a thread pool that that blocks clients submitting additional tasks if there are already activetasks running threads and waitingtasks tasks waiting in its queue.
get a named threadfactory that just builds daemon threads. the supplied exception handler and normal priority
subclass constructors must call this.
compares two version name strings using maven's comparableversion class. the first version to compare the second version to compare integer if version2 precedes version1, and 0 if and only if the two versions are equal.
get a parameter from a servletrequest. return null if the parameter contains only white spaces.
initial html header
an exception if it is not present or if it is not a valid number.
reads the lines in a file. file name is null
this is a driver for the example programs. it looks at the first command line argument and tries to find an example program with that name. if it is found, it calls the main method in that class with the rest of the command line arguments.
create a description of an example program.
invoke the example application with the given arguments
forcibly terminates the currently running java virtual machine. is a haltexception its status overrides that passed in.
like #terminate(int, string) but uses the given throwable to build the message to display or throw as an  is an exitexception its status overrides that passed in.
forcibly terminates the currently running java virtual machine. the exception argument is rethrown if jvm halting is disabled. trace.
inner termination: either exit with the exception's exit code, or, if system exits are disabled, rethrow the exception.
main entry point.
creates a new keyprovider from the given configuration and configuration key name. the configuration
skip to the new value.
creates a http servlet response serializing the exception in it as json. response
validates the status of an httpurlconnection against an expected http status code. if the current status code is not the expected one it throws an exception with a detail message using server side error messages if available.  note: this method will throw the deserialized exception even if not declared in the throws of the method signature. expected one.
creates a http jax-rpc response serializing the exception in it as json.
handle an incoming signal.
return a command to set owner.
a command to get a given user's groups list. if the os is not windows, the command will get the user's primary group first and finally get the groups list which includes the primary group. i.e. the user's primary group will be included twice.
validate the accessibility of the hadoop home directory. path is not a reference to a valid directory.
create a new instance of the shellcommandexecutor to execute a command. as the current working directory for the command. if null, the current working directory is not modified. key-value pairs specified in the map. if null, the current environment is not modified. command will be killed and the status marked as timed-out. if 0, the command will not be timed out. vars from the parent process or not.
static method to return a set of all shell objects.
returns a command to run the given script. the script interpreter is inferred by platform: cmd on windows or bash otherwise.
static method to execute a shell command. covers most of the simple cases without requiring the user to implement the shell interface.
look for setsid.
return a command to send a signal to a given pid.
run the command.
locate the winutils binary, or fail with a meaningful exception and stack trace as an rte. this method is for use in methods which don't explicitly throw an ioexception.
returns the commands of this instance. arguments with spaces in are presented with quotes round; other arguments are presented raw
return a command to create symbolic links.
static method to destroy all running shell processes. iterates through a map of all currently running shell processes and destroys them one by one. this method is thread safe
execute the shell command. not well constructed.
quote the given arg so that bash will interpret it as a single value. note that this quotes it for one level of bash, if you are passing it into a badly written shell script, you need to fix your shell script.
a command to get a given user's group id list. the command will get the user's primary group first and finally get the groups list which includes the primary group. i.e. the user's primary group will be included twice. this command does not support windows and will only return group names.
return a command to read the target of the a symbolic link.
return a command to set permission.
inner logic of #getqualifiedbin(string), accessible for tests.
checks if a given command (string[]) fits in the windows maximum command line length note that the input is expected to already include space delimiters, no extra count will be added for delimiters.
create a filenotfoundexception with the inner nested cause set to the given exception. compensates for the fact that fnfe doesn't have an initializer that takes an exception.
resize the internal table to given capacity.
retrieve an atomic view of the included and excluded hosts.
retrieve an atomic view of the included and excluded hosts.
returns the contents of the machinelist as a collection this can be used for testing
accepts an ip address and return true if ipaddress is in the list
accepts a collection of ipcidrhost addresses
create a line reader that reads from the given stream using the given buffer-size, and using a custom delimiter of array of bytes.
read a line terminated by a custom delimiter.
create a line reader that reads from the given stream using the io.file.buffer.size specified in the given configuration, and using a custom delimiter of array of bytes.
read a line terminated by one of cr, lf, or crlf.
create a line reader that reads from the given stream using the given buffer-size.
create a line reader that reads from the given stream using the default buffer-size, and using a custom delimiter of array of bytes.
shut down all threadpools immediately.
create a asyncdiskservices with a set of volumes (specified by their root directories). the asyncdiskservices uses one threadpool per volume to do the async disk operations.
gracefully start the shut down of all threadpools.
execute the task sometime in the future, using threadpools.
wait for the termination of the thread pools.
cause the current thread to sleep as close as possible to the provided number of milliseconds. this method will log and ignore any
convenience method that returns a resource as inputstream from the classpath using given classloader. 
convenience method that returns a resource as inputstream from the classpath.  uses the thread's context classloader to load resource.
composes numchecksumstoread additional crcs into the current digest out of checksumin, with each crc expected to correspond to exactly
updates with a single additional crc which corresponds to an underlying data size of bytespercrc.
composes length crc_size_in_bytes more crcs from crcbuffer, with each crc expected to correspond to exactly bytespercrc underlying data bytes.
returns a crccomposer which will collapse crcs for every combined underlying data size which aligns with the specified stripe boundary. for example, if "update" is called with 20 crcs and bytespercrc == 5, and stripelength == 10, then every two (10 5) consecutive crcs will be combined with each other, yielding a list of 10 crc "stripes" in the final digest, each corresponding to 10 underlying data bytes. using a stripelength greater than the total underlying data size is equivalent to using a non-striped crccomposer.
returns byte representation of composed crcs; if no stripelength was specified, the digest should be of length equal to exactly one crc. otherwise, the number of crcs in the returned array is equal to the total sum bytespercrc divided by stripelength. if the sum of bytespercrc is not a multiple of stripelength, then the last crc in the array corresponds to totallength % stripelength underlying data bytes.
a tool to test native library availability,
convert from a json file.
create an instance bound to a specific type.
load from a json text file.
load from a hadoop filesystem. there's a check for data availability after the file is open, by raising an eofexception if stream.available == 0. this allows for a meaningful exception without the round trip overhead of a getfilestatus call before opening the file. it may be brittle against an fs stream which doesn't return a value here, but the standard filesystems all do. json parsing and mapping problems are converted to ioes.
write the json as bytes, then close the file.
convert from json.
try to perform some disk io by writing to the given file without using native io.
generate a path name for a test file under the given directory.
checks that the current running process can read, write, and execute the given directory by using methods of the file object. executable
performs some disk io by writing to a new file in the given directory and sync'ing file contents to disk. this increases the likelihood of catching catastrophic diskcontroller failures sooner. disk io against the file.
the semantics of mkdirswithexistscheck method is different from the mkdirs method provided in the sun's java.io.file class in the following way: while creating the non-existent parent directories, this method checks for the existence of those directories if the mkdir fails at any point (since that directory might have just been created by some other process). if both mkdir() and the exists() check fails for any seemingly non-existent directory, then we signal an error; sun's mkdir would signal an error (return false) if a directory it is attempting to create already exists or the mkdir fails.
replace the fileioprovider for tests. this method must not be used outside of unit tests.
create the directory or check permissions if it already exists. the semantics of mkdirswithexistsandpermissioncheck method is different from the mkdirs method provided in the sun's java.io.file class in the following way: while creating the non-existent parent directories, this method checks for the existence of those directories if the mkdir fails at any point (since that directory might have just been created by some other process). if both mkdir() and the exists() check fails for any seemingly non-existent directory, then we signal an error; sun's mkdir would signal an error (return false) if a directory it is attempting to create already exists or the mkdir fails.
starts timing for the instrumented read lock. it records the time to threadlocal.
increment the reference count.
mark the status as closed. once the status is closed, it cannot be reopened. before we do.
decrement the reference count. references.
decrement the reference count, checking to make sure that the closeablereferencecount is not closed.
given an array of bytes it will convert the bytes to a hex string representation of the bytes
split a string using the given separator, with no escaping performed.
splits a comma separated value string, trimming leading and trailing whitespace on each value. duplicate and empty values are removed. collection if null string input
given a hexstring this will return the byte array corresponding to the string string. the size of the byte array is therefore hex.length2
get stack trace for a given thread.
splits a comma or newline separated value string, trimming leading and trailing whitespace on each value. may be null input
make a string representation of the exception.
same as wordutils#wrap in commons-lang 2.6. unlike commons-lang3, leading spaces on the first line are not stripped. as 1 null uses the system property line separator
from a list of command-line arguments, remove an option.
given the time in long milliseconds, returns a string in the sortable format xhrs, ymins, zsec. x, y, and z are always two-digit. if the time is more than 100 hours ,it is displayed as 99hrs, 59mins, 59sec.
returns an arraylist of strings.
concatenates strings, using a separator.
returns a collection of strings. string to parse delimiter to separate the values
generate the text for the startupshutdown message of processes.
formats time in ms and appends difference (finishtime - starttime) as returned by formattimediff(). if finish time is 0, empty string is returned, if start time is 0 then difference is not appended to return value.
escapes html special characters present in the string.
split a string using the given separator
convert some_stuff to somestuff
return a message for logging.
checks if the string contains only unicode letters. null will return false. an empty string (length()=0) will return true.  stringutils.isalpha(null) = false stringutils.isalpha("") = true stringutils.isalpha(" ") = false stringutils.isalpha("abc") = true stringutils.isalpha("ab2c") = false stringutils.isalpha("ab-c") = false 
given an array of strings, return a comma-separated list of its elements. otherwise
concatenates strings, using a separator.
convert a string to long. the input string is first be trimmed and then it is parsed with traditional binary prefix. for example, "-1230k" will be converted to -1230 1024 = -1259520; "891g" will be converted to 891 1024^3 = 956703965184;
formats time in ms and appends difference (finishtime - starttime) as returned by formattimediff(). if finish time is 0, empty string is returned, if start time is 0 then difference is not appended to return value.
finds the first occurrence of the separator character ignoring the escaped separators starting from the index. note the substring between the index and the position of the separator is passed.
compare strings locale-freely by using string#equalsignorecase.
matches a template string against a pattern, replaces matched tokens with the supplied replacements, and returns the result. the regular expression must use a capturing group. the value of the first capturing group is used to look up the replacement. if no replacement is found for the token, then it is replaced with the empty string. for example, assume template is "%foo%_%bar%_%baz%", pattern is "%(.?)%", and replacements contains 2 entries, mapping "foo" to "zoo" and "baz" to "zaz". the result returned would be "zoo__zaz". group capturing group to their replacement values
from a list of command-line arguments, return the first non-option argument. non-option arguments are those which either come after a double dash (--) or do not start with a dash.
from a list of command-line arguments, remove both an option and the next argument. option otherwise.
given the time in long milliseconds, returns a string in the format xhrs, ymins, z sec.
the string array to be parsed into an uri array. equivalent to str. if any string in str violates rfc2396.
convert a long integer to a string with traditional binary prefix.
given a full hostname, return the word upto the first dot.
interns and returns a reference to the representative instance for any of a collection of string instances that are equal to each other. retains weak reference to the instance, and so does not prevent it from being garbage-collected.
interns and returns a reference to the representative instance for any of a collection of string instances that are equal to each other. retains strong reference to the instance, thus preventing it from being garbage-collected.
a wrapper method that makes a call to islocked() of the underlying reentrantlock object. queries if this lock is held by any thread. this method is designed for use in monitoring of the system state, not for synchronization control.
parse a comma-separated list of authentication mechanisms. each such mechanism should be of the form 'scheme:auth' -- the same syntax used for the 'addauth' command in the zk cli.
parse comma separated list of acl entries to secure generated nodes, e.g. sasl:hdfshost1@my.domain:cdrwa,sasl:hdfshost2@my.domain:cdrwa
because zk acls and authentication information may be secret, allow the configuration values to be indirected through a file by specifying the configuration as "@pathtofile". if this syntax is used, this function will return the contents of the file as a string. file if the configured value starts with "@"
parse acl permission string, partially borrowed from zookeepermain private method
log the current thread stacks at info level.
gets all the declared fields of a class including fields declared in superclasses.
this code is to support backward compatibility and break the compile time dependency of core on mapred. this should be made deprecated along with the mapred package hadoop-1230. should be removed when mapred package is removed.
gets all the declared methods of a class including methods declared in superclasses.
log the current thread stacks at info level.
make a copy of the writable object using serialization to a buffer
create an object for the given class and initialize it from conf
print all of the thread's information and stack traces.
print out a prompt to the user, and return true if the user responds with "y" or "yes". (case insensitive)
runs the given tool by tool#run(string[]), after parsing with the given generic arguments. uses the given configuration, or builds one if null. sets the tool's configuration with the possibly modified version of the conf.
get a source name by given directory name.
add the file read latency to mutablequantiles metrics.
add the file write latency to mutablequantiles metrics.
get a metric by given directory name.
this method creates the connection context using exactly the same logic as the old connection context as was done for writable where the effective and real users are set based on the auth method.
read a variable length integer in the same format that protobufs encodes.
stop elapsed time and make the state of stopwatch stop.
start to measure times and make the state of stopwatch running.
clear all registered shutdownhooks.
adds a shutdownhook with a priority, the higher the priority the earlier will run. shutdownhooks with same priority run in a non-deterministic order.
returns the list of shutdownhooks in order of execution, highest priority first.
indicates if a shutdownhook is registered or not.
adds a shutdownhook with a priority and timeout the higher the priority the earlier will run. shutdownhooks with same priority run in a non-deterministic order. the shutdown hook will be terminated if it has not been finished in the specified period of time.
indicates if shutdown is in progress or not.
shutdown the executor thread itself.
removes a shutdownhook. false otherwise.
execute the shutdown. this is exposed purely for testing: do not invoke it.
get the shutdown timeout in seconds, from the supplied configuration.
writes big-endian representation of value into buf starting at offset. buf.length must be greater than or equal to offset + 4.
for use with debug statements; verifies bytes.length on creation, expecting it to represent exactly one crc, and returns a hex formatted value.
reads 4-byte big-endian int value from buf starting at
compute x^( lengthbytes 8) mod mod, where mod is in "reversed" (little-endian) format such that mod  1 represents x^31 and has an implicit term x^32.
for use with debug statements; verifies bytes.length on creation, expecting it to be divisible by crc byte size, and returns a list of hex formatted values.
returns a comparable value for a qualifier. this method takes into account the ordering of known qualifiers then unknown qualifiers with lexical ordering. just returning an integer with the index here is faster, but requires a lot of ifthenelse to check for -1 or qualifiers.size and then resort to lexical ordering. most comparisons are decided by the first character, so this is still fast. if more characters are needed then it requires a lexical sort anyway.
instantiate.
add an element to the end of the list.
add an element to the front of the list.
remove all elements.
writes the current checksum to the stream. if reset is true, then resets the checksum.
this constructs a datachecksum by reading header_len bytes from input stream in
implementation of chunked verification specifically on byte arrays. this is to avoid the copy when dealing with bytebuffers that have array backing.
verify that the given checksums match the given data. the 'mark' of the bytebuffer parameters may be modified by this function,. but the position is maintained. checksums
the flag is volatile to avoid synchronization here. re-entrancy is unlikely except in failure mode (and inexpensive).
crc type, suitable for use with further crc arithmetic. to the given type.
implementation of chunked calculation specifically on byte arrays. this is to avoid the copy when dealing with bytebuffers that have array backing.
compares the checksum located at buf[offset] with the current checksum.
writes the current checksum to a buffer. if reset is true, then resets the checksum.
writes the checksum header to the output stream out.
calculate checksums for the given data. the 'mark' of the bytebuffer parameters may be modified by this function, but the position is maintained. stored. enough space must be available in this buffer to put the checksums.
the limit is disabled if it is <= 0. the creation of an entry is expired if it is added to the cache longer than c. the access of an entry is expired if it is not accessed longer than a.
evict expired entries.
visit all key, value pairs in the identityhashstore.
parse the user-specified options, get the generic options, and modify configuration accordingly.
windows powershell and cmd can parse key=value themselves, because pkey=value is same as pkey value under windows. however this is not compatible with how we get arbitrary key values in -dkey=value format. under windows -d key=value or -dkey=value might be passed as [-dkey, value] or [-d key, value]. this method does undo these and return a modified args list by manually changing [-d, key, value] into [-d, key=value]
print the usage message for generic command-line options supported.
specify properties of each generic option. important as optionbuilder is not thread safe, subclasses must synchronize use on optionbuilder.class
if libjars are set in the conf, parse the libjars.
modify configuration according user-specified generic options.
takes input as a comma separated list of files and verifies if they exist. it defaults for file: if the files specified do not have a scheme. it returns the paths uri converted defaulting to file:. so an input of homeuserfile1,homeuserfile2 would return file:homeuserfile1,file:homeuserfile2. true, any directory followed by a wildcard is a valid entry and is replaced with the list of jars in that directory. it is used to support the wildcard notation in a classpath. if the input files argument is null
interrupted false otherwise
executorservice#awaittermination(long, java.util.concurrent.timeunit) calls in milli seconds. false otherwise
deepest recursion before giving up and doing a heapsort. returns 2 ceil(log(n)).
returns a diskvalidator instance corresponding to the passed clazz.
returns diskvalidator instance corresponding to its name. the diskvalidator parameter can be "basic" for basicdiskvalidator or "read-write" for readwritediskvalidator.
apply delta to accumulators.
simple 'main' to facilitate manual testing of the pause monitor. this main function just leaks memory into a list. running this class with a 1gb heap will very quickly go into "gc hell" and result in log messages about the gc pauses.
read sysblockdisknamequeuehw_sector_size file, parse and calculate sector size for a specific disk.
constructor which allows assigning the proc directories. this will be used only in unit tests.
read procdiskstats file, parse and calculate amount of bytes read and written fromto disks.
read procnetdev file, parse and calculate amount of bytes read and written through the network.
read procstat file, parse and calculate cumulative cpu.
read proccpuinfo, parse and calculate cpu information.
read procmeminfo, parse and compute memory information.
test the sysinfolinux.
log a warning if the lock was held for too long. should be invoked by the caller immediately after releasing the lock.
create a instrumented lock instance which logs a warning message when lock held time is above given threshold. given logger instead this is to avoid spamming to many logs time as being "too long"
returns the class object (of type class<t>) of the argument of type t.
converts the given list<t> to a an array of t[].
method to check if the output string has line which begins with error. string
method used to determine if or not node health monitoring service should be started or not. returns true if following conditions are met:  path to node health check script is not empty node health check script file exists 
method used to start the node health monitoring.
method used to terminate the node health monitoring service.
deletes the path. checks for existence of path as well.
delete a znode.
utility function to ensure that the configured base znode exists. this recursively creates the znode as well as all of its parents.
get the data in a znode.
get the data in a znode.
utility method to fetch the zk acls from the configuration. cannot be read
create a znode.
start the connection to the zookeeper ensemble.
close the connection with zookeeper.
add a path to reap children from
the reaper must be started
use #get(long, timeunit) timeout parameters to wait.
helper routine to shutdown a executorservice.
compute the hash of the specified file
this utility method converts the name of the configured hash type to a symbolic constant.
removes a specified key from this counting bloom filter.  invariant: nothing happens if the specified key does not belong to this counter bloom filter.
hashes a specified key into several integers.
constructor.  builds a hash function that must obey to a given maximum number of returned values and a highest value.
performs the selective clearing for a given key.
adds a false positive information to this retouched bloom filter.  invariant: if the false positive is null, nothing happens.
adds a collection of false positive information to this retouched bloom filter.
clears a specified bit in the bit vector and keeps up-to-date the keylist vectors.
removes a given key from this filer.
adds an array of false positive information to this retouched bloom filter.
creates and initialises the various vectors.
adds a list of false positive information to this retouched bloom filter.
constructor
constructor.
adds an array of keys to this filter.
adds a list of keys to this filter.
adds a collection of keys to this filter.
adds a new row to this dynamic bloom filter.
constructor.  builds an empty dynamic bloom filter. dynamic bloom filter row.
get the groups for the users given and print formatted output to the
add a new field to the table under construction. defaults to integer.max_value.
add a new row.
return the ith row of the column as a set of wrapped strings, each at most wrapwidth in length.
stops the minikdc
starts the minikdc.
creates a minikdc. this directory an apacheds working directory will be created, this directory will be deleted when the minikdc stops.
creates multiple principals in the kdc and adds them to a keytab file. created.
this method checks if the specified authtoken belongs to the specified http authentication scheme. authentication scheme. specified authentication scheme false otherwise.
this method provides an instance of authenticationhandler based on specified authhandlername. authentication handler.
this method checks if the specified http authentication scheme value is valid. http authentication scheme.
authenticates an http client request.  it extracts the pseudoauthenticator#user_name parameter from the query string and creates an authenticationtoken with it.  if the http client request does not contain the pseudoauthenticator#user_name parameter and the handler is configured to allow anonymous users it returns the authenticationtoken#anonymous token.  if the http client request does not contain the pseudoauthenticator#user_name parameter and the handler is configured to disallow anonymous users it throws an authenticationexception.
verify the signature of the jwt token in this method. this method depends on the public key that was established during init based upon the provisioned public key. override this method in subclasses in order to customize the signature verification behavior.
create the url to be used for authentication of the user in the absence of a jwt token within the incoming request.
encapsulate the acquisition of the jwt token from http cookies within the request.
initializes the authentication handler instance.  this method is invoked by the authenticationfilter#init method.  configuration properties to initialize the handler. thrown if the handler could not be initialized.
validate that the expiration time of the jwt token has not been violated. if it has then throw an authenticationexception. override this method in subclasses in order to customize the expiration validation behavior.
validate whether any of the accepted audience claims is present in the issued token claims list for audience. override this method in subclasses in order to customize the audience validation behavior. the jwt token where the allowed audiences will be found
this method provides a single method for validating the jwt for use in request processing. it provides for the override of specific aspects of this implementation through submethods used within but also allows for the override of the entire token validation algorithm.
returns the authenticationtoken for the request.  it looks at the received http cookies and extracts the value of the authenticatedurl#auth_cookie if present. it verifies the signature and if correct it creates the authenticationtoken and returns it.  if this method returns null the filter will invoke the configured authenticationhandler to perform user authentication.
creates the hadoop authentication http cookie. cookie. it has no effect if its value < 0. xxx the following code duplicate some logic in jetty servlet api, because of the fact that hadoop is stuck at servlet 2.5 and jetty 6 right now.
returns if a custom implementation of a signersecretprovider is being used.
returns the filtered configuration (only properties starting with the specified prefix). the property keys are also trimmed from the prefix. the returned properties object is used to initialized the  this method can be overriden by subclasses to obtain the configuration from other configuration source than the web.xml file.
this method verifies if the specified token type matches one of the the token types supported by a specified authenticationhandler. this method is specifically designed to work with multiple authentication schemes while the authenticationhandler interface supports a single type via should be used for verification. false otherwise
if the request has a valid authentication token it allows the request to continue to the target resource, otherwise it triggers an authentication sequence using the configured authenticationhandler.
initializes the authentication filter and signer secret provider. it instantiates and initializes the specified authenticationhandler.
returns the full url of the request including the query string.  used as a convenience method for logging purposes.
destroys the filter.  it invokes the authenticationhandler#destroy() method to release any resources it may hold.
initializes the authentication handler instance.  it creates a kerberos context using the principal and keytab specified in the configuration.  this method is invoked by the authenticationfilter#init method.
it enforces the the kerberos spnego authentication sequence returning an completed successfully. and valid, null if it is in progress (in this case the handler handles the response to the client).
this method parses the user-agent string and returns whether or not it refers to a browser. if its not a browser, then kerberos authentication will be used; if it is a browser, alternateauthenticate from the subclass will be used.  a user-agent string is considered to be a browser if it does not contain any of the values from alt-kerberos.non-browser.user-agents; the default behavior is to consider everything a browser unless it contains one of: "java", "curl", "wget", or "perl". subclasses can optionally override this method to use different behavior.
implements the spnego authentication sequence interaction using the current default principal in the kerberos cache (normally set via kinit).
performs spnego authentication against the specified url.  if a token is given it does a nop and returns the given token.  if no token is given, it will perform the spnego authentication sequence using an http options request.
if the specified url does not support spnego authentication, a fallback authenticator will be used.  this implementation returns a pseudoauthenticator.
returns an authenticated httpurlconnection.
helper method that injects an authentication token to send with a connection. callers should prefer using automatically manages authentication tokens.
installs a cookie handler for the http request to manage session cookies.
creates a token using an existing string representation of the token.
returns the string representation of the token.
helper method that extracts an authentication token received from a connection.  this method is used by authenticator implementations.
creates an authenticatedurl. kerberosauthenticator is used.
performs simple authentication against the specified url.  if a token is given it does a nop and returns the given token.  if no token is given, it will perform an http options request injecting an additional parameter #user_name in the query string with the value returned by the #getusername() method.  if the response is successful it will update the authentication token.
disconnects from zookeeper unless told not to.
add an entry to the jaas configuration with the passed in name, principal, and keytab. the other necessary options will be set for you.
pulls data from zookeeper. if isinit is false, it will only parse the next secret and version. if isinit is true, it will also parse the current and previous secrets, and the next rollover date; it will also init the secrets. hence, isinit should only be true on startup.
this constructor lets you set the seed of the random number generator and is meant for testing.
pushes proposed data to zookeeper. if a different server pushes its data first, it gives up.
serialize the data to attempt to push into zookeeper. the format is this:  [data_version, newsecretlength, newsecret, currentsecretlength, currentsecret, previoussecretlength, previoussecret, nextrolloverdate]  only previoussecret can be null, in which case the format looks like this:  [data_version, newsecretlength, newsecret, currentsecretlength, currentsecret, 0, nextrolloverdate] 
this method creates the curator client and connects to zookeeper.
starts the scheduler for the rollover to run at an interval.
rolls the secret. it is called automatically at the rollover interval.
generates the token.
check if the provided value is invalid. throw an error if it is invalid, nop otherwise.
splits the string representation of a token into attributes pairs. attribute pairs.
create kerberos principal for a given service and hostname, inferring realm from the fqdn of the hostname. it converts hostname to lower case. if hostname is null or "0.0.0.0", it uses dynamically looked-up fqdn of the current host instead. if domain_realm mappings are inadequately specified, it will use default_realm, per usual kerberos behavior. if default_realm also gives a null value, then a principal without realm will be returned, which by kerberos definitions is just another way to specify default realm. service for which you want to generate the principal. fully-qualified domain name. if no ip address for the local host could be found.
get all the unique principals present in the keytabfile. name of the keytab file to be read. if keytab entries cannot be read from the file.
extract the tgs server principal from the given gssapi kerberos or spnego wrapped token.
get all the unique principals from keytabfile which matches a pattern.
returns a signed string.
creates a signer instance using the specified signersecretprovider. the signersecretprovider should already be initialized.
returns then signature of a string.
verifies a signed string and extracts the original string.
this constructor lets you set the seed of the random number generator and is meant for testing.
gets an rsapublickey from the provided pem encoding. - the pem encoding from config without the header and footer
create a name from the full kerberos principal name.
get the configured default realm. used syncronized method here, because double-check locking is overhead.
get the translation of the principal name into an operating system user name.
put the name back together from the parts.
get the rules.
'hadoop' indicates '@' or '' are not allowed the result evaluation. 'mit' indicates that auth_to_local rules follow mit kerberos evaluation.
try to apply this rule to the given name represented as a parameter array. are the components of the name "ab@foo" -> "foo", "a", "b"
replace the matches of the from pattern in the base string with the value of the to string.
replace the numbered parameters of the form $n where n is from 1 to the length of params. normal text is copied directly and $n is replaced by the corresponding parameter.
intercept the given method invocation, submit the actual calling of the method to the correct task executor and return immediately to the caller. otherwise.
this implementation searches for a unique org.springframework.core.task.taskexecutor bean in the context, or for an executor bean named "taskexecutor" otherwise. if neither of the two is resolvable (e.g. if no beanfactory was configured at all), this implementation falls back to a newly created simpleasynctaskexecutor instance for local use if no default could be found.
find the bean name for the given invocation. assumes that an exposebeannameadvisor has been included in the interceptor chain.
create a string name for the given methodinvocation that can be used for tracelogging purposes. this name is made up of the configured prefix, followed by the fully-qualified name of the method being invoked, followed by the configured suffix.
writes a log message before the invocation based on the value of entermessage. if the invocation succeeds, then a log message is written on exit based on the value written based on the value of exceptionmessage.
adds the string representation of the method return value to the supplied stringbuffer. correctly handles
replace the placeholders in the given message with the supplied values, or values derived from those supplied. used to derive values for all placeholders except $[exception] and $[returnvalue]. used to replace the $[returnvalue] placeholder. may be null. the value of throwable.tostring() is replaced for the
adds a comma-separated list of the short class names of the method argument types to the output. for example, if a method has signature will be string, object. arguments will be retrieved from the corresponding method.
checks to see if the supplied string has any placeholders that are not specified as constants on this class and throws an
configure this aspect with the given executor and exception handler suppliers, applying the corresponding default if a supplier is not resolvable.
retrieve or build a default executor for this advice instance. an executor returned from here will be cached for further use. the default implementation searches for a unique taskexecutor bean in the context, or for an executor bean named "taskexecutor" otherwise. if neither of the two is resolvable, this implementation will return null.
delegate for actually executing the given task with the chosen executor.
retrieve a target executor for the given qualifier.
determine the specific executor to use when executing the given method. should preferably return an asynclistenabletaskexecutor implementation.
handles a fatal error thrown while asynchronously invoking the specified if the return type of the method is a future object, the original exception can be propagated by just throwing it at the higher level. however, for all other cases, the exception will not be transmitted back to the client. in that later case, the current asyncuncaughtexceptionhandler will be used to manage such exception.
count the thrown exception and put the stack trace in the details portion of the key. this will allow the stack trace to be viewed in the jamon web application.
wraps the invocation with a jamon monitor and writes the current performance statistics to the log (if enabled).
determines whether or not logging is enabled for the particular methodinvocation. if not, the method invocation proceeds as normal, otherwise the method invocation is passed to the invokeundertrace method for handling.
write the supplied trace message and throwable to the supplied log instance. to be called by #invokeundertrace for enterexit outcomes, potentially including an exception. note that an exception's stack trace won't get logged when #setlogexceptionstacktrace is "false". by default messages are written at trace level. subclasses can override this method to control which level the message is written at, typically also overriding #islogenabled accordingly.
return the appropriate log instance to use for the given is set, the log instance will be for the target class of the default static logger.
return the aop alliance methodinvocation object associated with the current invocation. or if the exposeinvocationinterceptor was not added to this interceptor chain
parses the supplied  element and registers the resulting with the supplied beandefinitionregistry.
return true if the supplied node describes an advice type. may be one of: ' before', ' after', ' after-returning', ' after-throwing' or ' around'.
parses one of ' before', ' after', ' after-returning', ' after-throwing' or ' around' and registers the resulting beandefinition with the supplied beandefinitionregistry.
gets the advice implementation class corresponding to the supplied element.
creates a beandefinition for the aspectjexpressionpointcut class using the supplied pointcut expression.
parses the pointcut or pointcut-ref attributes of the supplied and returns its bean name, otherwise returns the bean name of the referred pointcut.
parses the supplied  and registers the resulting pointcut with the beandefinitionregistry.
create a rootbeandefinition for the advisor described in the supplied. does not parse any associated ' pointcut' or ' pointcut-ref' attributes.
creates the rootbeandefinition for a pojo advice bean. also causes pointcut parsing to occur so that the pointcut may be associate with the advice bean. this same pointcut is also configured as the pointcut for the enclosing advisor definition using the supplied mutablepropertyvalues.
parse a ' declare-parents' element and register the appropriate declareparentsadvisor with the beandefinitionregistry encapsulated in the supplied parsercontext.
register the beandefinitionparser beandefinitionparsers for the ' config', ' spring-configured', ' aspectj-autoproxy' and ' scoped-proxy' tags.
create a composablepointcut based on the given pointcut.
invoke the given target via reflection, as part of an aop method invocation.
can the given pointcut apply at all on the given class? this is an important test as it can be used to optimize out a pointcut for a class. for this bean includes any introductions
determine the target class of the given bean instance which might be an aop proxy. returns the target class for an aop proxy or the plain class otherwise. never null)
determine the sublist of the candidateadvisors list that is applicable to the given class. (may be the incoming list as-is)
select an invocable method on the target type: either the given method itself if actually exposed on the target type, or otherwise a corresponding method on one of the target type's interfaces or on the target type itself. target type (typically due to a proxy mismatch)
perform the least expensive check for a pointcut match.
add the specified interface to the list of interfaces to introduce.
create a defaultintroductionadvisor for the given advice. the interface to introduce (may be null)
this method is implemented only to restore the logger. we don't make the logger static as that would mean that subclasses would use this class's log category.
check whether the specified interfaces is a published introduction interface.
subclasses may need to override this if they want to perform custom behaviour in around advice. however, subclasses should invoke this method, which handles introduced interfaces and forwarding to the target.
returns true if the exclusion pattern at index patternindex matches the supplied candidate string.
compiles the supplied string[] into an array of
returns true if the pattern at index patternindex matches the supplied candidate string.
set the regular expressions defining methods to match. matching will be the union of all these; if any match, the pointcut matches.
set the regular expressions defining methods to match for exclusion. matching will be the union of all these; if any match, the pointcut matches.
try to match the regular expression against the fully qualified name of the target class as well as against the method's declaring class, plus the name of the method.
match all methods that either (or both) of the given methodmatchers matches. of the given methodmatchers matches
apply the given methodmatcher to the given method, supporting an (if applicable). asking is the subject on one or more introductions; false otherwise
match all classes that both of the given classfilters match. of the given classfilter match
match all classes that either (or both) of the given classfilters matches. of the given classfilter matches
create a new annotationmatchingpointcut for the given annotation type. (can be null) (can be null) as well as meta-annotations for the annotation type
private constructor to share common code between impl-based delegate and reference-based delegate (cannot use method such as init() to share common code, due the use of final fields).
deduce the parameter names for an advice method. see the aspectjadviceparameternamediscoverer class level javadoc for this class for details of the algorithm used.
match the given list of extracted variable names to argument slots.
an advice method can never be a constructor in spring.
parse the string pointcut expression looking for this(), target() and args() expressions. if we find one, try and extract a candidate variable name and bind it.
if a returning variable was specified and there is only one choice remaining, bind it.
parse the string pointcut expression looking for: @this, @target, @args, @within, @withincode, @annotation. if we find one of these pointcut expressions, try and extract a candidate variable name (or variable names, in the case of args). some more support from aspectj in doing this exercise would be nice... :)
match up args against unbound arguments of primitive types.
given an args pointcut body (could be args or at_args), add any candidate variable names to the given list.
if a throwing name was specified and there is exactly one choice remaining (argument that is a subtype of throwable) then bind it.
following aspectj semantics, if a return value is null (or return type is void), then the return type of target method should be used to determine whether advice is invoked or not. also, even if the return type is void, if the type of argument declared in the advice method is object, then the advice must still get invoked.
create a new aspectjpointcutadvisor for the given advice.
return the aspectjprecedenceinformation provided by this advisor or its advice. if neither the advisor nor the advice have precedence information, this method will return null.
return true if the advisor is a form of before advice.
return true if the advisor is a form of after advice.
all parameters from argumentindexoffset onwards are candidates for pointcut parameters - but returning and throwing vars are handled differently and must be removed from the list if present.
take the arguments at the method execution join point and output a set of arguments to the advice method.
create a parameternamediscoverer to be used for argument binding. the default implementation creates a defaultparameternamediscoverer and adds a specifically configured aspectjadviceparameternamediscoverer.
build a 'safe' pointcut that excludes the aspectj advice method itself.
get the current join point match at the join point we are being dispatched on.
we need to hold the throwing name at this level for argument binding calculations, this method allows the afterthrowing advice subclass to set the name.
create a new abstractaspectjadvice for the given advice method.
lazily instantiate joinpoint for the current invocation. requires methodinvocation to be bound with exposeinvocationinterceptor. do not use if access is available to the current reflectivemethodinvocation (in an around advice). spring aop invocation.
we need to hold the returning name at this level for argument binding calculations, this method allows the afterreturning advice subclass to set the name.
add special advisors if necessary to work with a proxy chain that contains aspectj advisors. this will expose the current spring aop invocation (necessary for some aspectj pointcut matching) and make available the current aspectj joinpoint. the call will have no effect if there are no aspectj advisors in the advisor chain.
determine whether the given advisor contains an aspectj advice.
initialize the underlying aspectj pointcut parser.
create a new aspectjexpressionpointcut with the given settings.
get a new pointcut expression based on a target class's loader rather than the default.
build the underlying aspectj pointcut expression.
check whether this pointcut is ready to match, lazily building the underlying aspectj pointcut expression.
sort the rest by aspectj precedence. if two pieces of advice have come from the same aspect they will have the same order. advice from the same aspect is then further ordered according to the following rules:  if either of the pair is after advice, then the advice declared last gets highest precedence (runs last) otherwise the advice declared first gets highest precedence (runs first)  important: advisors are sorted in precedence order, from highest precedence to lowest. "on the way in" to a join point, the highest precedence advisor should run first. "on the way out" of a join point, the highest precedence advisor should run last.
check whether the given aspect bean is eligible for auto-proxying. if no <aop:include> elements were used then "includepatterns" will be then one of the patterns must match.
set a list of regex patterns, matching eligible @aspectj bean names. default is to consider all @aspectj beans as eligible.
duplicates some logic from getadvice, but importantly does not force creation of the advice.
build a org.springframework.aop.aspectj.declareparentsadvisor for the given introduction field. resulting advisors will need to be evaluated for targets.
create an aspectmetadata instance for the supplied aspect type.
get the singleton aspect instance for the supplied aspect type. an instance is created if one cannot be found in the instance cache.
add an aspect of the supplied type to the end of the advice chain.
add the supplied aspect instance to the chain. the type of the aspect instance supplied must be a singleton aspect. true singleton lifecycle is not honoured when using this method - the caller is responsible for managing the lifecycle of any aspects added in this way.
create a metadataawareaspectinstancefactory for the supplied aspect type. if the aspect type has no per clause, then a singletonmetadataawareaspectinstancefactory is returned, otherwise a prototypeaspectinstancefactory is returned.
create a beanfactoryaspectinstancefactory, providing a type that aspectj should introspect to create ajtype metadata. use if the beanfactory may consider the type to be a subclass (as when using cglib), and the information should relate to a superclass. ( null indicates resolution through beanfactory#gettype via the bean name)
look for aspectj-annotated aspect beans in the current bean factory, and return to a list of spring aop advisors representing them. creates a spring advisor for each aspectj advice method.
we need to detect this as "code-style" aspectj aspects should not be interpreted by spring aop.
create a new aspectmetadata instance for the given aspect class.
extract contents from string of form pertarget(contents).
create a prototypeaspectinstancefactory. aspectj will be called to introspect to create ajtype metadata using the type returned for the given bean name from the beanfactory.
set the owning beanfactory. we need to save a reference so that we can use the getbean method on every invocation.
replaces this object with a singletontargetsource on serialization. protected as otherwise it won't be invoked for subclasses. (the writereplace() method must be visible to the class being serialized.) with this implementation of this method, there is no need to mark non-serializable fields in this class or subclasses as transient.
returns the lazy-initialized target object, creating it on-the-fly if it doesn't exist already.
subclasses can override this if they want to return a specific commons pool. they should apply any configuration properties to the pool here. default is a genericobjectpool instance with the given pool size.
activate this proxy configuration.
propagate advice change event to all advisedsupportlisteners.
create the advisor (interceptor) chain. advisors that are sourced from a beanfactory will be refreshed each time a new prototype instance is added. interceptors added programmatically through the factory api are unaffected by such changes.
invoked when advice chain is created. add the given advice, advisor or object to the interceptor list. because of these three possibilities, we can't type the signature more strongly. bean factory
add all global interceptors and pointcuts.
look at bean factory metadata to work out whether this bean name, which concludes the interceptornames list, is an advisor or advice, or may be a target.
check the interceptornames list whether it contains a target name as final element. if found, remove the final name from the list and set it as targetname.
blow away and recache singleton on an advice change.
convert the following object sourced from calling getbean() on a name in the interceptornames array to an advisor or targetsource.
return an independent advisor chain. we need to do this every time a new prototype instance is returned, to return distinct instances of prototype advisors and advices.
return a proxy. invoked when clients obtain beans from this factory bean. create an instance of the aop proxy to be returned by this factory. the instance will be cached for a singleton, and create on each call to
return the singleton instance of this class's proxy object, lazily creating it if it hasn't been created already.
return a targetsource to use when creating a proxy. if the target was not specified at the end of the interceptornames list, the targetsource will be this class's targetsource member. otherwise, we get the target bean and wrap it in a targetsource if necessary.
create a new prototype instance of this class's created proxy object, backed by an independent advisedsupport configuration.
finds any #equals or #hashcode method that may be defined on the supplied set of interfaces.
construct a new jdkdynamicaopproxy for the given aop configuration. exception in this case, rather than let a mysterious failure happen later.
implementation of invocationhandler.invoke. callers will see exactly the exception thrown by the target, unless a hook method throws an exception.
process a return value. wraps a return of this if necessary to be the
create a new cglibaopproxy for the given aop configuration. exception in this case, rather than let a mysterious failure happen later.
set constructor arguments to use for creating the proxy.
implementation of callbackfilter.accept() to return the index of the callback we need. the callbacks for each proxy are built up of a set of fixed callbacks for general use and then a set of callbacks that are specific to a method for use on static targets with a fixed advice chain. the callback used is determined thus:  for exposed proxies exposing the proxy requires code to execute before and after the methodchain invocation. this means we must use dynamicadvisedinterceptor, since all other interceptors can avoid the need for a trycatch block for object.finalize(): no override for this method is used. for equals(): the equalsinterceptor is used to redirect equals() calls to a special handler to this proxy. for methods on the advised class: the adviseddispatcher is used to dispatch the call directly to the target for advised methods: if the target is static and the advice chain is frozen then a fixedchainstatictargetinterceptor specific to the method is used to invoke the advice chain. otherwise a dynamicadvisedinterceptor is used. for non-advised methods: where it can be determined that the method will not return this or when proxyfactory.getexposeproxy() returns false, then a dispatcher is used. for static targets, the staticdispatcher is used; and for dynamic targets, a dynamicunadvisedinterceptor is used. if it possible for the method to return this then a staticunadvisedinterceptor is used for static targets - the dynamicunadvisedinterceptor already considers this. 
checks to see whether the supplied class has already been validated and validates it if not.
checks for final methods on the given class, as well as package-visible methods across classloaders, and writes warnings to the log for each one found.
determine a targetsource for the given target (or targetsource). used as our targetsource; otherwise it is wrapped in a singletontargetsource.
create a proxy for the specified targetsource that extends the target class of the targetsource.
make the given proxy available via the currentproxy() method. note that the caller should be careful to keep the old value as appropriate.
try to return the current aop proxy. this method is usable only if the calling method has been invoked via aop, and the aop framework has been set to expose proxies. otherwise, this method will throw an illegalstateexception. method was invoked outside an aop invocation context, or because the aop framework has not been configured to expose the proxy
copy the aop configuration from the given advisedsupport object, but allow substitution of a fresh targetsource and a given interceptor chain.
for debuggingdiagnostic use.
is the given advice included in any advisor within this proxy configuration?
count advices of the given class.
add all of the given advisors to this proxy configuration.
cannot add introductions this way unless the advice implements introductioninfo.
determine a list of org.aopalliance.intercept.methodinterceptor objects for the given method, based on this configuration.
add a new proxied interface.
build a configuration-only copy of this advisedsupport, replacing the targetsource.
prepare a proxyfactory for the given bean. subclasses may customize the handling of the target instance and in particular the exposure of the target class. the default introspection of interfaces for non-target-class proxies and the configured advisor will be applied afterwards; #customizeproxyfactory allows for late customizations of those parts right before proxy creation.
check the interfaces on the given bean class and apply them to the proxyfactory, if appropriate. calls #isconfigurationcallbackinterface and #isinternallanguageinterface to filter for reasonable proxy interfaces, falling back to a target-class proxy otherwise.
determine whether the given interface is a well-known internal language interface and therefore not to be considered as a reasonable proxy interface. if no reasonable proxy interface is found for a given bean, it will get proxied with its full target class, assuming that as the user's intention.
determine the ultimate target class of the given bean instance, traversing not only a top-level proxy but any number of nested proxies as well  as long as possible without side effects, that is, just for singleton targets. object as fallback; never null)
extract the user-specified interfaces that the given proxy implements, i.e. all non-advised interfaces that the proxy implements. in the original order (never null or empty)
adapt the given arguments to the target signature in the given method, if necessary: in particular, if a given vararg argument array does not match the array type of the declared vararg parameter in the method.
determine the complete set of interfaces to proxy for the given aop configuration. this will always add the advised interface unless the advisedsupport's
determine whether the advisors contain matching introductions.
this implementation returns a shallow copy of this invocation object, including an independent copy of the original arguments array. we want a shallow copy in this case: we want to use the same interceptor chain and other object references, but we want an independent value for the current interceptor index.
return user attributes associated with this invocation. this method provides an invocation-bound alternative to a threadlocal. this map is initialized lazily and is not used in the aop framework itself. (never null)
this implementation returns a shallow copy of this invocation object, using the given arguments array for the clone. we want a shallow copy in this case: we want to use the same interceptor chain and other object references, but we want an independent value for the current interceptor index.
set the names of the beans that should automatically get wrapped with proxies. a name can specify a prefix to match by ending with "", e.g. "mybean,tx" will match the bean named "mybean" and all beans whose name start with "tx". note: in case of a factorybean, only the objects created by the factorybean will get proxied. this default behavior applies as of spring 2.0. if you intend to proxy a factorybean instance itself (a rare use case, but spring 1.2's default behavior), specify the bean name of the factorybean including the factory-bean prefix  e.g. 
identify as bean to proxy if the bean name is in the configured list of names.
find all eligible advisor beans in the current bean factory, ignoring factorybeans and excluding beans that are currently in creation.
create a target source for bean instances. uses any targetsourcecreators if set. returns null if no custom targetsource should be used. this implementation uses the "customtargetsourcecreators" property. subclasses can override this method to use a different mechanism.
resolves the specified interceptor names to advisor objects.
return whether the given bean class represents an infrastructure class that should never be proxied. the default implementation considers advices, advisors and aopinfrastructurebeans as infrastructure classes.
wrap the given bean if necessary, i.e. if it is eligible for being proxied.
create an aop proxy for the given bean. specific to this bean (may be empty, but not null) already pre-configured to access the bean
determine the advisors for the given bean, including the specific interceptors as well as the common interceptor, all adapted to the advisor interface. specific to this bean (may be empty, but not null)
find all eligible advisors for auto-proxying this class. if there are no pointcuts or interceptors
set the name of the currently proxied bean instance.
consider advisor beans with the specified prefix as eligible, if activated.
determine the original target class for the specified bean, if possible, otherwise falling back to a regular gettype lookup.
determine whether the given bean name indicates an "original instance" according to autowirecapablebeanfactory#original_instance_suffix, skipping any proxy attempts for it.
determine whether the given bean should be proxied with its target class rather than its interfaces. checks the of the corresponding bean definition.
expose the given target class for the specified bean, if possible.
build an internal beanfactory for resolving target beans.
destroys the internal beanfactory on shutdown of the targetsourcecreator.
determine the exception handle method for the given exception.
create a new throwsadviceinterceptor for the given throwsadvice. (usually a org.springframework.aop.throwsadvice implementation)
create a new defaultadvisoradapterregistry, registering well-known adapters.
generate a scoped proxy for the supplied target bean, registering the target bean with an internal name and setting 'targetbeanname' on the scoped proxy.
compare each individual entry in a list after having sorted both lists (optionally using a comparator). specifying the comparator, both lists will be sorted not using any comparator.
checks whether the model value under the given modelname exists and checks it type, based on the expectedtype. if the model entry exists and the type matches, the model value is returned.
inspect the expectedmodel to see if all elements in the model appear and are equal.
consume the result of the jsonpath evaluation and provide a target class.
consume the result of the jsonpath evaluation.
execute the given runnable, catch any assertionerror, decorate with assertionerror containing diagnostic information about the request and response, and then re-throw.
return details of executed requests.
return a matching expectation, or null if none match.
return an assertionerror that a sub-class can raise for an unexpected request.
create a new request expectation that should be called a number of times as indicated by requestcount.
note that as of 5.0.3, the creation of the response, which may block intentionally, is separated from request count tracking, and this method no longer increments the count transparently. instead
parse the request body and the given string as xml and assert that the two are "similar" - i.e. they contain the same elements and attributes regardless of order. use of this matcher assumes the  href="http:xmlunit.sourceforge.net">xmlunit library is available.
parse the request content as node and apply the given matcher.
parse the body as form data and compare to the given multivaluemap.
parse the request body and the given string as json and assert the two are "similar" - i.e. they contain the same attribute-value pairs regardless of formatting. can compare in two modes, depending on strict parameter value:   true: strict checking. not extensible, and strict array ordering.  false: lenient checking. extensible, and non-strict array ordering.  use of this matcher requires the  href="http:jsonassert.skyscreamer.org">jsonassert library.
parse the request content as domsource and apply the given matcher.
evaluate the json path expression against the request content and assert the resulting value with the given hamcrest matcher.
evaluate the json path expression against the request content and assert that the result is a string.
evaluate the json path expression against the response content and assert that a value, possibly null, exists. if the json path expression is not that the list of values at the given path is not empty.
evaluate the json path expression against the request content and assert that the result is a java.util.map.
evaluate the json path expression against the request content and assert that a non-empty value exists at the given path. for the semantics of empty, consult the javadoc for
evaluate the json path expression against the request content and assert that a value does not exist at the given path. if the json path expression is not jsonpath#isdefinite definite, this method asserts that the value at the given path is empty.
evaluate the json path expression against the request content and assert that the result is an array.
evaluate the json path expression against the request content and assert that an empty value exists at the given path. for the semantics of empty, consult the javadoc for
evaluate the json path expression against the request content and assert that a non-null value exists at the given path. if the json path expression is not jsonpath#isdefinite definite, this method asserts that the value at the given path is not empty.
evaluate the json path expression against the request content and assert that the result is a number.
evaluate the json path expression against the request content and assert that the result is equal to the supplied value.
evaluate the json path expression against the supplied content and assert that a value, including null values, does not exist at the given path. if the json path expression is not that the list of values at the given path is empty.
evaluate the json path expression against the request content and assert that the result is a boolean.
an overloaded variant of #value(matcher) that also accepts a target type for the resulting value that the matcher can work reliably against. this can be useful for matching numbers reliably  for example, to coerce an integer into a double.
access to response body matchers using an xpath to inspect a specific subset of the body. the xpath expression can be a parameterized string using formatting specifiers as defined in
class constructor, not for direct instantiation. use formatting specifiers defined in string#format(string, object...)
assert that content exists at the given xpath.
apply the xpath and assert the number found with the given matcher.
apply the xpath and assert the number of nodes found.
apply the xpath and assert the boolean value found.
apply the xpath and assert the number of nodes found with the given
apply the xpath and assert the number of nodes found.
assert that content does not exist at the given xpath.
apply the xpath and assert it with the given matcher.
apply the xpath and assert the string content found.
apply the xpath and assert the string content found with the given matcher.
private constructor, not for direct instantiation.
perform a request and return a type that allows chaining further actions, such as asserting expectations, on the result. see static factory methods in
static method for matching with an array of result matchers.
add interceptors mapped to a set of path patterns.
build a org.springframework.test.web.servlet.mockmvc instance.
create a new instance using the supplied mockmvc instance.
add additional webrequestmatcher instances that return true if a supplied host matches  for example, "example.com" or
supply the webclient that the client #build built by this builder should delegate to when processing non- webrequestmatcher matching requests. that do not match; never null
create a new mockmvcwebclientbuilder based on the supplied instance from; never null
create a new mockmvchtmlunitdriverbuilder based on the supplied instance from (never null)
supply the webconnectionhtmlunitdriver that the driver processing non- webrequestmatcher matching requests. for requests that do not match (never null)
create a new mockmultiparthttpservletrequest based on the supplied servletcontext and the mockmultipartfiles added to this builder.
update the contextpath, servletpath, and pathinfo of the request.
merges the properties of the "parent" requestbuilder accepting values only if not already set in "this" instance.
set session attributes.
set flash attributes.
set the 'content-type' header of the request.
set the 'accept' header to the given media type(s).
add a map of request parameters to the mockhttpservletrequest, for example when testing a form submission. if called more than once, new values get added to existing ones.
build a mockhttpservletrequest.
create a requestbuilder for an async dispatch from the usage involves performing a request that starts async processing first:  class="code"> mvcresult mvcresult = this.mockmvc.perform(get("1")) .andexpect(request().asyncstarted()) .andreturn();  and then performing the async dispatch re-using the mvcresult:  class="code"> this.mockmvc.perform(asyncdispatch(mvcresult)) .andexpect(status().isok()) .andexpect(content().contenttype(mediatype.application_json)) .andexpect(content().string("\"name\":\"joe\",\"somedouble\":0.0,\"someboolean\":false")); 
assert the type of the handler that processed the request.
assert the controller method used to process the request. the expected method is specified through a "mock" controller method invocation similar to mvcuricomponentsbuilder#frommethodcall(object). for example, given this controller:  class="code"> @restcontroller public class simplecontroller @requestmapping("") public responseentity<void> handle() return responseentity.ok().build();  a test that has statically imported mvcuricomponentsbuilder#on can be performed as follows:  class="code"> mockmvc.perform(get("")) .andexpect(handler().methodcall(on(simplecontroller.class).handle()));  or the "mock" controller itself after an invocation
assert a session attribute value with the given hamcrest matcher.
assert the primary value of the named response header parsed into a date using the preferred date format described in rfc 7231. the resultmatcher returned by this method throws an header, or if the supplied value does not match the primary value.
assert the primary value of the named response header as a long. the resultmatcher returned by this method throws an header, or if the supplied value does not match the primary value.
protected constructor, not for direct instantiation. use formatting specifiers defined in string#format(string, object...)
print the modelandview.
print the supplied cookies in a human-readable form, assuming the
print "output" flash attributes.
print the request.
print the response.
access to response body assertions using an xpath expression to inspect a specific subset of the body. the xpath expression can be a parameterized string using formatting specifiers as defined in string#format(string, object...).
assert the model has no errors.
assert the number of model attributes.
assert a field error code for a model attribute using a org.hamcrest.matcher.
assert the given model attribute field(s) have errors.
assert a field error code for a model attribute using exact string match.
resolve the testcontextbootstrapper type for the test class in the supplied bootstrapcontext, instantiate it, and provide it a reference to the bootstrapcontext. if the bootstrapwith @bootstrapwith annotation is present on the test class, either directly or as a meta-annotation, then its otherwise, either the defaulttestcontextbootstrapper or the webtestcontextbootstrapper will be used, depending on the presence of
create the bootstrapcontext for the specified class test class. uses reflection to create a org.springframework.test.context.support.defaultbootstrapcontext that uses a org.springframework.test.context.cache.defaultcacheawarecontextloaderdelegate.
provide a string representation of the context configuration attributes and declaring class.
construct a new contextconfigurationattributes instance for the corresponding attributes.
attempt to create a copy of the supplied testcontext using its copy constructor.
hook for post-processing a test after execution of after lifecycle callbacks of the underlying test framework  for example, tearing down test fixtures, ending a transaction, etc. this method must be called immediately after framework-specific after lifecycle callbacks (e.g., methods annotated with junit 4's org.junit.after @after). for historical reasons, this method is named aftertestmethod. since the introduction of #aftertestexecution, a more suitable name for this method might be something like aftertestteardown or this method due to backward compatibility concerns. the managed testcontext will be updated with the supplied each registered testexecutionlistener will be given a chance to perform its post-processing. if a listener throws an exception, the remaining registered listeners will still be called. after all listeners have executed, the first caught exception will be rethrown with any subsequent exceptions throwable#addsuppressed suppressed in the first exception. note that registered listeners will be executed in the opposite test instance method or by a testexecutionlistener, or null if none was thrown
get a copy of the testexecutionlistener testexecutionlisteners registered for this testcontextmanager in reverse order.
hook for pre-processing a test immediately before execution of the java.lang.reflect.method test method in the supplied or logging purposes. this method must be called after framework-specific before lifecycle callbacks (e.g., methods annotated with junit 4's the managed testcontext will be updated with the supplied an attempt will be made to give each registered if a listener throws an exception, however, the remaining registered listeners will not be called. test instance
hook for post-processing a test immediately after execution of the java.lang.reflect.method test method in the supplied or logging purposes. this method must be called before framework-specific after lifecycle callbacks (e.g., methods annotated with junit 4's the managed testcontext will be updated with the supplied each registered testexecutionlistener will be given a chance to perform its post-processing. if a listener throws an exception, the remaining registered listeners will still be called. after all listeners have executed, the first caught exception will be rethrown with any subsequent exceptions throwable#addsuppressed suppressed in the first exception. note that registered listeners will be executed in the opposite order in which they were registered. test instance test method or by a testexecutionlistener, or null if none was thrown
hook for post-processing a test class after execution of all tests within the class. should be called after any framework-specific after class methods (e.g., methods annotated with junit 4's each registered testexecutionlistener will be given a chance to perform its post-processing. if a listener throws an exception, the remaining registered listeners will still be called. after all listeners have executed, the first caught exception will be rethrown with any subsequent exceptions throwable#addsuppressed suppressed in the first exception. note that registered listeners will be executed in the opposite
hook for pre-processing a test before execution of before lifecycle callbacks of the underlying test framework  for example, setting up test fixtures, starting a transaction, etc. this method must be called immediately prior to framework-specific before lifecycle callbacks (e.g., methods annotated with junit 4's org.junit.before @before). for historical reasons, this method is named beforetestmethod. since the introduction of #beforetestexecution, a more suitable name for this method might be something like beforetestsetup or this method due to backward compatibility concerns. the managed testcontext will be updated with the supplied an attempt will be made to give each registered if a listener throws an exception, however, the remaining registered listeners will not be called. test instance
register the supplied array of testexecutionlistener testexecutionlisteners by appending them to the list of listeners used by this testcontextmanager.
hook for pre-processing a test class before execution of any tests within the class. should be called prior to any framework-specific before class methods (e.g., methods annotated with junit 4's an attempt will be made to give each registered execution. if a listener throws an exception, however, the remaining registered listeners will not be called. exception
hook for preparing a test instance prior to execution of any individual test methods, for example for injecting dependencies, etc. should be called immediately after instantiation of the test instance. the managed testcontext will be updated with the supplied an attempt will be made to give each registered listener throws an exception, however, the remaining registered listeners will not be called.
provide a string representation of the #gettestclass() test class, the name of the #getcontextloader() contextloader, and the
determine if the supplied object is equal to this mergedcontextconfiguration instance by comparing both object's #getlocations() locations,
load the applicationcontext for the supplied merged context configuration. supports both the smartcontextloader and contextloader spis.
generate a text string containing the implementation type of this cache and its statistics. the string returned by this method contains all information required for compliance with the contract for #logstatistics().
execute the given sql script. use with caution outside of a transaction! the script will normally be loaded by classpath. do not use this method to execute ddl if you expect rollback. exception in the event of an error
construct a new springjunit4classrunner and initialize a standard junit tests.
wrap the statement returned by the parent implementation with a default functionality while adding support for the spring testcontext framework.
augment the default junit behavior execution chain. furthermore, support for timeouts has been moved down the execution chain in order to include execution of org.junit.before @before and org.junit.after @after methods within the timed execution. note that this differs from the default junit behavior of executing executing the actual test method in a separate thread. thus, the net effect is that @before and @after methods will be executed in the same thread as the test method. as a consequence, junit-specified timeouts will work fine in combination with spring transactions. however, junit-specific timeouts still differ from spring-specific timeouts in that the former execute in a separate thread while the latter simply execute in the main thread (like regular tests).
wrap the statement returned by the parent implementation with a junit functionality while adding support for the spring testcontext framework.
return true if ignore @ignore is present for the supplied via @ifprofilevalue.
get the exception that the supplied frameworkmethod test method is expected to throw. supports junit's test#expected() @test(expected=...) annotation. can be overridden by subclasses.
perform the same logic as but with additional support for spring's @timed annotation. supports both spring's org.springframework.test.annotation.timed @timed and junit's test#timeout() @test(timeout=...) annotations, but not both simultaneously. or the supplied statement as appropriate
perform the same logic as except that the expected exception is retrieved using
wrap the statement returned by the parent implementation with a default junit functionality while adding support for the spring testcontext framework.
perform the same logic as except that tests are determined to be ignored by
check whether the test is enabled in the current execution environment. this prevents classes with a non-matching @ifprofilevalue annotation from running altogether, even skipping the execution of
wrap the statement returned by the parent implementation with a default functionality while adding support for the spring testcontext framework.
apply class-level features of the spring testcontext framework to the supplied base statement. specifically, this method retrieves the testcontextmanager used by this rule and its associated springmethodrule and invokes the testcontextmanager#beforetestclass() beforetestclass() and testcontextmanager#aftertestclass() aftertestclass() methods on the testcontextmanager. in addition, this method checks whether the test is enabled in the current execution environment. this prevents classes with a non-matching @ifprofilevalue annotation from running altogether, even skipping the execution of beforetestclass() methods in testexecutionlisteners. features of the spring testcontext framework
apply instance-level and method-level features of the spring testcontext framework to the supplied base statement. specifically, this method invokes the on the testcontextmanager, potentially with spring timeouts and repetitions. in addition, this method checks whether the test is enabled in the current execution environment. this prevents methods with a non-matching @ifprofilevalue annotation from running altogether, even skipping the execution of preparetestinstance() methods in testexecutionlisteners. and method-level features of the spring testcontext framework
wrap the supplied statement with a runaftertestmethodcallbacks statement.
wrap the supplied statement with a runpreparetestinstancecallbacks statement.
wrap the supplied statement with a runbeforetestmethodcallbacks statement.
evaluate the next statement statement in the execution chain repeatedly, using the specified repeat count.
evaluate the next statement in the execution chain (typically an instance of exceptions thrown, and then invoke testcontextmanager#aftertestclass(). if the invocation of aftertestclass() throws an exception, it will also be tracked. multiple exceptions will be combined into a multiplefailureexception.
evaluate the next statement in the execution chain (typically an instance of exceptions thrown, and then invoke first caught exception (if any). if the invocation of aftertestmethod() throws an exception, that exception will also be tracked. multiple exceptions will be combined into a
evaluate the next statement statement in the execution chain (typically an instance of springrepeat) and throw a than the specified timeout.
determine if the test specified by arguments to the the current environment, as configured via the ifprofilevalue @ifprofilevalue annotation. if the test is not annotated with @ifprofilevalue it is considered enabled. if a test is not enabled, this method will abort further evaluation of the execution chain with a failed assumption; otherwise, this method will simply evaluate the next statement in the execution chain.
evaluate the next statement in the execution chain (typically an instance of runbeforetestexecutioncallbacks), catching any exceptions thrown, and then invoke testcontextmanager#aftertestexecution supplying the first caught exception (if any). if the invocation of aftertestexecution() throws an exception, that exception will also be tracked. multiple exceptions will be combined into a
ensure that the supplied webmergedcontextconfiguration does not contain mergedcontextconfiguration#getclasses() classes.
load bean definitions into the supplied genericwebapplicationcontext context from the locations in the supplied webmergedcontextconfiguration, using an
returns a webmergedcontextconfiguration if the test class in the supplied mergedcontextconfiguration is annotated with the supplied instance unmodified.
load bean definitions into the supplied genericwebapplicationcontext context from the locations in the supplied webmergedcontextconfiguration using a
resource types for detection of defaults. consequently, this method is not supported.
not as a legacy org.springframework.test.context.contextloader contextloader. consequently, this method is not supported.
not as a legacy org.springframework.test.context.contextloader contextloader. consequently, this method is not supported.
ensure that the supplied webmergedcontextconfiguration does not contain mergedcontextconfiguration#getlocations() locations.
not as a legacy org.springframework.test.context.contextloader contextloader. consequently, this method is not supported.
register classes in the supplied genericwebapplicationcontext context from the classes in the supplied webmergedcontextconfiguration. each class must represent an annotated class. an bean definitions.
process annotated classes in the supplied contextconfigurationattributes. if the annotated classes are null or empty and #detectdefaultconfigurationclasses detect default configuration classes. if defaults are detected they will be supplied configuration attributes. otherwise, properties in the supplied configuration attributes will not be modified.
if the #reset_request_context_holder_attribute in the supplied (1) clean up thread-local state after each test method by requestcontextholder#resetrequestattributes() resetting spring web's into the test instance for subsequent tests by setting the in the test context to true. the #reset_request_context_holder_attribute and removed from the test context, regardless of their values.
provide a string representation of the #gettestclass() test class,
load a spring webapplicationcontext from the supplied implementation details:  calls #validatemergedcontextconfiguration(webmergedcontextconfiguration) to allow subclasses to validate the supplied configuration before proceeding. creates a genericwebapplicationcontext instance. if the supplied mergedcontextconfiguration references a the corresponding mergedcontextconfiguration#getparentapplicationcontext() applicationcontext will be retrieved and for the context created by this method. delegates to #configurewebresources to create the calls #preparecontext to allow for customizing the context before bean definitions are loaded. calls #customizebeanfactory to allow for customizing the context's defaultlistablebeanfactory. delegates to #loadbeandefinitions to populate the context from the locations or classes in the supplied mergedcontextconfiguration. delegates to annotationconfigutils for annotation configuration processors. calls #customizecontext to allow for customizing the context before it is refreshed.  configurableapplicationcontext#refresh refreshes the context and registers a jvm shutdown hook for it. 
not as a legacy org.springframework.test.context.contextloader contextloader. consequently, this method is not supported.
configures web resources for the supplied web application context (wac). implementation details if the supplied wac has no parent or its parent is not a wac, the supplied wac will be configured as the root wac (see "root wac configuration" below). otherwise the context hierarchy of the supplied wac will be traversed to find the top-most wac (i.e., the root); and the servletcontext of the root wac will be set as the servletcontext for the supplied wac. root wac configuration  the resource base path is retrieved from the supplied a resourceloader is instantiated for the mockservletcontext: if the resource base path is prefixed with " classpath:", a a mockservletcontext will be created using the resource base path and resource loader. the supplied genericwebapplicationcontext is then stored in the mockservletcontext under the finally, the mockservletcontext is set in the 
resolve the dependency for the supplied parameter from the supplied applicationcontext. provides comprehensive autowiring support for individual method parameters on par with spring's dependency injection facilities for autowired fields and methods, including support for autowired @autowired, placeholders and spel expressions in @value declarations. the dependency is required unless the parameter is annotated with flag set to false. if an explicit qualifier is not declared, the name of the parameter will be used as the qualifier for resolving ambiguities. differ from the class that declares the parameter in that it may be a subclass thereof, potentially substituting type variables dependency
due to a bug in javac on jdk versions prior to jdk 9, looking up annotations directly on a parameter will fail for inner class constructors. bug in javac in jdk < 9 the parameter annotations array in the compiled byte code excludes an entry for the implicit enclosing instance parameter for an inner class constructor. workaround this method provides a workaround for this off-by-one error by allowing the caller to access annotations on the preceding parameter object (i.e., an empty annotatedelement. warning the annotatedelement returned by this method should never be cast and treated as a parameter since the metadata (e.g., parameter#getname(), at the given index in an inner class constructor. if the aforementioned bug is in effect
resolve a value for the parameter in the supplied parametercontext by retrieving the corresponding dependency from the test's applicationcontext. delegates to parameterautowireutils#resolvedependency.
delegates to testcontextmanager#beforetestmethod.
get the testcontextmanager associated with the supplied extensioncontext.
delegates to testcontextmanager#aftertestexecution.
determine if the value for the parameter in the supplied parametercontext should be autowired from the test's applicationcontext. returns true if the parameter is declared in a constructor that is annotated with autowired @autowired and otherwise delegates to warning: if the parameter is declared in a constructor that is annotated with @autowired, spring will assume the responsibility for resolving all parameters in the constructor. consequently, no other registered
delegates to testcontextmanager#beforetestexecution.
delegates to testcontextmanager#aftertestmethod.
evaluate the expression configured via the supplied annotation type on the annotatedelement for the supplied extensioncontext. the annotation annotation flag from the annotation should be conditionevaluationresult#enabled enabled if the expression evaluates to true or test should be enabled; otherwise conditionevaluationresult#disabled disabled
delegates to the ihookcallback#runtestmethod(itestresult) test method in the supplied callback to execute the actual test and then tracks the exception thrown during test execution, if any.
execute the given sql script. use with caution outside of a transaction! the script will normally be loaded by classpath. do not use this method to execute ddl if you expect rollback. exception in the event of an error
resolve the bean definition profiles for the given class test class based on profiles configured declaratively via never null
load a spring applicationcontext from the supplied mergedcontextconfiguration. implementation details:  calls #validatemergedcontextconfiguration(mergedcontextconfiguration) to allow subclasses to validate the supplied configuration before proceeding. creates a genericapplicationcontext instance. if the supplied mergedcontextconfiguration references a the corresponding mergedcontextconfiguration#getparentapplicationcontext() applicationcontext will be retrieved and for the context created by this method. calls #preparecontext(genericapplicationcontext) for backwards compatibility with the org.springframework.test.context.contextloader contextloader spi. calls #preparecontext(configurableapplicationcontext, mergedcontextconfiguration) to allow for customizing the context before bean definitions are loaded. calls #customizebeanfactory(defaultlistablebeanfactory) to allow for customizing the context's defaultlistablebeanfactory. delegates to #loadbeandefinitions(genericapplicationcontext, mergedcontextconfiguration) to populate the context from the locations or classes in the supplied delegates to annotationconfigutils for annotation configuration processors. calls #customizecontext(genericapplicationcontext) to allow for customizing the context before it is refreshed. calls #customizecontext(configurableapplicationcontext, mergedcontextconfiguration) to allow for customizing the context before it is refreshed.  configurableapplicationcontext#refresh refreshes the context and registers a jvm shutdown hook for it. 
load a spring applicationcontext from the supplied locations. implementation details:  creates a genericapplicationcontext instance. calls #preparecontext(genericapplicationcontext) to allow for customizing the context before bean definitions are loaded. calls #customizebeanfactory(defaultlistablebeanfactory) to allow for customizing the context's defaultlistablebeanfactory. delegates to #createbeandefinitionreader(genericapplicationcontext) to create a from the specified locations. delegates to annotationconfigutils for annotation configuration processors. calls #customizecontext(genericapplicationcontext) to allow for customizing the context before it is refreshed.  configurableapplicationcontext#refresh refreshes the context and registers a jvm shutdown hook for it.  note: this method does not provide a means to set active bean definition profiles for the loaded context. see #loadcontext(mergedcontextconfiguration) and abstractcontextloader#preparecontext(configurableapplicationcontext, mergedcontextconfiguration) for an alternative.
provide a string representation of this bootstrap context's state.
provide a string representation of this test context's state.
for backwards compatibility with the contextloader spi, the default implementation simply delegates to #processlocations(class, string...), passing it the contextconfigurationattributes#getdeclaringclass() declaring class and contextconfigurationattributes#getlocations() resource locations retrieved from the supplied processed locations are then the supplied configuration attributes. can be overridden in subclasses  for example, to process annotated classes instead of resource locations.
prepare the configurableapplicationcontext created by this the default implementation:  sets the active bean definition profiles from the supplied context. adds propertysource propertysources for all resource locations and inlined properties from the supplied mergedcontextconfiguration to the environment of the context. determines what (if any) context initializer classes have been supplied via the mergedcontextconfiguration and instantiates and given application context.  any applicationcontextinitializers implementing org.springframework.core.annotation.order @order will be sorted appropriately.   
generate the default classpath resource locations array based on the supplied class. for example, if the supplied class is com.example.mytest, the generated locations will contain a single string with a value of is the value of the first configured generated location actually exists in the classpath. as of spring 3.1, the implementation of this method adheres to the contract defined in the smartcontextloader spi. specifically, this method will preemptively verify that the generated default location actually exists. if it does not exist, this method will log a warning and return an empty array. subclasses can override this method to implement a different default location generation strategy.
customize the configurableapplicationcontext created by this into the context but before the context has been refreshed. the default implementation delegates to all that have been registered with the supplied mergedconfig.
ensure that the supplied mergedcontextconfiguration does not contain mergedcontextconfiguration#getclasses() classes.
resolve active bean definition profiles for the supplied class. note that the activeprofiles#inheritprofiles inheritprofiles flag of specifically, if the inheritprofiles flag is set to true, profiles defined in the test class will be merged with those defined in superclasses. profiles from superclasses if appropriate (never null)
detect the default configuration classes for the supplied test class. the returned class array will contain all static nested classes of the supplied class that meet the requirements for @configuration class implementations as specified in the documentation for the implementation of this method adheres to the contract defined in the spi. specifically, this method uses introspection to detect default configuration classes that comply with the constraints required of configuration class does not meet these requirements, this method will log a debug message, and the potential candidate class will be ignored. never null
resolve the contextloader class class to use for the supplied list of contextconfigurationattributes and then instantiate and return that contextloader. if the user has not explicitly declared which loader to use, the value returned from #getdefaultcontextloaderclass will be used as the default context loader class. for details on the class resolution process, see #resolveexplicitcontextloaderclass and resolved; must not be null not be null; must be ordered bottom-up (i.e., as if we were traversing up the class hierarchy) (never null) returns null
build the mergedcontextconfiguration merged context configuration for the supplied class testclass, context configuration attributes, and parent context configuration. should be built (must not be null) specified test class, ordered bottom-up (i.e., as if we were traversing up the class hierarchy); never null or empty context in a context hierarchy, or null if there is no parent be passed to the mergedcontextconfiguration constructor initializers are required; typically true but may be set to false if the configured loader supports empty configuration
get the names of the default testexecutionlistener classes for this bootstrapper. the default implementation looks up all configured in all meta-infspring.factories files on the classpath. this method is invoked by #getdefaulttestexecutionlistenerclasses(). classes
build a new defaulttestcontext using the class test class in the bootstrapcontext associated with this bootstrapper and by delegating to #buildmergedcontextconfiguration() and concrete subclasses may choose to override this method to return a custom testcontext implementation.
resolve the contextloader class class to use for the supplied list of contextconfigurationattributes. beginning with the first level in the context configuration attributes hierarchy:  if the contextconfigurationattributes#getcontextloaderclass() contextloaderclass property of contextconfigurationattributes is configured with an explicit class, that class will be returned. if an explicit contextloader class is not specified at the current level in the hierarchy, traverse to the next level in the hierarchy and return to step #1.  must not be null; must be ordered bottom-up (i.e., as if we were traversing up the class hierarchy) attributes, or null if no explicit loader is found
get the default testexecutionlistener classes for this bootstrapper. this method is invoked by #gettestexecutionlisteners() and delegates to #getdefaulttestexecutionlistenerclassnames() to retrieve the class names. if a particular class cannot be loaded, a debug message will be logged, but the associated exception will not be rethrown.
resource types for detection of defaults. consequently, this method is not supported.
ensure that the supplied mergedcontextconfiguration does not contain mergedcontextconfiguration#getclasses() classes.
resolve the set of merged applicationcontextinitializer classes for the supplied list of contextconfigurationattributes. note that the contextconfiguration#inheritinitializers inheritinitializers flag of contextconfiguration @contextconfiguration will be taken into consideration. specifically, if the inheritinitializers flag is set to configuration attributes, context initializer classes defined at the given level will be merged with those defined in higher levels of the class hierarchy. not be null or empty; must be ordered bottom-up (i.e., as if we were traversing up the class hierarchy) superclasses if appropriate (never null)
not as a legacy org.springframework.test.context.contextloader contextloader. consequently, this method is not supported.
not as a legacy org.springframework.test.context.contextloader contextloader. consequently, this method is not supported.
register classes in the supplied genericapplicationcontext context from the classes in the supplied mergedcontextconfiguration. each class must represent an annotated class. an bean definitions. note that this method does not call #createbeandefinitionreader since annotatedbeandefinitionreader is not an instance of
process annotated classes in the supplied contextconfigurationattributes. if the annotated classes are null or empty and #detectdefaultconfigurationclasses detect default configuration classes. if defaults are detected they will be supplied configuration attributes. otherwise, properties in the supplied configuration attributes will not be modified.
ensure that the supplied mergedcontextconfiguration does not contain mergedcontextconfiguration#getlocations() locations.
not as a legacy org.springframework.test.context.contextloader contextloader. consequently, this method is not supported.
not as a legacy org.springframework.test.context.contextloader contextloader. consequently, this method is not supported.
build a context hierarchy map for the supplied class test class and its superclasses, taking into account context hierarchies declared via contexthierarchy @contexthierarchy and each value in the map represents the consolidated list of contextconfigurationattributes context configuration attributes for a given level in the context hierarchy (potentially across the test class hierarchy), keyed by the contextconfiguration#name() name of the context hierarchy level. if a given level in the context hierarchy does not have an explicit name (i.e., configured via contextconfiguration#name), a name will be generated for that hierarchy level by appending the numerical level to the #generated_context_hierarchy_level_prefix. (must not be null) keyed by context hierarchy level name; never null attributes for each level in the @contexthierarchy do not define unique context configuration within the overall hierarchy.
convenience method for creating a contextconfigurationattributes instance from the supplied contextconfiguration annotation and declaring class and then adding the attributes to the supplied list.
resolve the list of lists of contextconfigurationattributes context configuration attributes for the supplied class test class and its superclasses, taking into account context hierarchies declared via the outer list represents a top-down ordering of context configuration attributes, where each element in the list represents the context configuration declared on a given test class in the class hierarchy. each nested list contains the context configuration attributes declared either via a single instance of @contextconfiguration on the particular class or via multiple instances of @contextconfiguration declared within a single @contexthierarchy instance on the particular class. furthermore, each nested list maintains the order in which note that the contextconfiguration#inheritlocations inheritlocations and be taken into consideration. if these flags need to be honored, that must be handled manually when traversing the nested lists returned by this method. (must not be null) never null neither @contextconfiguration nor @contexthierarchy is present on the supplied class in the class hierarchy declares both @contextconfiguration and
resolve the list of contextconfigurationattributes context configuration attributes for the supplied class test class and its superclasses. note that the contextconfiguration#inheritlocations inheritlocations and be taken into consideration. if these flags need to be honored, that must be handled manually when traversing the list returned by this method. (must not be null) bottom-up (i.e., as if we were traversing up the class hierarchy); never null
delegates to an appropriate candidate smartcontextloader to load an applicationcontext. delegation is based on explicit knowledge of the implementations of the default loaders for #getxmlloader() xml configuration files and groovy scripts and #getannotationconfigloader() annotated classes. specifically, the delegation algorithm is as follows:  if the resource locations in the supplied mergedcontextconfiguration are not empty and the annotated classes are empty, the xml-based loader will load the applicationcontext. if the annotated classes in the supplied mergedcontextconfiguration are not empty and the resource locations are empty, the annotation-based loader will load the applicationcontext. 
delegates to candidate smartcontextloaders to process the supplied delegation is based on explicit knowledge of the implementations of the default loaders for #getxmlloader() xml configuration files and groovy scripts and #getannotationconfigloader() annotated classes. specifically, the delegation algorithm is as follows:  if the resource locations or annotated classes in the supplied candidate loader will be allowed to process the configuration as is, without any checks for detection of defaults. otherwise, the xml-based loader will be allowed to process the configuration in order to detect default resource locations. if the xml-based loader detects default resource locations, an info message will be logged. subsequently, the annotation-based loader will be allowed to process the configuration in order to detect default configuration classes. if the annotation-based loader detects default configuration classes, an info message will be logged.  resource locations and annotated classes configuration classes; if the annotation-based loader detects default resource locations; if neither candidate loader detects defaults for the supplied context configuration; or if both candidate loaders detect defaults for the supplied context configuration
performs dependency injection on the and the test instance via its own checking dependencies). the #reinject_dependencies_attribute will be subsequently removed from the test context, regardless of its value.
performs dependency injection and bean initialization for the supplied the #reinject_dependencies_attribute will be subsequently removed from the test context, regardless of its value. be performed (never null)
if the #reinject_dependencies_attribute in the supplied this method will have the same effect as otherwise, this method will have no effect.
add the given inlined properties (in the form of key-value pairs) to the supplied configurableenvironment environment. all key-value pairs will be added to the environment as a single mappropertysource with the highest precedence. for details on the parsing of inlined properties, consult the javadoc for #convertinlinedpropertiestomap. potentially empty but never null
convert the supplied inlined properties (in the form of key-value pairs) into a map keyed by property name, preserving the ordering of property names in the returned map. parsing of the key-value pairs is achieved by converting all pairs into virtual properties files in memory and delegating to for a full discussion of inlined properties, consult the javadoc for testpropertysource#properties. but never null a given inlined property contains multiple key-value pairs
add the properties files from the given resource locations to the supplied configurableenvironment environment. property placeholders in resource locations (i.e., $...) will be environment#resolverequiredplaceholders(string) resolved against the environment. each properties file will be converted to a resourcepropertysource that will be added to the propertysources of the environment with highest precedence. never null to the environment; potentially empty but never null
perform the actual work for #beforetestmethod and #aftertestmethod by dirtying the context if appropriate (i.e., according to the required modes). potentially be marked as dirty; never null be marked dirty in the current phase; never null be marked dirty in the current phase; never null
perform the actual work for #beforetestclass and #aftertestclass by dirtying the context if appropriate (i.e., according to the required mode). potentially be marked as dirty; never null be marked dirty in the current phase; never null
detect a default properties file for the supplied class, as specified in the class-level javadoc for testpropertysource.
provide a string representation of the @testpropertysource attributes and declaring class.
convert the supplied paths to classpath resource paths. for each of the supplied paths:  a plain path  for example, "context.xml"  will be treated as a classpath resource that is relative to the package in which the specified class is defined. a path starting with a slash will be treated as an absolute path within the classpath, for example: "orgexampleschema.sql". a path which is prefixed with a url protocol (e.g., 
retrieve the datasource to use for the supplied testcontext test context. the following algorithm is used to retrieve the datasource from the org.springframework.context.applicationcontext applicationcontext of the supplied test context:  look up the datasource by type and name, if the supplied attempt to look up the single datasource by type. attempt to look up the primary datasource by type. attempt to look up the datasource by type and the  should be retrieved; never null (may be null or empty) named datasource
retrieve the platformtransactionmanager transaction manager to use for the supplied testcontext test context. the following algorithm is used to retrieve the transaction manager from the org.springframework.context.applicationcontext applicationcontext of the supplied test context:  look up the transaction manager by type and explicit name, if the supplied transaction manager does not exist. attempt to look up the single transaction manager by type. attempt to look up the primary transaction manager by type. attempt to look up the transaction manager via a attempt to look up the transaction manager by type and the name.  should be retrieved; never null (may be null or empty) named transaction manager exists in the applicationcontext
create a delegating transactionattribute for the supplied target the test class and test method to build the name of the transaction.
determine whether a test-managed transaction is currently active.
start a new transaction for the configured test context. only call this method if #endtransaction has been called or if no transaction has been previously started.
immediately force a commit or rollback of the transaction for the configured test context, according to the #isflaggedforrollback rollback flag.
run all aftertransaction @aftertransaction methods for the specified testcontext test context. if one of the methods fails, the caught exception will be logged as an error, and the remaining methods will be given a chance to execute. after all methods have executed, the first caught exception, if any, will be rethrown.
determine whether or not to rollback transactions for the supplied possible method-level override via the rollback @rollback annotation. should be retrieved
if a transaction is currently active for the supplied and run aftertransaction @aftertransaction methods.  @aftertransaction methods are guaranteed to be invoked even if an error occurs while ending the transaction.
if the test method of the supplied testcontext test context is configured to run within a transaction, this method will run transaction. note that if a @beforetransaction method fails, any remaining will not be started.
run all beforetransaction @beforetransaction methods for the specified testcontext test context. if one of the methods fails, however, the caught exception will be rethrown in a wrapped be given a chance to execute.
determine whether or not to rollback transactions by default for the supplied testcontext test context. supports rollback @rollback or commit @commit at the class-level. should be retrieved
get the platformtransactionmanager transaction manager to use for the supplied testcontext test context and qualifier. delegates to #gettransactionmanager(testcontext) if the supplied qualifier is null or empty. should be retrieved may be null or empty
provide a string representation of the merged sql script configuration.
construct a mergedsqlconfig instance by merging the configuration from the supplied local (potentially method-level) @sqlconfig annotation with class-level configuration discovered on the supplied testclass. local configuration overrides class-level configuration. if the test class is not annotated with @sqlconfig, no merging takes place and the local configuration is used "as is".
detect a default sql script by implementing the algorithm defined in
execute the sql scripts configured via the supplied sql @sql annotation for the given executionphase and testcontext. special care must be taken in order to properly support the configured
execute sql scripts configured via sql @sql for the supplied
determine if the value (or one of the values) in the supplied ifprofilevalue @ifprofilevalue annotation is enabled in the current environment. the test is enabled environment or if the supplied ifprofilevalue is
retrieves the profilevaluesource type for the specified @profilevaluesourceconfiguration annotation and instantiates a new instance of that type. if profilevaluesourceconfiguration @profilevaluesourceconfiguration is not present on the specified class or if a custom profilevaluesource is not declared, the default systemprofilevaluesource will be returned instead. be retrieved class
parse the given xml content to a document.
parse the expected and actual content strings as xml and assert that the two are "similar" -- i.e. they contain the same elements and attributes regardless of order. use of this method assumes the  href="https:github.comxmlunitxmlunit">xmlunit library is available.
throws an unsupportedoperationexception since the type of annotation represented by the #getannotationattributes annotationattributes in an untypedannotationdescriptor is unknown.
provide a textual representation of this annotationdescriptor.
find the annotationdescriptor for the supplied annotationtype on the supplied class, traversing its annotations, interfaces, and superclasses if no annotation can be found on the given class itself. this method explicitly handles class-level annotations which are not declared as java.lang.annotation.inherited inherited as well as meta-annotations. the algorithm operates as follows:  search for the annotation on the given class and return a corresponding recursively search through all annotations that the given class declares. recursively search through all interfaces implemented by the given class. recursively search through the superclass hierarchy of the given class.  in this context, the term recursively means that the search process continues by returning to step #1 with the current annotation, interface, or superclass as the class to look for annotations on. otherwise null
perform the search algorithm for #findannotationdescriptor(class, class), avoiding endless recursion by tracking which annotations have already been visited. otherwise null
find the untypedannotationdescriptor for the first class in the inheritance hierarchy of the specified clazz (including the specified clazz itself) which declares at least one of the specified annotationtypes. this method traverses the annotations, interfaces, and superclasses of the specified clazz if no annotation can be found on the given class itself. this method explicitly handles class-level annotations which are not declared as java.lang.annotation.inherited inherited as well as meta-annotations. the algorithm operates as follows:  search for a local declaration of one of the annotation types on the given class and return a corresponding untypedannotationdescriptor if found. recursively search through all annotations that the given class declares. recursively search through all interfaces implemented by the given class. recursively search through the superclass hierarchy of the given class.  in this context, the term recursively means that the search process continues by returning to step #1 with the current annotation, interface, or superclass as the class to look for annotations on. was found; otherwise null
perform the search algorithm for #findannotationdescriptorfortypes(class, class...), avoiding endless recursion by tracking which annotations have already been visited. was found; otherwise null
invoke the setter method with the given name on the supplied target object with the supplied value. this method traverses the class hierarchy in search of the desired method. in addition, an attempt will be made to make non- public methods accessible, thus allowing one to invoke protected, in addition, this method supports javabean-style property names. for example, if you wish to set the name property on the target object, you may pass either "name" or "setname" as the method name. method property name
get the value of the field field with the given name from the provided targetobject targetclass. if the supplied targetobject is a proxy, it will be aoptestutils#getultimatetargetobject unwrapped allowing the field to be retrieved from the ultimate target of the proxy. this method traverses the class hierarchy in search of the desired field. in addition, an attempt will be made to make non- public fields accessible, thus allowing one to get protected, be null if the field is an instance field
set the field field with the given name type on the provided targetobject targetclass to the supplied if the supplied targetobject is a proxy, it will be aoptestutils#getultimatetargetobject unwrapped allowing the field to be set on the ultimate target of the proxy. this method traverses the class hierarchy in search of the desired field. in addition, an attempt will be made to make non- public fields accessible, thus allowing one to set protected, be null if the field is an instance field
invoke the getter method with the given name on the supplied target object with the supplied value. this method traverses the class hierarchy in search of the desired method. in addition, an attempt will be made to make non- public methods accessible, thus allowing one to invoke protected, in addition, this method supports javabean-style property names. for example, if you wish to get the name property on the target object, you may pass either "name" or "getname" as the method name. method property name
invoke the method with the given name on the supplied target object with the supplied arguments. this method traverses the class hierarchy in search of the desired method. in addition, an attempt will be made to make non- public methods accessible, thus allowing one to invoke protected,
assert two objects are not equal and raise an assertionerror otherwise. for example:  class="code"> assertnotequals("response header [" + name + "]", expected, actual); 
get the target object of the supplied candidate object. if the supplied candidate is a spring be returned; otherwise, the candidate will be returned as is. never null)
get the ultimate target object of the supplied candidate object, unwrapping not only a top-level proxy but also any number of nested proxies. if the supplied candidate is a spring nested proxies will be returned; otherwise, the candidate will be returned as is. never null)
variant of #evaluatejsonpath(string) with a target type. this can be useful for matching numbers reliably for example coercing an integer into a double.
evaluate the json path expression against the supplied content and assert that the result is equal to the expected value.
evaluate the json path and return the resulting value.
count the rows in the given table, using the provided where clause. if the provided where clause contains text, it will be prefixed with " where " and then appended to the generated select statement. for example, if the provided table name is "person" and the provided where clause is "name = 'bob' and age > 25", the resulting sql statement to execute will be
delete rows from the given table, using the provided where clause. if the provided where clause contains text, it will be prefixed with " where " and then appended to the generated delete statement. for example, if the provided table name is "person" and the provided where clause is "name = 'bob' and age > 25", the resulting sql statement to execute will be as an alternative to hard-coded values, the "?" placeholder can be used within the where clause, binding to the given arguments. to guess the corresponding sql type); may also contain sqlparametervalue objects which indicate not only the argument value but also the sql type and optionally the scale.
drop the specified tables.
delete all rows from the specified tables.
configure a custom handler to consume the response body. by default, response body content is consumed in full and cached for subsequent access in tests. use this option to take control over how the response body is consumed. when the body has been "written" (i.e. consumed).
default constructor.
create a mockjspwriter for the given response.
find a headervalueholder by name, ignoring casing.
constructor for a part with a filename and byte[] content.
serialize the attributes of this session into an object that can be turned into a byte array with standard java serialization.
clear all of this session's attributes.
create a new mockservletcontext using the supplied resource base path and resource loader. registers a mockrequestdispatcher for the servlet named @literal 'default'.
set the name of the default servlet. also #unregisternameddispatcher unregisters the current default it with a mockrequestdispatcher for the provided never null or empty
build a full resource location for the given path, prepending the resource base path of this mockservletcontext.
create new mockservletconfig.
create a new mockmultipartfile with the given content.
invoke registered filter filters andor servlet also saving the request and response.
return the primary value for the given header, if any. will return the first value in case of multiple values.
return all values for the given header as a list of strings. as of servlet 3.0, this method is also defined in httpservletresponse. as of spring 3.1, it returns a list of stringified values for servlet 3.0 compatibility. consider using #getheadervalues(string) for raw object access.
return the primary value for the given header as a string, if any. will return the first value in case of multiple values. as of servlet 3.0, this method is also defined in httpservletresponse. as of spring 3.1, it returns a stringified value for servlet 3.0 compatibility. consider using #getheadervalue(string) for raw object access.
return all values for the given header as a list of value objects.
obtain the underlying mockhttpservletresponse, unwrapping
add an array of values for the specified http parameter. if there are already one or more values registered for the given parameter name, the given values will be added to the end of the list.
add all provided parameters without replacing any existing values. to replace existing values, use
set all provided parameters replacing any existing values for the provided parameter names. to add without replacing existing values, use #addparameters(java.util.map).
return the long timestamp for the date header with the given name. if the internal value representation is a string, this method will try to parse it as a date using the supported date formats:  "eee, dd mmm yyyy hh:mm:ss zzz" "eee, dd-mmm-yy hh:mm:ss zzz" "eee mmm dd hh:mm:ss yyyy" 
add an http header entry for the given name. while this method can take any object as a parameter, it is recommended to use the following types:  string or any object to be converted using tostring(); see #getheader. string, number, or date for date headers; see #getdateheader. string or number for integer headers; see #getintheader.  string[] or collection for multiple values; see #getheaders. 
get the content of the request body as a string, using the configured
create a new mockhttpservletrequest with the supplied servletcontext, the preferred locale will be set to locale#english.
factory method that parses the value of a "set-cookie" header.
if the name is the expected name specified in the constructor, return the object provided in the constructor. if the name is unexpected, a respective namingexception gets thrown.
bind the given object to the given name. note: not intended for direct use by applications if setting up a jvm-level jndi environment. use simplenamingcontextbuilder to set up jndi bindings then.
look up the object with the given name. note: not intended for direct use by applications. will be used by any standard initialcontext jndi lookups.
bind the given object under the given name, for all naming contexts that this context builder will generate.
register the context builder by registering it with the jndi namingmanager. note that once this has been done, new initialcontext() will always return a context from this factory. use the emptyactivatedcontextbuilder() static method to get an empty context (for example, in test methods). registered with the jndi namingmanager
simple initialcontextfactorybuilder implementation, creating a new simplenamingcontext instance.
if no simplenamingcontextbuilder is already configuring jndi, create and activate one. otherwise take the existing activated simplenamingcontextbuilder, clear it and return it. this is mainly intended for test suites that want to reinitialize jndi bindings from scratch repeatedly. to control jndi bindings
delegate options requests to #processrequest, if desired. applies httpservlet's standard options processing otherwise, and also if there is still no 'allow' header set after dispatching.
initialize and publish the webapplicationcontext for this servlet. delegates to #createwebapplicationcontext for actual creation of the context. can be overridden in subclasses.
instantiate the webapplicationcontext for this servlet, either a default or a #setcontextclass custom context class, if set. this implementation expects custom contexts to implement the interface. can be overridden in subclasses. do not forget to register this servlet instance as application listener on the created context (for triggering its #onrefresh callback, and to call before returning the context instance.
build servletrequestattributes for the given request (potentially also holding a reference to the response), taking pre-bound attributes (and their type) into consideration. the previously bound instance (or not binding any, if none bound before)
refresh this servlet's application context, as well as the dependent state of the servlet.
delegate the webapplicationcontext before it is refreshed to any "contextinitializerclasses" servlet init-param. see also #postprocesswebapplicationcontext, which is designed to allow subclasses (as opposed to end-users) to modify the application context, and is called immediately before this method.
determine the username for the given request. the default implementation takes the name of the userprincipal, if any. can be overridden in subclasses.
process this request, publishing an event regardless of the outcome. the actual event handling is performed by the abstract
retrieve a webapplicationcontext from the servletcontext attribute with the #setcontextattribute configured name. the subclasses may override this method to provide a different
provide request parameters identifying the request for this flashmap.
delegates to the handler and interceptors' tostring().
apply prehandle methods of registered interceptors. next interceptor or the handler itself. else, dispatcherservlet assumes that this interceptor has already dealt with the response itself.
create a new handlerexecutionchain. (in the given order) before the handler itself executes
trigger aftercompletion callbacks on the mapped handlerinterceptors. will just invoke aftercompletion for all interceptors whose prehandle invocation has successfully completed and returned true.
apply afterconcurrenthandlerstarted callback on mapped asynchandlerinterceptors.
return the array of interceptors to apply (in the given order).
apply posthandle methods of registered interceptors.
create new servletconfigpropertyvalues. we can't accept default values
map config parameters onto bean properties of this servlet, and invoke subclass initialization. properties are missing), or if subclass initialization fails.
return the underlying modelmap instance (never null).
create a list of default strategy objects for the given strategy interface. the default implementation uses the "dispatcherservlet.properties" file (in the same package as the dispatcherservlet class) to determine the class names. it instantiates the strategy objects through the context's beanfactory.
return the handleradapter for this handler object.
process the actual dispatching to the handler. the handler will be obtained by applying the servlet's handlermappings in order. the handleradapter will be obtained by querying the servlet's installed handleradapters to find the first that supports the handler class. all http methods are handled by this method. it's up to handleradapters or handlers themselves to decide which methods are acceptable.
initialize the flashmapmanager used by this servlet instance. if no implementation is configured then we default to
convert the request into a multipart request, and make multipart resolver available. if no multipart resolver is set, simply use the existing request.
initialize the requesttoviewnametranslator used by this servlet instance. if no implementation is configured then we default to defaultrequesttoviewnametranslator.
restore the request attributes after an include.
check "javax.servlet.error.exception" attribute for a multipart exception.
determine an error modelandview via the registered handlerexceptionresolvers. (for example, if multipart resolution failed)
initialize the handlermappings used by this class. if no handlermapping beans are defined in the beanfactory for this namespace, we default to beannameurlhandlermapping.
do we need view name translation?
exposes the dispatcherservlet-specific request attributes and delegates to #dodispatch for the actual dispatching.
no handler found -> set appropriate http response status.
render the given modelandview. this is the last stage in handling a request. it may involve resolving the view by name.
return the handlerexecutionchain for this request. tries all handler mappings in order.
initialize the handleradapters used by this class. if no handleradapter beans are defined in the beanfactory for this namespace, we default to simplecontrollerhandleradapter.
initialize the multipartresolver used by this class. if no bean is defined with the given name in the beanfactory for this namespace, no multipart handling is provided.
initialize the themeresolver used by this class. if no bean is defined with the given name in the beanfactory for this namespace, we default to a fixedthemeresolver.
handle the result of handler selection and handler invocation, which is either a modelandview or an exception to be resolved to a modelandview.
build a localecontext for the given request, exposing the request's primary locale as current locale. the default implementation uses the dispatcher's localeresolver to obtain the current locale, which might change during a request.
return the default strategy object for the given strategy interface. the default implementation delegates to #getdefaultstrategies, expecting a single object in the list.
resolve the given view name into a view object (to be rendered). the default implementations asks all viewresolvers of this dispatcher. can be overridden for custom resolution strategies, potentially based on specific model attributes or request parameters. (typically in case of problems creating an actual view object)
initialize the viewresolvers used by this class. if no viewresolver beans are defined in the beanfactory for this namespace, we default to internalresourceviewresolver.
initialize the handlerexceptionresolver used by this class. if no bean is defined with the given name in the beanfactory for this namespace, we default to no exception resolver.
initialize the localeresolver used by this class. if no bean is defined with the given name in the beanfactory for this namespace, we default to acceptheaderlocaleresolver.
resolves the message, escapes it if demanded, and writes it to the page (or exposes it as variable).
resolve the specified message into a concrete message string. the returned message string should be unescaped.
resolve the given arguments object into an arguments array.
create and expose the current requestcontext. delegates to #dostarttaginternal() for actual work.
set the path that this tag should apply. e.g. "customer" to allow bind paths like "address.street" rather than "customer.address.street".
set the context path for the url. defaults to the current context.
build the query string from available parameters that have not already been applied as template params. the names and values of parameters are url encoded. template params with a '?' instead of 
set the value of the url.
build the url for the tag from the tag attributes and parameters.
replace template markers in the url matching available parameters. the name of matched parameters are added to the used parameters set. parameter values are url encoded.
the passwordinputtag only writes it's value if the
optionally writes the supplied value under the supplied attribute name into the supplied and then the objectutils#getdisplaystring string representation is written as the attribute value. if the resultant string representation is null or empty, no attribute is written.
overridden to default to true in case of no explicit default given.
writes the html ' input' tag to the supplied tagwriter including the databound value.
get the next unique id (within the given pagecontext) for the supplied name.
render the inner ' option' tags using the supplied collection of objects as the source. the value of the #valueproperty field is used when rendering the ' value' of the ' option' and the value of the
write the ' option' tags for the configured #optionsource to the supplied tagwriter.
render the inner ' option' tags using the supplied
render an html ' option' with the supplied value and label. marks the value as 'selected' if either the item itself or its value match the bound value.
renders the ' input(radio)' element with the configured value matches the #getvalue bound value.
adds input-specific optional attributes as defined by this base class.
writes the opening ' label' tag and forces a block tag so that body content is written correctly.
close the current opening tag (if necessary) and appends the supplied value as inner text.
close the current tag, allowing to enforce a full closing tag. correctly writes an empty tag if no inner text or nested tags have been written. rendered in any case, even in case of a non-block tag
write an html attribute with the specified name and value. be sure to write all attributes before writing any inner text or nested tags.
returns true if the supplied candidate value is equal to the value bound to the supplied bindstatus. equality in this case differs from standard java equality and is described in more detail  href="#equality-contract">here.
writes the ' input' tag to the supplied tagwriter. uses the value returned by #gettype() to determine which type of ' input' element to render.
writes the ' value' attribute to the supplied tagwriter. subclasses may choose to override this implementation to control exactly when the value is written.
build the display value of the supplied object, html escaped as required. if the supplied value is not a string and the supplied to obtain the display value.
the user customised the output of the error messages - flush the buffered content into the main javax.servlet.jsp.jspwriter.
determine the ' id' attribute value for this tag, autogenerating one if none specified.
get the bindstatus for this tag.
process the given form field through a requestdatavalueprocessor instance if one is configured or otherwise returns the same value.
process the action through a requestdatavalueprocessor instance if one is configured or otherwise returns the action unmodified.
writes the given values as hidden fields.
writes the opening part of the block ' form' tag and exposes the form object name in the javax.servlet.jsp.pagecontext.
resolve the value of the ' action' attribute. if the user configured an ' action' value then the result of evaluating this value is used. if the user configured an ' servletrelativeaction' value then the value is prepended with the context and servlet paths, and the result is used. otherwise, the originating uri is used.
exposes any bind status error messages under #messages_attribute this key in the pagecontext#page_scope. only called if #shouldrender() returns true.
appends a counter to a specified id as well, since we're dealing with multiple html elements.
renders the ' input type="radio"' element with the configured value matches the bound value.
appends a counter to a specified id, since we're dealing with multiple html elements.
returns ' true' if the bound value requires the resultant ' select' tag to be multi-select.
renders the html ' select' tag to the supplied renders nested ' option' tags if the bound value for the nested optiontag optiontags.
if using a multi-select, a hidden element is needed to make sure all items are correctly unselected on the server-side in response to a
writes the optional attributes configured via this base class to the supplied tagwriter. the set of optional attributes that will be rendered includes any non-standard dynamic attributes. called by #writedefaultattributes(tagwriter).
render the ' input(checkbox)' with the supplied value, marking the ' input' element as 'checked' if the supplied value matches the bound value.
render the ' input(checkbox)' with the supplied value, marking the ' input' element as 'checked' if the supplied boolean is
initialize the view beanfactory from the resourcebundle, for the given locale locale. synchronized because of access by parallel threads.
close the bundle view factories on context shutdown.
exposes a jstl localizationcontext for spring's locale and messagesource.
initialize the view bean factory from the xml file. synchronized because of access by parallel threads.
apply this view's content type as specified in the "contenttype" bean property to the given response. only applies the view's contenttype if no content type has been set on the response before. this allows handlers to override the default content type beforehand.
find the registered requestdatavalueprocessor, if any, and allow it to update the redirect target url.
determine whether the given model element should be exposed as a query property. the default implementation considers strings and primitives as eligible, and also arrays and collectionsiterables with corresponding elements. this can be overridden in subclasses.
append the query string of the current request to the target redirect url.
append query properties to the redirect url. stringifies, url-encodes and formats model attributes as query properties.
replace uri template variables in the target url with encoded model attributes or uri variables from the current request. model attributes referenced in the url are removed from the model.
create the target url by checking if the redirect string is a uri template first, expanding it with the given model, and then optionally appending simple type model attributes as query string parameters.
send a redirect back to the http client.
determine name-value pairs for query strings, which will be stringified, url-encoded and formatted by #appendqueryproperties. this implementation filters the model through checking by default accepting strings, primitives and primitive wrappers only.
whether the given targeturl has a host that is a "foreign" system in which case httpservletresponse#encoderedirecturl will not be applied. this method returns true if the #sethosts(string[]) property is configured and the target url has a host that does not match. the url does not have a host or the "host" property is not configured.
determines the status code to use for http 1.1 compatible requests. the default implementation returns the #setstatuscode(httpstatus) statuscode property if set, or the value of the #response_status_attribute attribute. if neither are set, it defaults to httpstatus#see_other (303).
creates a new view instance of the specified view class and configures it. does not perform any lookup for pre-defined view instances. spring lifecycle methods as defined by the bean container do not have to be called here; those will be applied by the loadview method after this method returns. subclasses will typically call super.buildview(viewname) first, before setting further properties themselves. loadview will then apply spring lifecycle methods at the end of this process.
overridden to implement check for "redirect:" prefix. not possible in loadview, since overridden superclass always creating instances of the required view class.
set the view class that should be used to create views. (by default, abstracturlbasedview)
apply the containing applicationcontext's lifecycle methods to the given view instance, if such a context is available. or a decorated variant)
delegates to buildview for creating a new instance of the specified view class. applies the following spring lifecycle methods (as supported by the generic spring bean factory):  applicationcontextaware's setapplicationcontext initializingbean's afterpropertiesset 
prepare for rendering, and determine the request dispatcher path to forward to (or to include). this implementation simply returns the configured url. subclasses can override this to determine a resource to render, typically interpreting the url in a different manner.
render the internal resource given the specified model. this includes setting the model as request attributes.
determines the list of mediatype for the given httpservletrequest.
creates a combined output map (never null) that includes dynamic values and static attributes. dynamic values take precedence over static attributes.
get the request handle to expose to #rendermergedoutputmodel, i.e. to the view. the default implementation wraps the original request for exposure of spring beans as request attributes (if demanded).
set static attributes for this view from a map. this allows to set any kind of attribute values, for example bean references. "static" attributes are fixed attributes that are specified in the view instance configuration. "dynamic" attributes, on the other hand, are values passed in as part of the model. can be populated with a "map" or "props" element in xml bean definitions.
prepare the given response for rendering. the default implementation applies a workaround for an ie bug when sending download content via https.
set static attributes as a csv string. format is: attname0=value1,attname1=value1 "static" attributes are fixed attributes that are specified in the view instance configuration. "dynamic" attributes, on the other hand, are values passed in as part of the model.
write the given temporary outputstream to the http response.
expose the model objects in the given map as request attributes. names will be taken from the model map. this method is suitable for all resources reachable by javax.servlet.requestdispatcher.
set the content type of the response to the configured to a concrete media type.
create a requestcontext to expose under the specified attribute name. the default implementation creates a standard requestcontext instance for the given request and model. can be overridden in subclasses for custom instances. with dynamic values taking precedence over static attributes
transform the request uri (in the context of the webapp) stripping slashes and extensions, and replacing the separator as required. as determined by the urlpathhelper if desired
autodetect a markuptemplateengine via the applicationcontext. called if a markuptemplateengine has not been manually configured.
return a template compiled by the configured groovy markup template engine for the given view url.
create a parent classloader for groovy to use as parent classloader when loading and compiling templates.
resolve a template from the given template path. the default implementation uses the locale associated with the current request, as obtained through org.springframework.context.i18n.localecontextholder localecontextholder, to find the template file. effectively the locale configured at the engine level is ignored.
filter and optionally wrap the model in mappingjacksonvalue container.
write the actual json content to the stream.
filter out undesired attributes from the given model. the return value can be either another map or a single value object. the default implementation removes bindingresult instances and entries not included in the #setmodelkeys modelkeys property.
build a freemarker template model for the given model map. the default implementation builds a allhttpscopeshashmodel.
invoked on startup. looks for a single freemarkerconfig bean to find the relevant configuration for this factory. checks that the template for the default locale can be found: freemarker will check non-locale-specific templates if a locale-specific one is not found.
autodetect a freemarkerconfig object via the applicationcontext.
check that the freemarker template used for this view exists and is valid. can be overridden to customize the behavior, for example in case of multiple templates to be rendered into a single view.
build a freemarker httpsessionhashmodel for the given request, detecting whether a session already exists and reacting accordingly.
return the configured freemarker objectwrapper, or the
process the freemarker template to the servlet response. can be overridden to customize the behavior.
invokes #buildfeeditems(map, httpservletrequest, httpservletresponse) to get a list of feed items.
invokes #buildfeedentries(map, httpservletrequest, httpservletresponse) to get a list of feed entries.
create a tiles request. this implementation creates a servletrequest.
creates and exposes a tilescontainer for this web application, delegating to the tilesinitializer.
specify whether to apply tiles 3.0's "complete-autoload" configuration. see org.apache.tiles.extras.complete.completeautoloadtilescontainerfactory for details on the complete-autoload mode. note: specifying the complete-autoload mode effectively disables all other bean properties on this configurer. the entire initialization procedure is then left to org.apache.tiles.extras.complete.completeautoloadtilesinitializer.
see scripttemplateconfigurer#setresourceloaderpath(string) documentation.
locate the object to be marshalled. the default implementation first attempts to look under the configured locate an object of marshaller#supports(class) supported type.
check whether the given value from the current view's model is eligible for marshalling through the configured marshaller. the default implementation calls marshaller#supports(class), unwrapping a given jaxbelement first if applicable.
convert the supplied object into an xslt source if the
close the underlying resource managed by the supplied source if applicable. only works for streamsource streamsources.
create the transformer instance used to prefer the xslt transformation. the default implementation simply calls templates#newtransformer(), and configures the transformer with the custom uriresolver if specified.
copy the configured output properties, if any, into the
instantiate a new transformerfactory for this view. the default implementation simply calls if a #settransformerfactoryclass "transformerfactoryclass" has been specified explicitly, the default constructor of the specified class will be called instead. can be overridden in subclasses.
configure the supplied httpservletresponse. the default implementation of this method sets the from the "media-type" and "encoding" output properties specified in the transformer.
get the xslt source for the xslt template under the #seturl configured url.
load the templates instance for the stylesheet at the configured location.
the actual render step: taking the poi workbook and rendering it to the given response.
renders the excel view, given the specified model.
prepare the given pdfwriter. called before building the pdf document, that is, before the call to document.open(). useful for registering a page event listener, for example. the default implementation sets the viewer preferences as returned by this class's getviewerpreferences() method.
read the raw pdf resource into an itext pdfreader. the default implementation resolve the specified "url" property as applicationcontext resource.
registers an httprequesthandleradapter under a well-known name unless already registered.
registers a map corsconfiguration> (mapped corsconfigurations) under a well-known name unless already registered. the bean definition may be updated if a non-null cors configuration is provided.
registers an httprequesthandleradapter under a well-known name unless already registered.
find the contentnegotiationmanager bean created by or registered with the annotation-driven element.
adds an alias to an existing well-known name or registers a new instance of a urlpathhelper under that well-known name, unless already registered.
registers an handlermappingintrospector under a well-known name unless already registered.
registers a simplecontrollerhandleradapter under a well-known name unless already registered.
adds an alias to an existing well-known name or registers a new instance of a pathmatcher under that well-known name, unless already registered.
enable forwarding to the "default" servlet identified by the given name. this is useful when the default servlet cannot be autodetected, for example when it has been manually configured.
return a handler mapping instance ordered at integer#max_value containing the or null if default servlet handling was not been enabled.
return the registered corsconfiguration objects, keyed by path pattern.
returns a requestmappinghandleradapter for processing requests through annotated controller methods. consider overriding one of these other more fine-grained methods:   #addargumentresolvers for adding custom argument resolvers.  #addreturnvaluehandlers for adding custom return value handlers.  #configuremessageconverters for adding custom message converters. 
return a formattingconversionservice for use with annotated controllers. see #addformatters as an alternative to overriding this method.
return a requestmappinghandlermapping ordered at 0 for mapping requests to annotated controllers.
a resourceurlprovider bean for use with the mvc dispatcher.
provide access to the shared return value handlers used by the this method cannot be overridden; use #addreturnvaluehandlers instead.
return the configurablewebbindinginitializer to use for initializing all webdatabinder instances.
register a viewresolvercomposite that contains a chain of view resolvers to use for view resolution. by default this resolver is ordered at 0 unless content negotiation view resolution is used in which case the order is raised to ordered.highest_precedence. if no other resolvers are configured, to allow other potential viewresolver beans to resolve views.
return an instance of compositeuricomponentscontributor for use with
callback for building the pathmatchconfigurer. delegates to #configurepathmatch.
return a global validator instance for example for validating delegates to #getvalidator() first and if that returns null checks the classpath for the presence of a jsr-303 implementations before creating a optionalvalidatorfactorybean.if a jsr-303 implementation is not available, a no-op validator is returned.
return a beannameurlhandlermapping ordered at 2 to map url paths to controller bean names.
return a contentnegotiationmanager instance to use to determine requested mediatype media types in a given request.
return a handler mapping ordered at 1 to map url paths directly to view names. to configure view controllers, override
provide access to the shared handler interceptors used to configure this method cannot be overridden; use #addinterceptors instead.
return a global urlpathhelper instance for path matching patterns in handlermapping handlermappings. this instance can be configured using the pathmatchconfigurer in #configurepathmatch(pathmatchconfigurer).
provide access to the shared custom argument resolvers used by the this method cannot be overridden; use #addargumentresolvers instead.
a method available to subclasses for adding default adds the following exception resolvers:   exceptionhandlerexceptionresolver for handling exceptions through  responsestatusexceptionresolver for exceptions annotated with  defaulthandlerexceptionresolver for resolving known spring exception types 
provides access to the shared httpmessageconverter httpmessageconverters used by the requestmappinghandleradapter and the this method cannot be overridden; use #configuremessageconverters instead. also see #adddefaulthttpmessageconverters for adding default message converters.
returns a handlerexceptionresolvercomposite containing a list of exception resolvers obtained either through #configurehandlerexceptionresolvers or through #adddefaulthandlerexceptionresolvers. note: this method cannot be made final due to cglib constraints. rather than overriding it, consider overriding #configurehandlerexceptionresolvers which allows for providing a list of resolvers.
adds a set of default httpmessageconverter instances to the given list. subclasses can call this method from #configuremessageconverters.
return a global pathmatcher instance for path matching patterns in handlermapping handlermappings. this instance can be configured using the pathmatchconfigurer in #configurepathmatch(pathmatchconfigurer).
return a handler mapping ordered at integer.max_value with a mapped default servlet handler. to configure "default" servlet handling, override #configuredefaultservlethandling.
return a handler mapping ordered at integer.max_value-1 with mapped resource handlers. to configure resource handling, override
register a viewresolver bean instance. this may be useful to configure a custom (or 3rd party) resolver implementation. it may also be used as an alternative to other registration methods in this class when they don't expose some more advanced property that needs to be set.
register tiles 3.x view resolver. note that you must also configure tiles by adding a
register jsp view resolver with the specified prefix and suffix. when this method is invoked more than once, each call will register a new viewresolver instance. note that since it's not easy to determine if a jsp exists without forwarding to it, using multiple jsp-based view resolvers only makes sense in combination with the "viewnames" property on the resolver indicating which view names are handled by which resolver.
register a script template view resolver with an empty default view name prefix and suffix.
register a groovy markup view resolver with an empty default view name prefix and a default suffix of ".tpl".
register a freemarker view resolver with an empty default view name prefix and a default suffix of ".ftl". note that you must also configure freemarker by adding a
map a simple controller to the given url path (or pattern) in order to set the response status to the given code without rendering a body.
map a view controller to the given url path (or pattern) in order to render a response with a pre-configured status code and view. patterns like "admin" or "articlesarticlename:\\w+" are allowed. see org.springframework.util.antpathmatcher for more details on the syntax.
map a view controller to the given url path (or pattern) in order to redirect to another url. by default the redirect url is expected to be relative to the current servletcontext, i.e. as relative to the web application root.
return the handlermapping that contains the registered view controller mappings, or null for no registrations.
return the registered corsconfiguration objects, keyed by path pattern.
enable cross-origin request handling for the specified path pattern. exact path mapping uris (such as "admin") are supported as well as ant-style path patterns (such as "admin"). by default, all origins, all headers, credentials and get, is set to 30 minutes. the following defaults are applied to the corsregistration:  allow all origins. allow "simple" methods get, head and post. allow all headers. set max age to 1800 seconds (30 minutes). 
adds the provided webrequestinterceptor. registered interceptor further for example adding url patterns it should apply to.
adds the provided handlerinterceptor. registered interceptor further for example adding url patterns it should apply to.
return a resourcehttprequesthandler instance.
return a handler mapping with the mapped resource handlers; or null in case of no registrations.
add a resource handler for serving static resources based on the specified url path patterns. the handler will be invoked for every incoming request that matches to one of the specified path patterns. patterns like "static" or "cssfilename:\\w+\\.css" are allowed. see org.springframework.util.antpathmatcher for more details on the syntax. registered resource handler
build the underlying interceptor. if url patterns are provided, the returned type is mappedinterceptor; otherwise handlerinterceptor.
configure a path prefix to apply to matching controller methods. prefixes are used to enrich the mappings of every @requestmapping method whose controller type is matched by the corresponding consider using org.springframework.web.method.handlertypepredicate handlertypepredicate to group controllers.
initialize the wrapped servlet instance.
map specific url paths to specific cache seconds. overrides the default cache seconds setting of this interceptor. can specify "-1" to exclude a url path from default caching. supports direct matches, e.g. a registered "test" matches "test", and a various ant-style pattern matches, e.g. a registered "t" matches both "test" and "team". for details, see the antpathmatcher javadoc. note: path patterns are not supposed to overlap. if a request matches several mappings, it is effectively undefined which one will apply (due to the lack of key ordering in java.util.properties). cache seconds (as values, need to be integer-parsable)
return the name of the view to delegate to, or null if using a view instance.
return a modelandview object with the specified view name. the content of the requestcontextutils#getinputflashmap "input" flashmap is also added to the model.
retrieves the url path to use for lookup and delegates to
extract the url filename from the given request uri.
extract a url path from the given request, suitable for view name extraction.
combine "this" request mapping info (i.e. the current instance) with another request mapping info instance. example: combine type- and method-level request mappings.
checks if all conditions in this request mapping info match the provided request and returns a potentially new request mapping info with conditions tailored to the current request. for example the returned instance may contain the subset of url patterns that match to the current request, sorted with best matching patterns on top.
any partial matches for "methods", "consumes", "produces", and "params"?
create a new partialmatch instance.
iterate all requestmappinginfo's once again, look if any match by url at least and raise exceptions according to what doesn't match. but not by http method but not by consumableproducible media types
return declared "producible" types but only among those that also match the "methods" and "consumes" conditions.
return declared http methods.
any partial matches for "methods" and "consumes"?
expose uri template variables, matrix variables, and producible media types in the request.
any partial matches for "methods", "consumes", and "produces"?
return declared "params" conditions but only among those that also match the "methods", "consumes", and "params" conditions.
any partial matches for "methods"?
return declared "consumable" types but only among those that also match the "methods" condition.
send an sse event prepared with the given builder. for example:  static import of sseemitter sseemitter emitter = new sseemitter(); emitter.send(event().name("update").id("1").data(myobject)); 
process the given reactive return value and decide whether to adapt it to a responsebodyemitter or a deferredresult. with a deferredresult
return the list of return value handlers to use including built-in and custom handlers provided via #setreturnvaluehandlers.
find an @exceptionhandler method for the given exception. the default implementation searches methods in the class hierarchy of the controller first and if not found, it continues searching for additional @exceptionhandler methods assuming some controlleradvice @controlleradvice spring-managed beans were detected.
return the list of argument resolvers to use including built-in resolvers and custom resolvers provided via #setcustomargumentresolvers.
configure the complete list of supported argument types thus overriding the resolvers that would otherwise be configured by default.
find an @exceptionhandler method and invoke it to handle the raised exception.
configure the complete list of supported return value types thus overriding handlers that would otherwise be configured by default.
set modelandviewcontainer#setrequesthandled(boolean) to to the response. if subsequently the underlying method returns
create a uricomponentsbuilder by invoking a "mock" controller method. the controller method and the supplied argument values are then used to delegate to #frommethod(class, method, object...). for example, given this controller:  class="code"> @requestmapping("peopleidaddresses") class addresscontroller @requestmapping("country") public httpentity<void> getaddressesforcountry(@pathvariable string country) ... @requestmapping(value="", method=requestmethod.post) public void addaddress(address address) ...  a uricomponentsbuilder can be created:  class="code"> inline style with static import of "mvcuricomponentsbuilder.on" mvcuricomponentsbuilder.frommethodcall( on(addresscontroller.class).getaddressesforcountry("us")).buildandexpand(1); longer form useful for repeated invocation (and void controller methods) addresscontroller controller = mvcuricomponentsbuilder.on(addresscontroller.class); controller.addaddress(null); builder = mvcuricomponentsbuilder.frommethodcall(controller); controller.getaddressesforcountry("us") builder = mvcuricomponentsbuilder.frommethodcall(controller);  note: this method extracts values from "forwarded" and "x-forwarded-" headers if found. see class-level docs. invocation or the "mock" controller itself after an invocation
create a new methodargumentbuilder instance.
an alternative to #frommethodcall(object) that accepts a when using mvcuricomponentsbuilder outside the context of processing a request or to apply a custom baseurl not matching the current request. note: this method extracts values from "forwarded" and "x-forwarded-" headers if found. see class-level docs. and therefore not modified and may be re-used for further calls. invocation or the "mock" controller itself after an invocation
an alternative to #frommappingname(string) that accepts a when using mvcuricomponentsbuilder outside the context of processing a request or to apply a custom baseurl not matching the current request. note: this method extracts values from "forwarded" and "x-forwarded-" headers if found. see class-level docs. and therefore not modified and may be re-used for further calls. if there is no unique match
an alternative to #fromcontroller(class) that accepts a when using mvcuricomponentsbuilder outside the context of processing a request or to apply a custom baseurl not matching the current request. note: this method extracts values from "forwarded" and "x-forwarded-" headers if found. see class-level docs. and therefore not modified and may be re-used for further calls.
return the sessionattributeshandler instance for the given handler type (never null).
configure the complete list of supported argument types thus overriding the resolvers that would otherwise be configured by default.
configure the complete list of supported return value types thus overriding handlers that would otherwise be configured by default.
return the list of argument resolvers to use including built-in resolvers and custom resolvers provided via #setcustomargumentresolvers.
return the list of argument resolvers to use for @initbinder methods including built-in and custom resolvers.
return the list of return value handlers to use including built-in and custom handlers provided via #setreturnvaluehandlers.
template method to create a new initbinderdatabinderfactory instance. the default implementation creates a servletrequestdatabinderfactory. this can be overridden for custom servletrequestdatabinder subclasses.
configure the supported argument types in @initbinder methods.
invoke the requestmapping handler method preparing a modelandview if view resolution is required.
set the response status according to the responsestatus annotation.
invoke the method and handle the return value through one of the configured handlermethodreturnvaluehandler handlermethodreturnvaluehandlers.
merge uri variables into the property values to use for data binding.
whether the given methodparameter method parameter is a multi-part supported. supports the following:  annotated with @requestpart of type multipartfile unless annotated with @requestparam of type javax.servlet.http.part unless annotated with 
return a map with all uri template variables or an empty map.
adapt the given argument against the method parameter, if necessary.
constructor with converters and request~ and responsebodyadvice.
whether to raise a fatal bind exception on validation errors.
create a new httpinputmessage from the given nativewebrequest.
create the method argument value of the expected parameter type by reading from the given httpinputmessage. parameter type, e.g. for httpentity.
validate the binding target if applicable. the default implementation checks for @javax.validation.valid, spring's org.springframework.validation.annotation.validated, and custom annotations whose name starts with "valid".
return the media types supported by all provided message converters sorted by specificity via mediatype#sortbyspecificity(list).
return the generic type of the returntype (or of the nested type if it is an httpentity).
constructor with list of converters and contentnegotiationmanager as well as requestresponse body advice instances.
check if the path has a file extension and whether the extension is either #whitelisted_extensions whitelisted or explicitly if not, and the status is in the 2xx range, a 'content-disposition' header with a safe attachment file name ("f.txt") is added to prevent rfd exploits.
returns the media types that can be produced. the resulting media types are:  the producible media types specified in the request mappings, or media types of configured converters that can write the specific return value, or  mediatype#all 
creates a new httpoutputmessage from the given nativewebrequest.
writes the given return type to the given output message. by the accept header on the request cannot be met by the message converters
throws methodargumentnotvalidexception if validation fails. is true and there is no body content or if there is no suitable converter to read the content with.
returns an instance of extendedservletrequestdatabinder.
provides handling for standard spring mvc exceptions.
customize the response for httprequestmethodnotsupportedexception. this method logs a warning, sets the "allow" header, and delegates to
customize the response for httpmediatypenotsupportedexception. this method sets the "accept" header and delegates to
customize the response for nohandlerfoundexception. this method delegates to #handleexceptioninternal.
a single place to customize the response body of all exception types. the default implementation sets the webutils#error_exception_attribute request attribute and creates a responseentity from the given body, headers, and status.
create a requestmappinginfo from the supplied a directly declared annotation, a meta-annotation, or the synthesized result of merging annotation attributes within an annotation hierarchy.
uses method and type-level @ requestmapping annotations to create the requestmappinginfo. does not have a @requestmapping annotation.
resolve placeholder values in the given array of patterns.
simple constructor with reactive type support based on a default instance of
complete constructor with pluggable "reactive" type support.
this implementation downcasts webdatabinder to
obtain a value from the request that may be used to instantiate the model attribute through type conversion from string to the target type. the default implementation looks for the attribute name to match a uri variable first and then a request parameter.
create a model attribute from a string request value (e.g. uri template variable, request parameter) using type conversion. the default implementation converts only if there a registered conversion found
each attribute value is formatted as a string before being merged.
each attribute value is formatted as a string before being added.
each value is formatted as a string before being added.
handle the case where an argument annotated with @valid such as an requestbody or requestpart argument fails validation. by default, an http 400 error is sent back to the client.
handle the case when a required parameter is missing. the default implementation sends an http 400 error, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the missingservletrequestparameterexception could be rethrown as-is.
handle the case when a org.springframework.web.bind.webdatabinder conversion error occurs. the default implementation sends an http 400 error, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the typemismatchexception could be rethrown as-is.
handle the case where an modelattribute @modelattribute method argument has binding or validation errors and is not followed by another method argument of type bindingresult. by default, an http 400 error is sent back to the client.
handle the case where a cannot write to a http request. the default implementation sends an http 500 error, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the httpmessagenotwritableexception could be rethrown as-is.
handle the case when a declared path variable does not match any extracted uri variable. the default implementation sends an http 500 error, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the missingpathvariableexception could be rethrown as-is.
handle the case where an requestpart @requestpart, a multipartfile, or a javax.servlet.http.part argument is required but is missing. by default, an http 400 error is sent back to the client.
handle the case when a org.springframework.web.bind.webdatabinder conversion cannot occur. the default implementation sends an http 500 error, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the conversionnotsupportedexception could be rethrown as-is.
handle the case where no request handler method was found for the particular http request method. the default implementation logs a warning, sends an http 405 error, sets the "allow" header, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the httprequestmethodnotsupportedexception could be rethrown as-is. at the time of the exception (for example, if multipart resolution failed)
handle the case where no org.springframework.http.converter.httpmessageconverter message converters were found for the put or posted content. the default implementation sends an http 415 error, sets the "accept" header, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the httpmediatypenotsupportedexception could be rethrown as-is.
handle the case where a org.springframework.http.converter.httpmessageconverter message converter cannot read from a http request. the default implementation sends an http 400 error, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the httpmessagenotreadableexception could be rethrown as-is.
handle the case where no org.springframework.http.converter.httpmessageconverter message converters were found that were acceptable for the client (expressed via the accept header. the default implementation sends an http 406 error and returns an empty modelandview. alternatively, a fallback view could be chosen, or the httpmediatypenotacceptableexception could be rethrown as-is.
handle the case where no handler was found during the dispatch. the default implementation sends an http 404 error and returns an empty or the nohandlerfoundexception could be rethrown as-is. at the time of the exception (for example, if multipart resolution failed)
handle the case when an unrecoverable binding exception occurs - e.g. required header, required cookie. the default implementation sends an http 400 error, and returns an empty modelandview. alternatively, a fallback view could be chosen, or the exception could be rethrown as-is.
invoked to send a server error. sets the status to 500 and also sets the request attribute "javax.servlet.error.exception" to the exception.
handle the case where an async request timed out. the default implementation sends an http 503 error. at the time of the exception (for example, if multipart resolution failed)
apply the resolved status code and reason to the response. the default implementation sends a response error using and then returns an empty modelandview.
template method that handles the responsestatus @responsestatus annotation. the default implementation delegates to #applystatusandreason with the status code and reason from the annotation. time of the exception, e.g. if multipart resolution failed
template method that handles an responsestatusexception. the default implementation delegates to #applystatusandreason with the status code and reason from the exception. time of the exception, e.g. if multipart resolution failed
checks if any of the contained media type expressions match the given request 'content-type' header and returns an instance that is guaranteed to contain matching expressions only. the match is performed via or a new condition with matching expressions only; or null if no expressions match
returns the media types for this condition excluding negated expressions.
on a pre-flight request match to the would-be, actual request. hence empty conditions is a match, otherwise try to match to the http method in the "access-control-request-method" header.
returns a new instance with a union of the http request methods from "this" and the "other" instance.
if one instance is empty, return the other. if both instances have conditions, combine the individual conditions after ensuring they are of the same type and number.
if one instance is empty, the other "wins". if both instances have conditions, compare them in the order in which they were provided.
delegate to all contained conditions to match the request and return the resulting "matching" condition instances. an empty compositerequestcondition matches to all requests.
private constructor with already parsed media type expressions.
checks if any of the contained media type expressions match the given request 'content-type' header and returns an instance that is guaranteed to contain matching expressions only. the match is performed via or a new condition with matching expressions; or null if no expressions match.
return the contained producible media types excluding negated expressions.
compares this and another "produces" condition as follows:  sort 'accept' header media types by quality value via get the first index of matching media types in each "produces" condition first matching with mediatype#equals(object) and then with mediatype#includes(mediatype). if a lower index is found, the condition at that index wins. if both indexes are equal, the media types at the index are compared further with mediatype#specificity_comparator.  it is assumed that both instances have been obtained via contains the matching producible media type expression only or is otherwise empty.
same as #producesrequestcondition(string[], string[]) but also accepting a contentnegotiationmanager.
checks if any of the patterns match the given request and returns an instance that is guaranteed to contain matching patterns, sorted via a matching pattern is obtained by making checks in the following order:  direct match pattern match with "." appended if the pattern doesn't already contain a "." pattern match pattern match with "" appended if the pattern doesn't already end in ""  or a new condition with sorted matching patterns; or null if no patterns match.
private constructor accepting a collection of patterns.
find the patterns matching the given lookup path. invoking this method should yield results equivalent to those of calling this method is provided as an alternative to be used if no request is available (e.g. introspection, tooling, etc).
compare the two conditions based on the url patterns they contain. patterns are compared one at a time, from top to bottom via patterns match equally, but one instance has more patterns, it is considered a closer match. it is assumed that both instances have been obtained via contain only patterns that match the request and are sorted with the best matches on top.
returns a new instance with url patterns from the current instance ("this") and the "other" instance as follows:  if there are patterns in both instances, combine the patterns in "this" with the patterns in "other" using pathmatcher#combine(string, string). if only one instance has patterns, use them. if neither instance has patterns, use an empty string (i.e. ""). 
returns a new instance with the union of the header expressions from "this" and the "other" instance.
returns "this" instance if the request matches all expressions; or null otherwise.
returns "this" instance if the request matches all param expressions; or null otherwise.
returns a new instance with the union of the param expressions from "this" and the "other" instance.
ensure the held request conditions are of the same type.
combine the request conditions held by the two requestconditionholder instances after making sure the conditions are of the same type. or if one holder is empty, the other holder is returned.
get the matching condition for the held request condition wrap it in a new requestconditionholder instance. or otherwise if this is an empty holder, return the same holder instance.
determine the default locale for the given request, called if no locale cookie has been found. the default implementation returns the specified default locale, if any, else falls back to the request's accept-header locale.
determine the default locale for the given request, called if no locale session attribute has been found. the default implementation returns the specified default locale, if any, else falls back to the request's accept-header locale.
specify whether to parse request parameter values as bcp 47 language tags instead of java's legacy locale specification format. note: as of 5.1, this resolver leniently accepts the legacy
create a new webcontentgenerator. generator should support http methods get, head and post by default, or false if it should be unrestricted
prevent the response from being cached. only called in http 1.0 compatibility mode. see http:www.mnot.netcache_docs.
set http headers to allow caching for the given number of seconds. tells the browser to revalidate the resource if mustrevalidate is should be cacheable for (typically only necessary for controllers with last-modified support)
prepare the given response according to the settings of this generator. applies the number of cache seconds specified for this generator.
apply the given cache seconds and generate corresponding http headers, i.e. allow caching for the given number of seconds in case of a positive value, prevent caching if given a 0 value, do nothing else. does not tell the browser to revalidate the resource. response should be cacheable for, 0 to prevent caching
set the http cache-control header according to the given settings.
set the http methods that this content generator should support. default is get, head and post for simple form controller types; unrestricted for general controllers and interceptors.
check the given request for supported methods and a required session, if any.
whether the given flashmap matches the current request. uses the expected request path and query parameters saved in the flashmap.
return a flashmap contained in the given list that matches the request.
return a list of expired flashmap instances contained in the given list.
add the given filter to the servletcontext and map it to the  a default filter name is chosen based on its concrete type the asyncsupported flag is set depending on the return value of #isasyncsupported() asyncsupported a filter mapping is created with dispatcher types request, on the return value of #isasyncsupported() asyncsupported  if the above defaults are not suitable or insufficient, override this method and register filters directly with the servletcontext.
register a dispatcherservlet against the given servlet context. this method will create a dispatcherservlet with the name returned by from #createservletapplicationcontext(), and mapping it to the patterns returned from #getservletmappings(). further customization can be achieved by overriding #customizeregistration(servletregistration.dynamic) or
this implementation creates an annotationconfigwebapplicationcontext, providing it the annotated classes returned by #getservletconfigclasses().
this implementation creates an annotationconfigwebapplicationcontext, providing it the annotated classes returned by #getrootconfigclasses(). returns null if #getrootconfigclasses() returns null.
extract the error codes from the objecterror list.
create a new bindstatus instance, representing a field or object status. will be resolved (e.g. "customer.address.street")
extract the error messages from the objecterror list.
retrieves saved flashmap instances from the http session, if any.
look for the webapplicationcontext associated with the dispatcherservlet that has initiated request processing, and for the global context if none was found associated with the current request. the global context will be found via the servletcontext or via contextloader's current context. note: this variant remains compatible with servlet 2.5, explicitly checking a given servletcontext instead of deriving it from the request. if no request-specific context has been found, or null if none
convenience method that retrieves the #getoutputflashmap "output" flashmap, updates it with the path and query params of the target url, and then saves it using the #getflashmapmanager flashmapmanager.
retrieves the current theme from the given request, using the themeresolver and themesource bound to the request by the dispatcherservlet.
return read-only "input" flash attributes from request before redirect.
checks jstl's "javax.servlet.jsp.jstl.fmt.localizationcontext" context-param and creates a corresponding child message source, with the provided spring-defined messagesource as parent. (to check jstl-related context-params in web.xml) the applicationcontext of the current dispatcherservlet jstl-defined bundle, then the spring-defined messagesource
exposes jstl-specific request attributes specifying locale and resource bundle for jstl's formatting and message tags, using spring's locale and messagesource. including the applicationcontext to expose as messagesource
exposes jstl-specific request attributes specifying locale and resource bundle for jstl's formatting and message tags, using spring's locale and messagesource. typically the current applicationcontext (may be null)
change the current theme to the specified theme by name, storing the new theme name through the configured themeresolver.
create a new requestcontext for the given request, using the given model attributes for errors retrieval. this works with all view implementations. it will typically be used by view implementations. if a servletcontext is specified, the requestcontext will also work with a root webapplicationcontext (outside a dispatcherservlet). fallback to root webapplicationcontext) for errors retrieval)
change the current theme to the specified one, storing the new theme name through the configured themeresolver.
change the current locale to the specified locale and time zone context, storing the new locale context through the configured localeresolver.
retrieve the errors instance for the given bind object.
determine the fallback theme for this context. the default implementation returns the default theme (with name "theme").
return a context-aware url for the given relative url with placeholders (named keys with braces ). for example, send in a relative url foobar?spam=spam and a parameter map
change the current locale to the specified one, storing the new locale through the configured localeresolver.
initialize a builder with a scheme, host,and port (but not path and query).
prepare a builder from the host, port, scheme, and path (but not the query) of the httpservletrequest.
prepare a builder from the host, port, scheme, context path, and servlet mapping of the given httpservletrequest. if the servlet is mapped by name, e.g. "main", the path will end with "main". if the servlet is mapped otherwise, e.g. if calling #fromcontextpath(httpservletrequest).
prepare a builder by copying the scheme, host, port, path, and query string of an httpservletrequest.
prepare a builder from the host, port, scheme, and context path of the given httpservletrequest.
remove any path extension from the httpservletrequest#getrequesturi() requesturi. this method must be invoked before any calls to #path(string) or #pathsegment(string...).  get http:foo.comrestbooks6.json servleturicomponentsbuilder builder = servleturicomponentsbuilder.fromrequesturi(this.request); string ext = builder.removepathextension(); string uri = builder.path("pages1.ext").buildandexpand(ext).touristring(); assertequals("http:foo.comrestbooks6pages1.json", result); 
register all handlers specified in the url map for the corresponding paths.
check whether this resolver is supposed to apply to the given handler. the default implementation checks against the configured at the time of the exception (for example, if multipart resolution failed) for the given request and handler
check whether this resolver is supposed to apply (i.e. if the supplied handler matches any of the configured #setmappedhandlers handlers or to the #doresolveexception template method.
determine the view name for the given exception, first checking against the
apply the specified http status code to the given response, if possible (that is, if not executing within an include request).
set the http status code that this exception resolver will apply for a given resolved error view. keys are view names; values are status codes. note that this error code will only get applied in case of a top-level request. it will not be set for an include request, since the http status cannot be modified from within an include. if not specified, the default status code will be applied.
find a matching view name in the given exception mappings.
return a modelandview for the given view name and exception. the default implementation adds the specified exception attribute. can be overridden in subclasses.
determine a match for the given lookup path.
create a new mappedinterceptor instance.
checks name and aliases of the given bean for urls, starting with "".
handle a request that is not authorized according to this interceptor. default implementation sends http status code 403 ("forbidden"). this method can be overridden to write a custom message, forward or redirect to some error page or login page, or throw a servletexception.
resolve the exception by iterating over the list of configured exception resolvers. the first one to return a modelandview wins. otherwise null is returned.
register the specified handler for the given url path. (a bean name will automatically be resolved into the corresponding handler bean)
look up a handler instance for the given url path. supports direct matches, e.g. a registered "test" matches "test", and various ant-style pattern matches, e.g. a registered "t" matches both "test" and "team". for details, see the antpathmatcher class. looks for the most exact pattern, where most exact is defined as the longest path pattern.
build a handler object for the given raw handler, exposing the actual handler, the #path_within_handler_mapping_attribute, as well as the #uri_template_variables_attribute before executing the handler. the default implementation builds a handlerexecutionchain with a special interceptor that exposes the path attribute and uri template variables
expose the path within the current mapping as request attribute.
find the handlermapping that would handle the given request and return it as a matchablehandlermapping that can be used to test request-matching criteria. if the matching handlermapping is not an instance of
adapt the given interceptor object to the handlerinterceptor interface. by default, the supported interceptor types are handlerinterceptor and webrequestinterceptor. each given webrequestinterceptor will be wrapped in a webrequesthandlerinterceptoradapter. can be overridden in subclasses.
get the "global" cors configurations.
look up a handler for the given request, falling back to the default handler if no specific one is found.
detect beans of type mappedinterceptor and add them to the list of mapped interceptors. this is called in addition to any mappedinterceptor mappedinterceptors that may have been provided via #setinterceptors, by default adding all beans of type mappedinterceptor from the current context and its ancestors. subclasses can override and refine this policy.
return all configured mappedinterceptor mappedinterceptors as an array.
update the handlerexecutionchain for cors-related handling. for pre-flight requests, the default implementation replaces the selected handler with a simple httprequesthandler that invokes the configured for actual requests, the default implementation inserts a handlerinterceptor that makes cors-related checks and adds cors headers.
set the "global" cors configurations based on url patterns. by default the first matching url pattern is combined with the cors configuration for the handler, if any.
build a handlerexecutionchain for the given handler, including applicable interceptors. the default implementation builds a standard handlerexecutionchain with the given handler, the handler mapping's common interceptors, and any are added in the order they were registered. subclasses may override this in order to extendrearrange the list of interceptors. note: the passed-in handler object may be a raw handler or a pre-built handlerexecutionchain. this method should handle those two cases explicitly, either building a new handlerexecutionchain or extending the existing chain. for simply adding an interceptor in a custom subclass, consider calling
return the adapted interceptors as handlerinterceptor array.
initialize the specified interceptors, checking for mappedinterceptor mappedinterceptors and adapting handlerinterceptors and webrequestinterceptor handlerinterceptors and
register all handlers found in the current applicationcontext. the actual url determination for a handler is up to the concrete which no such urls could be determined is simply not considered a handler.
scan beans in the applicationcontext, detect and register handler methods.
create the handlermethod instance.
look up a handler method for the given request.
look for handler methods in the specified handler bean.
look up the best-matching handler method for the current request. if multiple matches are found, the best match is selected.
invoked after all handler methods have been detected.
checks if the handler is a handlermethod and then delegates to the base class implementation of #shouldapplyto(httpservletrequest, object) passing the bean of the handlermethod. otherwise returns false.
set headers on the given servlet response. called for get requests as well as head requests.
look for a pathresourceresolver among the configured resource resolvers and set its allowedlocations property (if empty) to match the #setlocations locations configured on this class.
check whether the given path contains invalid escape sequences.
identifies invalid resource paths. by default rejects:  paths that contain "web-inf" or "meta-inf" paths that contain ".." after a call to paths that represent a org.springframework.util.resourceutils#isurl valid url or would represent one after the leading slash is removed.  note: this method assumes that leading, duplicate '' or control characters (e.g. white space) have been trimmed so that the path starts predictably with a single '' or does not have one.
processes a resource request. checks for the existence of the requested resource in the configured list of locations. if the resource does not exist, a 404 response will be returned to the client. if the resource exists, the request will be checked for the presence of the timestamp of the given resource, returning a 304 status code if the of the resource will be written to the response with caching headers set to expire one year in the future.
initialize the content negotiation strategy depending on the contentnegotiationmanager setup and the availability of a servletcontext.
find the resource under the given location. the default implementation checks if there is a readable
find a versionstrategy for the request path of the requested resource.
insert a fixed, prefix-based version in resource urls that match the given path patterns, for example: "versionjsmain.js". this is useful (vs. content-based versions) when using javascript module loaders. the version may be a random number, the current date, or a value fetched from a git commit sha, a property file, or environment variable and set with spel expressions in the configuration (e.g. see @value in java config). if not done already, variants of the given pathpatterns, prefixed with the version will be also configured. for example, adding a "js" path pattern will also cofigure automatically a "v1.0.0js" with "v1.0.0" the relative to the pattern configured with the resource handler
a transformer can use this method when a resource being transformed contains links to other resources. such links need to be replaced with the public facing link as determined by the resource resolver chain (e.g. the public url may have a version inserted).
transform the given relative request path to an absolute path, taking the path of the given request as a point of reference. the resulting path is also cleaned from sequences like "path..".
if the defaultservletname property has not been explicitly set, attempts to locate the default servlet using the known common container-specific names.
compare the given path against configured resource handler mappings and if a match is found use the resourceresolver chain of the matched public use. it is expected that the given path is what spring mvc would use for request mapping purposes, i.e. excluding context and servlet path portions. if several handler mappings match, the handler used will be the one configured with the most specific pattern.
a variation on #getforlookuppath(string) that accepts a full request url path (i.e. including context and servlet path) and returns the full request url path to expose for public use.
return the super class of the specified element or null if this
return the interfaces that are directly implemented by the specified element or an empty list if this element does not implement any interface.
create a new metadataprocessor instance.
create a webexchangedatabinder to apply data binding and validation with on the target, command object.
create a new handlerresult.
variant of #frommultipartasyncdata(string, publisher, class) that accepts a parameterizedtypereference for the element type, which allows specifying generic type information. note that you can also build the multipart data externally with
return a multipartinserter to write the given asynchronous parts, as multipart data. note that you can also build the multipart data externally with
return a multipartinserter to write the given parts, as multipart data. values in the map can be an object or an note that you can also build the multipart data externally with
return a forminserter to write the given key-value pair as url-encoded form data. the returned inserter allows for additional entries to be added via forminserter#with(string, object).
adapt the given request processor function to a filter function that only operates on the serverrequest.
return a composed filter function that first applies this filter, and then applies the
return a mutable builder for a handlerstrategies with default initialization.
create a builder with the given publisher.
get the first query parameter with the given name, if present.
get the path variable with the given name, if present.
route to the given handler function if the given request predicate applies. for instance, the following example routes get requests for "user" to the  class="code"> routerfunction<serverresponse> route = routerfunctions.route(requestpredicates.get("user"), usercontroller::listusers); 
convert the given routerfunction router function into a webhandler, using the given strategies.
route to the given router function if the given request predicate applies. this method can be used to create nested routes, where a group of routes share a common path (prefix), header, or other request predicate. for instance, the following example first creates a composed route that resolves to nested with a "user" path predicate, so that get requests for "user" will list users, and post request for "user" will create a new user.  class="code"> routerfunction<serverresponse> userroutes = routerfunctions.route(requestpredicates.method(httpmethod.get), this::listusers) .androute(requestpredicates.method(httpmethod.post), this::createuser); routerfunction<serverresponse> nestedroute = routerfunctions.nest(requestpredicates.path("user"), userroutes); 
initialized the router functions by detecting them in the application context.
create webclientresponseexception or an http status specific subclass.
constructor with a prepared message.
return a builder pre-configured with default configuration to start. this is the same as #withdefaults() but returns a mutable builder for further customizations.
create a response builder with the given status code and message body readers.
select the best media type for the current request through a content negotiation algorithm.
checks if all conditions in this request mapping info match the provided request and returns a potentially new request mapping info with conditions tailored to the current request. for example the returned instance may contain the subset of url patterns that match to the current request, sorted with best matching patterns on top.
combines "this" request mapping info (i.e. the current instance) with another request mapping info instance. example: combine type- and method-level request mappings.
iterate all requestmappinginfos once again, look if any match by url at least and raise exceptions accordingly. and http method but not by consumable media types method but not by producible media types method but not by query parameter conditions
expose uri template variables, matrix variables, and producible media types in the request.
create a new partialmatch instance.
return declared "producible" types but only among those that also match the "methods" and "consumes" conditions.
return declared "consumable" types but only among those that also match the "methods" condition.
iterate over registered invoke the one that supports it.
find a registered handlermethodargumentresolver that supports the given method parameter.
invoke the method for the given exchange.
evaluate the predicate on the method parameter type or on the generic type within a reactive type wrapper.
evaluate the predicate on the method parameter type but raise an within a reactive type wrapper.
evaluate the predicate on the method parameter type if it has the given annotation, nesting within java.util.optional if necessary, but raise an illegalstateexception if the same matches the generic type within a reactive type wrapper.
invoke the method for the given exchange.
look up the best-matching handler method for the current request. if multiple matches are found, the best match is selected.
look up a handler method for the given request.
create the handlermethod instance.
look for handler methods in a handler.
scan beans in the applicationcontext, detect and register handler methods.
initialize the org.springframework.ui.model model based on a (type-level) @sessionattributes annotation and
derive the model attribute name for the given method parameter based on a @modelattribute parameter annotation (if present) or falling back on parameter type based conventions.
find @modelattribute arguments also listed as @sessionattributes.
write a given body to the response with httpmessagewriter. could be different from bodyparameter when processing httpentity for example
provide the context required to apply #savemodel() after the controller method has been invoked.
read the body from a method argument with httpmessagereader. from bodyparam, e.g. for an httpentity argument
constructor that also accepts a reactiveadapterregistry.
check if the given methodparameter requires validation and if so return a (possibly empty) object[] with validation hints. a return value of
find @initbinder methods in @controlleradvice components or in the controller of the given @requestmapping method.
return an invocablehandlermethod for the given
find @modelattribute methods in @controlleradvice components or in the controller of the given @requestmapping method.
return the handler for the type-level @sessionattributes annotation based on the given controller method.
find an @exceptionhandler method in @controlleradvice components or in the controller of the given @requestmapping method.
apply type conversion if necessary.
create a new abstractnamedvalueargumentresolver instance. and #... spel expressions in default values, or null if default values are not expected to contain expressions
a null results in a false value for booleans or an exception for other primitives.
invoked when a named value is required, but exception in this case.
create a new namedvalueinfo based on the given namedvalueinfo with sanitized values.
resolve the given annotation-specified value, potentially containing placeholders and expressions.
resolve placeholder values in the given array of patterns.
uses method and type-level @ requestmapping annotations to create the requestmappinginfo. does not have a @requestmapping annotation.
configure path prefixes to apply to controller methods. prefixes are used to enrich the mappings of every @requestmapping method whose controller type is matched by a corresponding is used, assuming the input map has predictable order. consider using org.springframework.web.method.handlertypepredicate handlertypepredicate to group controllers.
create a requestmappinginfo from the supplied a directly declared annotation, a meta-annotation, or the synthesized result of merging annotation attributes within an annotation hierarchy.
retrieve "known" attributes from the session, i.e. attributes listed by name in @sessionattributes or attributes previously stored in the model that matched by type.
store a subset of the given attributes in the session. attributes not declared as session attributes via @sessionattributes are ignored.
select a default view name when a controller did not specify it. use the request path the leading and trailing slash stripped.
extract the error codes from the objecterror list.
create a new bindstatus instance, representing a field or object status. will be resolved (e.g. "customer.address.street")
extract the error messages from the objecterror list.
constructor with a fully initialized httpmessagewriter.
retrieve the errors instance for the given bind object.
return a context-aware url for the given relative url with placeholders -- named keys with braces . for example, send in a relative url and the result will be [contextpath]foobaz?spam=nuts. absolute path also url-encoded accordingly
append the query of the current request to the target redirect url.
whether the given targeturl has a host that is a "foreign" system in which case javax.servlet.http.httpservletresponse#encoderedirecturl will not be applied. this method returns true if the #sethosts(string[]) property is configured and the target url has a host that does not match. the url does not have a host or the "host" property is not configured.
send a redirect back to the http client.
create the target url and, if necessary, pre-pend the contextpath, expand uri template variables, append the current request query, and apply the configured #getrequestdatavalueprocessor() requestdatavalueprocessor.
expand uri template variables in the target url with either model attribute values or as a fallback with uri variable values from the current request. values are encoded.
creates a new view instance of the specified view class and configures it. does not perform any lookup for pre-defined view instances. spring lifecycle methods as defined by the bean container do not have to be called here: they will be automatically applied afterwards, provided that an #setapplicationcontext applicationcontext is available.
apply the containing applicationcontext's lifecycle methods to the given view instance, if such a context is available. or a decorated variant)
set the view class to instantiate through #createview(string). which by default is abstracturlbasedview
return the requestdatavalueprocessor to use. the default implementation looks in the #getapplicationcontext() spring configuration for a requestdatavalueprocessor bean with the name #request_data_value_processor_bean_name. application context.
prepare the model to use for rendering. the default implementation creates a combined output map that includes model as well as static attributes with the former taking precedence.
by default, resolve async attributes supported by the view implementations capable of taking advantage of reactive types can override this method if needed.
prepare the model to render. objects as values (map can also be null in case of empty model) match one of the #getsupportedmediatypes() supported media types.
check that the freemarker template used for this view exists and is valid. can be overridden to customize the behavior, for example in case of multiple templates to be rendered into a single view.
autodetect a freemarkerconfig object via the applicationcontext.
build a freemarker template model for the given model map. the default implementation builds a simplehash.
return the configured freemarker objectwrapper, or the
see scripttemplateconfigurer#setresourceloaderpath(string) documentation.
returns the media types for this condition excluding negated expressions.
checks if any of the contained media type expressions match the given request 'content-type' header and returns an instance that is guaranteed to contain matching expressions only. the match is performed via or a new condition with matching expressions only; or null if no expressions match.
on a pre-flight request match to the would-be, actual request. hence empty conditions is a match, otherwise try to match to the http method in the "access-control-request-method" header.
returns a new instance with a union of the http request methods from "this" and the "other" instance.
if one instance is empty, the other "wins". if both instances have conditions, compare them in the order in which they were provided.
delegate to all contained conditions to match the request and return the resulting "matching" condition instances. an empty compositerequestcondition matches to all requests.
if one instance is empty, return the other. if both instances have conditions, combine the individual conditions after ensuring they are of the same type and number.
private constructor with already parsed media type expressions.
same as #producesrequestcondition(string[], string[]) but also accepting a contentnegotiationmanager.
checks if any of the contained media type expressions match the given request 'content-type' header and returns an instance that is guaranteed to contain matching expressions only. the match is performed via or a new condition with matching expressions; or null if no expressions match.
return the contained producible media types excluding negated expressions.
compares this and another "produces" condition as follows:  sort 'accept' header media types by quality value via get the first index of matching media types in each "produces" condition first matching with mediatype#equals(object) and then with mediatype#includes(mediatype). if a lower index is found, the condition at that index wins. if both indexes are equal, the media types at the index are compared further with mediatype#specificity_comparator.  it is assumed that both instances have been obtained via contains the matching producible media type expression only or is otherwise empty.
checks if any of the patterns match the given request and returns an instance that is guaranteed to contain matching patterns, sorted. or a new condition with sorted matching patterns; or null if no patterns match.
compare the two conditions based on the url patterns they contain. patterns are compared one at a time, from top to bottom. if all compared patterns match equally, but one instance has more patterns, it is considered a closer match. it is assumed that both instances have been obtained via contain only patterns that match the request and are sorted with the best matches on top.
returns a new instance with url patterns from the current instance ("this") and the "other" instance as follows:  if there are patterns in both instances, combine the patterns in "this" with the patterns in "other" using pathpattern#combine(pathpattern). if only one instance has patterns, use them. if neither instance has patterns, use an empty string (i.e. ""). 
find the patterns matching the given lookup path. invoking this method should yield results equivalent to those of calling this method is provided as an alternative to be used if no request is available (e.g. introspection, tooling, etc).
returns a new instance with the union of the header expressions from "this" and the "other" instance.
returns "this" instance if the request matches all expressions; or null otherwise.
returns "this" instance if the request matches all param expressions; or null otherwise.
returns a new instance with the union of the param expressions from "this" and the "other" instance.
combine the request conditions held by the two requestconditionholder instances after making sure the conditions are of the same type. or if one holder is empty, the other holder is returned.
ensure the held request conditions are of the same type.
get the matching condition for the held request condition wrap it in a new requestconditionholder instance. or otherwise if this is an empty holder, return the same holder instance.
register a script template view resolver with an empty default view name prefix and suffix. note that you must also configure script templating by adding a scripttemplateconfigurer bean.
register a freemarkerviewresolver with a ".ftl" suffix. note that you must also configure freemarker by adding a freemarkerconfigurer bean.
enable cross origin request handling for the specified path pattern. exact path mapping uris (such as "admin") are supported as well as ant-style path patterns (such as "admin"). the following defaults are applied to the corsregistration:  allow all origins. allow "simple" methods get, head and post. allow all headers. set max age to 1800 seconds (30 minutes). 
returns a resourcewebhandler instance.
return a handler mapping with the mapped resource handlers; or null in case of no registrations.
add a resource handler for serving static resources based on the specified url path patterns. the handler will be invoked for every incoming request that matches to one of the specified path patterns. patterns like "static" or "cssfilename:\\w+\\.css" are allowed. see org.springframework.web.util.pattern.pathpattern for more details on the syntax. the registered resource handler
configure a path prefix to apply to matching controller methods. prefixes are used to enrich the mappings of every @requestmapping method whose controller type is matched by the corresponding consider using org.springframework.web.method.handlertypepredicate handlertypepredicate to group controllers.
return the configurablewebbindinginitializer to use for initializing all webdatabinder instances.
return a global validator instance for example for validating delegates to #getvalidator() first. if that returns null checks the classpath for the presence of a jsr-303 implementations before creating a optionalvalidatorfactorybean. if a jsr-303 implementation is not available, a "no-op" validator is returned.
return a formattingconversionservice for use with annotated controllers. see #addformatters as an alternative to overriding this method.
return a handler mapping ordered at integer.max_value-1 with mapped resource handlers. to configure resource handling, override
callback for building the pathmatchconfigurer. this method is final, use #configurepathmatching to customize path matching.
callback for building the global cors configuration. this method is final. use #addcorsmappings(corsregistry) to customize the cors conifg.
callback for building the viewresolverregistry. this method is final, use #configureviewresolvers to customize view resolvers.
a shortcut for decoding the raw content of the message to text with the given character encoding. this is useful for text websocket messages, or otherwise when the payload is expected to contain text.
upgrade to a websocket session and handle it with the given handler. websocket session handling.
constructor with an additional maxframepayloadlength argument.
create a new websocket session.
handle a close callback from the websockethandler adapter.
handle an error callback from the websockethandler adapter.
alternative constructor with completion mono<void> to propagate the session completion (success or error) (for client-side use).
build a requestedcontenttyperesolver that delegates to the list of resolvers configured through this builder.
private factory method to create the resolver.
add a resolver to get the requested content type from a query parameter. by default the query parameter name is "format".
register all handlers specified in the url map for the corresponding paths.
look up a handler instance for the given url lookup path. supports direct matches, e.g. a registered "test" matches "test", and various path pattern matches, e.g. a registered "t" matches both "test" and "team". for details, see the pathpattern class.
register the specified handler for the given url path. (a bean name will automatically be resolved into the corresponding handler bean)
set the "global" cors configurations based on url patterns. by default the first matching url pattern is combined with handler-level cors configuration if any.
find the resource under the given location. the default implementation checks if there is a readable
insert a fixed, prefix-based version in resource urls that match the given path patterns, for example: "versionjsmain.js". this is useful (vs. content-based versions) when using javascript module loaders. the version may be a random number, the current date, or a value fetched from a git commit sha, a property file, or environment variable and set with spel expressions in the configuration (e.g. see @value in java config). if not done already, variants of the given pathpatterns, prefixed with the version will be also configured. for example, adding a "js" path pattern will also cofigure automatically a "v1.0.0js" with "v1.0.0" the relative to the pattern configured with the resource handler
find a versionstrategy for the request path of the requested resource.
transform the given relative request path to an absolute path, taking the path of the given request as a point of reference. the resulting path is also cleaned from sequences like "path..".
a transformer can use this method when a resource being transformed contains links to other resources. such links need to be replaced with the public facing link as determined by the resource resolver chain (e.g. the public url may have a version inserted).
manually configure resource handler mappings. note: by default resource mappings are auto-detected from the spring applicationcontext. if this property is used, auto-detection is turned off.
get the public resource url for the given uri string. the uri string is expected to be a path and if it contains a query or fragment those will be preserved in the resulting public resource url.
check whether the given path contains invalid escape sequences.
identifies invalid resource paths. by default rejects:  paths that contain "web-inf" or "meta-inf" paths that contain ".." after a call to paths that represent a resourceutils#isurl valid url or would represent one after the leading slash is removed.  note: this method assumes that leading, duplicate '' or control characters (e.g. white space) have been trimmed so that the path starts predictably with a single '' or does not have one.
processes a resource request. checks for the existence of the requested resource in the configured list of locations. if the resource does not exist, a 404 response will be returned to the client. if the resource exists, the request will be checked for the presence of the timestamp of the given resource, returning a 304 status code if the of the resource will be written to the response with caching headers set to expire one year in the future.
set headers on the response. called for both get and head requests.
look for a pathresourceresolver among the configured resource resolvers and set its allowedlocations property (if empty) to match the #setlocations locations configured on this class.
create a new springobjenesis instance with the given standard instantiator strategy.
visits an iinc instruction.
constructs a new methodvisitor. opcodes#asm4, opcodes#asm5, opcodes#asm6 or opcodes#asm7. be null.
visits a zero operand instruction. aconst_null, iconst_m1, iconst_0, iconst_1, iconst_2, iconst_3, iconst_4, iconst_5, lconst_0, lconst_1, fconst_0, fconst_1, fconst_2, dconst_0, dconst_1, iaload, laload, faload, daload, aaload, baload, caload, saload, iastore, lastore, fastore, dastore, aastore, bastore, castore, sastore, pop, pop2, dup, dup_x1, dup_x2, dup2, dup2_x1, dup2_x2, swap, iadd, ladd, fadd, dadd, isub, lsub, fsub, dsub, imul, lmul, fmul, dmul, idiv, ldiv, fdiv, ddiv, irem, lrem, frem, drem, ineg, lneg, fneg, dneg, ishl, lshl, ishr, lshr, iushr, lushr, iand, land, ior, lor, ixor, lxor, i2l, i2f, i2d, l2i, l2f, l2d, f2i, f2l, f2d, d2i, d2l, d2f, i2b, i2c, i2s, lcmp, fcmpl, fcmpg, dcmpl, dcmpg, ireturn, lreturn, freturn, dreturn, areturn, return, arraylength, athrow, monitorenter, or monitorexit.
visits an annotation on an instruction. this method must be called just after the annotated instruction. it can be called several times for the same instruction. typereference#constructor_reference, typereference#method_reference, typereference#cast, typereference#constructor_invocation_type_argument, typereference#method_invocation_type_argument, typereference#constructor_reference_type_argument, or typereference#method_reference_type_argument. see typereference. static inner type within 'typeref'. may be @literal null if the annotation targets 'typeref' as a whole. interested in visiting this annotation.
visits a non standard attribute of this method.
visits an annotation on an exception handler type. this method must be called after the for the same exception handler. static inner type within 'typeref'. may be @literal null if the annotation targets 'typeref' as a whole. interested in visiting this annotation.
visits a method instruction. a method instruction is an instruction that invokes a method. invokevirtual, invokespecial, invokestatic or invokeinterface. type#getinternalname()).
visits a try catch block. null to catch any exceptions (for "finally" blocks). (by the #visitlabel method).
visits the default value of this annotation interface method. @literal null if this visitor is not interested in visiting this default value. the 'name' parameters passed to the methods of this annotation visitor are ignored. moreover, exacly one visit method must be called on this annotation visitor, followed by visitend.
visits an invokedynamic instruction. an integer, float, long, double, string, type, handle or constantdynamic value. this method is allowed to modify the content of the array so a caller should expect that this array may change.
visits a multianewarray instruction.
visits a local variable instruction. a local variable instruction is an instruction that loads or stores the value of a local variable. iload, lload, fload, dload, aload, istore, lstore, fstore, dstore, astore or ret. variable.
visits an annotation of this method. interested in visiting this annotation.
visits a line number declaration. compiled. (by the #visitlabel method).
visits a label. a label designates the instruction that will be visited just after it.
visits a parameter of this method. orand acc_mandated are allowed (see opcodes).
visits the current state of the local variables and operand stack elements. this method must() be called just before any instruction i that follows an unconditional branch instruction such as goto or throw, that is the target of a jump instruction, or that starts an exception handler block. the visited types must describe the values of the local variables and of the operand stack elements just before i is executed.  () this is mandatory only for classes whose version is greater than or equal to opcodes#v1_6.   the frames of a method must be given either in expanded form, or in compressed form (all frames must use the same format, i.e. you must not mix expanded and compressed frames within a single method):  in expanded form, all frames must have the f_new type. in compressed form, frames are basically "deltas" from the state of the previous frame:   opcodes#f_same representing frame with exactly the same locals as the previous frame and with the empty stack.  opcodes#f_same1 representing frame with exactly the same locals as the previous frame and with single value on the stack ( numstack is 1 and stack[0] contains value for the type of the stack item).  opcodes#f_append representing frame with current locals are the same as the locals in the previous frame, except that additional locals are defined ( numlocal is 1, 2 or 3 and local elements contains values representing added types).  opcodes#f_chop representing frame with current locals are the same as the locals in the previous frame, except that the last 1-3 locals are absent and with the empty stack (numlocal is 1, 2 or 3).  opcodes#f_full representing complete frame data.    in both cases the first frame, corresponding to the method's parameters and access flags, is implicit and must not be visited. also, it is illegal to visit two or more frames for the same code location (i.e., at least one instruction must be visited between two calls to visitframe). frames, or opcodes#f_full, opcodes#f_append, opcodes#f_chop, opcodes#f_same or opcodes#f_append, opcodes#f_same1 for compressed frames. types are represented by opcodes#top, opcodes#integer, opcodes#float, opcodes#long, opcodes#double, opcodes#null or reference types are represented by string objects (representing internal names), and uninitialized types by label objects (this label designates the new instruction that created this uninitialized value). content has the same format as the "local" array. instruction between the two (unless this frame is a opcodes#f_same frame, in which case it is silently ignored).
visits an annotation of a parameter this method. parameters in the method descriptor, and strictly smaller than the parameter count specified in #visitannotableparametercount. important note: a parameter index i is not required to correspond to the i'th parameter descriptor in the method descriptor, in particular in case of synthetic parameters (see https:docs.oracle.comjavasespecsjvmsse9htmljvms-4.html#jvms-4.7.18). interested in visiting this annotation.
visits the end of the method. this method, which is the last one to be called, is used to inform the visitor that all the annotations and attributes of the method have been visited.
visits the maximum stack size and the maximum number of local variables of the method.
visits a local variable declaration. variable type does not use generic types. (inclusive). visitor (by the #visitlabel method).
starts the visit of the method's code, if any (i.e. non abstract method).
visits a lookupswitch instruction. handler block for the keys[i] key.
visits a method instruction. a method instruction is an instruction that invokes a method. invokevirtual, invokespecial, invokestatic or invokeinterface. type#getinternalname()).
visits an annotation on a local variable type. typereference. static inner type within 'typeref'. may be @literal null if the annotation targets 'typeref' as a whole. of this local variable (inclusive). this local variable (exclusive). this array must have the same size as the 'start' array. the 'start' array. interested in visiting this annotation.
visits an instruction with a single int operand. or newarray. when opcode is bipush, operand value should be between byte.min_value and byte.max_value.  when opcode is sipush, operand value should be between short.min_value and short.max_value.  when opcode is newarray, operand value should be one of opcodes#t_boolean, opcodes#t_char, opcodes#t_float, opcodes#t_double, opcodes#t_byte,
visits an annotation on a type in the method signature. typereference#method_type_parameter_bound, typereference#method_return, typereference#method_receiver, typereference#method_formal_parameter or typereference#throws. see typereference. static inner type within 'typeref'. may be @literal null if the annotation targets 'typeref' as a whole. interested in visiting this annotation.
visits the number of method parameters that can have annotations. by default (i.e. when this method is not called), all the method parameters defined by the method descriptor can have annotations. must be less or equal than the number of parameter types in the method descriptor. it can be strictly less when a method has synthetic parameters and when these parameters are ignored when computing parameter indices for the purpose of parameter annotations (see https:docs.oracle.comjavasespecsjvmsse9htmljvms-4.html#jvms-4.7.18). annotations visible at runtime, @literal false to define the number of method parameters that can have annotations invisible at runtime.
visits a tableswitch instruction. handler block for the min + i key.
visits a jump instruction. a jump instruction is an instruction that may jump to another instruction. ifne, iflt, ifge, ifgt, ifle, if_icmpeq, if_icmpne, if_icmplt, if_icmpge, if_icmpgt, if_icmple, if_acmpeq, if_acmpne, goto, jsr, ifnull or ifnonnull. designates the instruction to which the jump instruction may jump.
visits a field instruction. a field instruction is an instruction that loads or stores the value of a field of an object. getstatic, putstatic, getfield or putfield.
visits a ldc instruction. note that new constant types may be added in future versions of the java virtual machine. to easily detect new constant types, implementations of this method should check for unexpected constant types, like this:  if (cst instanceof integer) ... else if (cst instanceof float) ... else if (cst instanceof long) ... else if (cst instanceof double) ... else if (cst instanceof string) ... else if (cst instanceof type) int sort = ((type) cst).getsort(); if (sort == type.object) ... else if (sort == type.array) ... else if (sort == type.method) ... else throw an exception else if (cst instanceof handle) ... else if (cst instanceof constantdynamic) ... else throw an exception  integer, a float, a long, a double, a string, a type of object or array sort for .class constants, for classes whose version is 49, a type of method sort for methodtype, a handle for methodhandle constants, for classes whose version is 51 or a constantdynamic for a constant dynamic for classes whose version is 55.
visits a type instruction. a type instruction is an instruction that takes the internal name of a class as parameter. anewarray, checkcast or instanceof. name of an object or array class (see type#getinternalname()).
put the given abstract type in the given bytevector, using the jvms verification_type_info format used in stackmaptable attributes. frame#reference_kind or frame#uninitialized_kind types. 4.7.4
sets the input frame from the given method description. this method is used to initialize the first frame of a method, which is implicit (i.e. not stored explicitly in the stackmaptable attribute).
pops as many abstract types from the output frame stack as described by the given descriptor.
merges the type at the given index in the given abstract type array with the given type. returns @literal true if the type array has been modified by this operation. this type should be of #constant_kind, #reference_kind or #uninitialized_kind kind, with positive or null array dimensions. dimensions.
adds an abstract type to the list of types on which a constructor is invoked in the basic block.
merges the input frame of the given frame with the input and output frames of this (the input and output frames of this frame are never changed). of a successor, in the control flow graph, of the basic block corresponding to this frame. table index of the caught exception type, otherwise 0.
simulates the action of the given instruction on the output stack frame.
sets the input frame from the given public api frame description. methodvisitor#visitframe. methodvisitor#visitframe.
returns the abstract type corresponding to the given type descriptor.
pushes the given abstract type on the output frame stack.
returns the "initialized" abstract type corresponding to the given abstract type. uninitialized_this or an uninitialized_kind abstract type for one of the types on which a constructor is invoked in the basic block. otherwise returns abstracttype.
makes the given methodwriter visit the input frame of this frame. the visit is done with the methodwriter#visitframestart, methodwriter#visitabstracttype and frame.
pushes the abstract type corresponding to the given descriptor on the output frame stack.
returns the abstract type corresponding to the given public api frame element type. methodvisitor#visitframe, i.e. either opcodes#top, opcodes#integer, opcodes#float, opcodes#long, opcodes#double, opcodes#null, or a new instruction (for uninitialized types).
replaces the abstract type stored at the given local variable index in the output frame.
visits an enumeration value of the annotation.
visits an array value of the annotation. note that arrays of primitive types (such as byte, boolean, short, char, int, long, float or double) can be passed as value to #visit visit. this is what classreader does. is not interested in visiting these values. the 'name' parameters passed to the methods of this visitor are ignored. all the array values must be visited before calling other methods on this annotation visitor.
visits a primitive value of the annotation. character, short, integer , long, float, double, value can also be an array of byte, boolean, short, char, int, long, float or double values (this is equivalent to using #visitarray and visiting each array element in turn, but is more convenient).
constructs a new annotationvisitor. opcodes#asm4, opcodes#asm5, opcodes#asm6 or opcodes#asm7. calls. may be null.
visits a nested annotation value of the annotation. visitor is not interested in visiting this nested annotation. the nested annotation value must be fully visited before calling other methods on this annotation visitor.
visits the end of the annotation.
puts the module, modulepackages and modulemainclass attributes generated by this modulewriter in the given bytevector.
returns the size of the module, modulepackages and modulemainclass attributes generated by this modulewriter. also add the names of these attributes in the constant pool.
visits an annotation of the field. interested in visiting this annotation.
visits the end of the field. this method, which is the last one to be called, is used to inform the visitor that all the annotations and attributes of the field have been visited.
visits a non standard attribute of the field.
constructs a new fieldvisitor. opcodes#asm4, opcodes#asm5, opcodes#asm6 or opcodes#asm7. null.
visits an annotation on the type of the field. static inner type within 'typeref'. may be @literal null if the annotation targets 'typeref' as a whole. interested in visiting this annotation.
finds the basic blocks that end a subroutine starting with the basic block corresponding to this label and, for each one of them, adds an outgoing edge to the basic block following the given subroutine call. in other words, completes the control flow graph by adding the edges corresponding to the return from this subroutine, when called from the given caller basic block. note: a precondition and postcondition of this method is that all labels must have a null this label. this label is supposed to correspond to the start of a subroutine.
makes the given visitor visit this label and its source line numbers, if applicable.
returns the bytecode offset corresponding to this label. this offset is computed from the start of the method's bytecode. this method is intended for attribute sub classes, and is normally not needed by class generators or adapters.
adds a source line number corresponding to this label.
puts a reference to this label in the bytecode of a method. if the bytecode offset of the label is known, the relative bytecode offset between the label and the instruction referencing it is computed and written directly. otherwise, a null relative offset is written and a new forward reference is declared for this label. reference to be appended.
finds the basic blocks that belong to the subroutine starting with the basic block corresponding to this label, and marks these blocks as belonging to this subroutine. this method follows the control flow graph to find all the blocks that are reachable from the current basic block without following any jsr target. note: a precondition and postcondition of this method is that all labels must have a null this label.
adds a forward reference to this label. this method must be called only for a true forward reference, i.e. only if this label is not resolved yet. for backward references, the relative bytecode offset of the reference can be, and must be, computed and stored directly. reference stored at referencehandle. #forward_reference_type_wide. stored.
returns the total size in bytes of all the attributes in the attribute list that begins with this attribute. this size includes the 6 header bytes (attribute_name_index and attribute_length) per attribute. also adds the attribute type names to the constant pool. null if they are not code attributes. corresponds to the 'code' field of the code attribute. attributes, or 0 if they are not code attributes. corresponds to the 'code_length' field of the code attribute. -1 if they are not code attributes. code attributes, or -1 if they are not code attribute. the attribute headers.
returns the byte array form of the content of this attribute. the 6 header bytes (attribute_name_index and attribute_length) must not be added in the returned bytevector. to add the items that corresponds to this attribute to the constant pool of this class. if this attribute is not a code attribute. corresponds to the 'code' field of the code attribute. attribute, or 0 if this attribute is not a code attribute. corresponds to the 'code_length' field of the code attribute. -1 if this attribute is not a code attribute. attribute, or -1 if this attribute is not a code attribute.
puts all the attributes of the attribute list that begins with this attribute, in the given byte vector. this includes the 6 header bytes (attribute_name_index and attribute_length) per attribute. null if they are not code attributes. corresponds to the 'code' field of the code attribute. attributes, or 0 if they are not code attributes. corresponds to the 'code_length' field of the code attribute. -1 if they are not code attributes. code attributes, or -1 if they are not code attribute.
reads a #type attribute. this method must return a new attribute object, of type #type, corresponding to the 'length' bytes starting at 'offset', in the given classreader. 6 attribute header bytes (attribute_name_index and attribute_length) are not taken into account here. 'charbuffer' parameter. in classreader#b, or -1 if the attribute to be read is not a code attribute. the 6 attribute header bytes (attribute_name_index and attribute_length) are not taken into account here. is not a code attribute.
adds a merged type in the type table of this symbol table. does nothing if the type table already contains a similar type. table. table. corresponding to the common super class of the given types.
adds a constant_utf8_info to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item.
adds a bootstrap method to the bootstrapmethods attribute of this symbol table. does nothing if the bootstrapmethods already contains a similar bootstrap method.
adds a constant_dynamic or a constant_invokedynamic_info to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item. symbol#constant_invoke_dynamic_tag. constant_invoke_dynamic_tag.
adds an frame#item_uninitialized type in the type table of this symbol table. does nothing if the type table already contains a similar type. frame#item_uninitialized type value.
adds a constant_integer_info or constant_float_info to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item.
adds a new constant_dynamic_info or constant_invokedynamic_info to the constant pool of this symbol table. symbol#constant_invoke_dynamic_tag. constant_invoke_dynamic_tag.
adds a type in the type table of this symbol table. does nothing if the type table already contains a similar type.
adds a new constant_fieldref_info, constant_methodref_info or constant_interfacemethodref_info to the constant pool of this symbol table. or symbol#constant_interface_methodref_tag.
adds a constant_class_info, constant_string_info, constant_methodtype_info, constant_module_info or constant_package_info to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item. symbol#constant_method_type_tag, symbol#constant_module_tag or symbol#constant_package_tag. package name, depending on tag.
adds a constant_nameandtype_info to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item.
adds a new constant_methodhandle_info to the constant pool of this symbol table. opcodes#h_putfield, opcodes#h_putstatic, opcodes#h_invokevirtual, opcodes#h_invokestatic, opcodes#h_invokespecial, opcodes#h_newinvokespecial or opcodes#h_invokeinterface.
constructs a new symboltable for the given classwriter, initialized with the constant pool and bootstrap methods of the given classreader. initialize the symboltable.
puts the given entry in the #entries hash set. this method does not check whether #entries already contains a similar entry or not. #entries is resized if necessary to avoid hash collisions (multiple entries needing to be stored at the same #entries array index) as much as possible, with reasonable memory usage.
adds a constant_methodhandle_info to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item. opcodes#h_putfield, opcodes#h_putstatic, opcodes#h_invokevirtual, opcodes#h_invokestatic, opcodes#h_invokespecial, opcodes#h_newinvokespecial or opcodes#h_invokeinterface.
constructs a new, empty symboltable for the given classwriter.
adds the given type symbol to #typetable. the index of this symbol must be equal to the current value of #typecount. entry's index by hypothesis.
adds a constant_fieldref_info, constant_methodref_info or constant_interfacemethodref_info to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item. or symbol#constant_interface_methodref_tag.
puts this symbol table's bootstrapmethods attribute in the given bytevector. this includes the 6 attribute header bytes and the num_bootstrap_methods value.
adds a number or string constant to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item. an integer, byte, character, short, boolean, float, long, double, string, type or handle.
adds a bootstrap method to the bootstrapmethods attribute of this symbol table. does nothing if the bootstrapmethods already contains a similar bootstrap method (more precisely, reverts the content of #bootstrapmethods to remove the last, duplicate bootstrap method).
read the bootstrapmethods 'bootstrap_methods' array binary content and add them as entries of the symboltable. symboltable.
adds a constant_long_info or constant_double_info to the constant pool of this symbol table. does nothing if the constant pool already contains a similar item.
puts a runtime[in]visible[type]annotations attribute containing this annotations and all its predecessors (see #previousannotation in the given bytevector. annotations are put in the same order they have been visited. "runtime[in]visible[type]annotations").
puts a runtime[in]visibleparameterannotations attribute containing all the annotation lists from the given annotationwriter sub-array in the given bytevector. runtime[in]visibleparameterannotations). element). [0..annotableparametercount[ are put).
returns the size of a runtime[in]visibleparameterannotations attribute containing all the annotation lists from the given annotationwriter sub-array. also adds the attribute name to the constant pool of the class. element). (elements [0..annotableparametercount[ are taken into account). to the given sub-array of annotationwriter lists. this includes the size of the attribute_name_index and attribute_length fields.
returns the size of a runtime[in]visible[type]annotations attribute containing this annotation and all its predecessors (see #previousannotation. also adds the attribute name to the constant pool of the class (if not null). annotation and all its predecessors. this includes the size of the attribute_name_index and attribute_length fields.
visits the nest host class of the class. a nest is a set of classes of the same package that share access to their private members. one of these classes, called the host, lists the other members of the nest, which in turn should link to the host of their nest. this method must be called only once and only if the visited class is a non-host member of a nest. a class is implicitly its own nest, so it's invalid to call this method with the visited class name as argument.
visits the enclosing class of the class. this method must be called only if the class has an enclosing class. not enclosed in a method of its enclosing class. the class is not enclosed in a method of its enclosing class.
visits an annotation on a type in the class signature. typereference#class_type_parameter_bound or typereference#class_extends. see static inner type within 'typeref'. may be @literal null if the annotation targets 'typeref' as a whole. interested in visiting this annotation.
visit the module corresponding to the class. acc_mandated. interested in visiting this module.
visits a field of the class. the field is synthetic andor deprecated. generic types. field does not have an initial value, must be an integer, a float, a long, a double or a string (for int, float, long or string fields respectively). this parameter is only used for static fields. its value is ignored for non static fields, which must be initialized through bytecode instructions in constructors or methods. visitor is not interested in visiting these annotations and attributes.
visits the end of the class. this method, which is the last one to be called, is used to inform the visitor that all the fields and methods of the class have been visited.
visits a member of the nest. a nest is a set of classes of the same package that share access to their private members. one of these classes, called the host, lists the other members of the nest, which in turn should link to the host of their nest. this method must be called only if the visited class is the host of a nest. a nest host is implicitly a member of its own nest, so it's invalid to call this method with the visited class name as argument.
visits the header of the class. and the major version in the 16 least significant bits. the class is deprecated. generic one, and does not extend or implement generic classes or interfaces. for interfaces, the super class is object. may be @literal null, but only for the type#getinternalname()). may be @literal null.
visits information about an inner class. this inner class is not necessarily a member of the class being visited. type#getinternalname()). may be @literal null for not member classes. @literal null for anonymous inner classes. class.
visits a non standard attribute of the class.
visits an annotation of the class. interested in visiting this annotation.
visits a method of the class. this method must return a new methodvisitor instance (or @literal null) each time it is called, i.e., it should not return a previously returned visitor. the method is synthetic andor deprecated. return type and exceptions do not use generic types. type#getinternalname()). may be @literal null. visitor is not interested in visiting the code of this method.
visits the source of the class. null. compiled elements of the class. may be @literal null.
constructs a new classvisitor. opcodes#asm4, opcodes#asm5, opcodes#asm6 or opcodes#asm7. null.
returns the size of the method_info jvms structure generated by this methodwriter. also add the names of the attributes of this method in the constant pool.
computes the maximum stack size of the method.
constructs a new methodwriter.
puts the given public api frame element type in #stackmaptableentries , using the jvms verification_type_info format used in stackmaptable attributes. methodvisitor#visitframe, i.e. either opcodes#top, opcodes#integer, opcodes#float, opcodes#long, opcodes#double, opcodes#null, or a new instruction (for uninitialized types).
computes all the stack map frames of the method, from scratch.
ends the visit of #currentframe by writing it in the stackmaptable entries and by updating the stackmaptable number_of_entries (except if the current frame is the first one, which is implicit in stackmaptable). then resets #currentframe to @literal null.
compresses and writes #currentframe in a new stackmaptable entry.
ends the current basic block. this method must be used in the case where the current basic block does not have any successor. warning: this method must be called after the currently visited instruction has been put in block after the current instruction).
returns whether the attributes of this method can be copied from the attributes of the given method (assuming there is no method visitor between the given classreader and this methodwriter). this method should only be called just after this methodwriter has been created, and before any content is visited. it returns true if the attributes corresponding to the constructor arguments (at most a signature, an exception, a deprecated and a synthetic attribute) are the same as the corresponding attributes in the given method. the attributes of this method might be copied. the attributes of this method might be copied. of this method might be copied contains a synthetic attribute. of this method might be copied contains a deprecated attribute. the attributes of this method might be copied. method_info jvms structure from which the attributes of this method might be copied, or 0. jvms structure from which the attributes of this method might be copied, or 0. method_info jvms structure in 'source.b', between 'methodinfooffset' and 'methodinfooffset' + 'methodinfolength'.
starts the visit of a new stack map frame, stored in #currentframe.
puts the content of the method_info jvms structure generated by this methodwriter into the given bytevector.
constructs a new fieldwriter.
returns the size of the field_info jvms structure generated by this fieldwriter. also adds the names of the attributes of this field in the constant pool.
puts the content of the field_info jvms structure generated by this fieldwriter into the given bytevector.
puts the type_path jvms structure corresponding to the given typepath into the given bytevector.
returns a string representation of this type path. #array_element steps are represented with '[', #inner_type steps with '.', #wildcard_bound steps with '' and #type_argument steps with their type argument index in decimal form followed by ';'.
converts a type path in string form, in the format used by #tostring(), into a typepath object. @literal null or empty.
visit a package of the current module.
visit an open package of the current module. acc_synthetic and acc_mandated. reflection to the classes of the open package, or @literal null.
visit a service used by the current module. the name must be the internal name of an interface or a class.
visit an implementation of a service. one provider).
constructs a new modulevisitor. or opcodes#asm7. be null.
visits the end of the module. this method, which is the last one to be called, is used to inform the visitor that everything have been visited.
visit the main class of the current module.
visit an exported package of the current module. acc_synthetic and acc_mandated. classes of the exported package, or @literal null.
visits a dependence of the current module. acc_static_phase, acc_synthetic and acc_mandated.
puts the given target_type and target_info jvms structures into the given bytevector. #targettypeandinfo. local_variable and resource_variable target types are not supported.
returns the size of values of this type. this method must not be used for method types.
appends the descriptor of the given class to the given string builder.
returns the type values corresponding to the argument types of the given method.
tests if the given object is equal to this type.
returns the type values corresponding to the argument types of the given method descriptor. descriptor.
returns the descriptor corresponding to the given constructor.
returns the type corresponding to the given field or method descriptor. descriptorbuffer. descriptorbuffer.
returns the number of dimensions of this array type. this method should only be used for an array type.
appends the descriptor corresponding to this type to the given string buffer.
returns the binary name of the class corresponding to this type. this method must not be used on method types.
returns a hash code value for this type.
returns the descriptor corresponding to the given class.
computes the size of the arguments and of the return value of a method. argumentssize, and the size of its return value, returnsize, packed into a single int i = i >> 2, and returnsize to i & 0x03).
returns the descriptor corresponding to the given argument and return types.
returns the type corresponding to the given class.
returns a jvm instruction opcode adapted to this type. this method must not be used for method types. iastore, iadd, isub, imul, idiv, irem, ineg, ishl, ishr, iushr, iand, ior, ixor and ireturn. example, if this type is float and opcode is ireturn, this method returns freturn.
returns the descriptor corresponding to the given method.
returns the type corresponding to the return type of the given method descriptor.
returns the descriptor corresponding to this type.
adds a handle to the constant pool of the class being build. does nothing if the constant pool already contains a similar item. this method is intended for attribute sub classes, and is normally not needed by class generators or adapters. opcodes#h_getstatic, opcodes#h_putfield, opcodes#h_putstatic, opcodes#h_invokevirtual, opcodes#h_invokestatic, opcodes#h_invokespecial,
returns the equivalent of the given class file, with the asm specific instructions replaced with standard ones. this is done with a classreader -> classwriter round trip. classwriter. ones.
returns the common super type of the two given types. the default implementation of this method loads the two given classes and uses the java.lang.class methods to find the common super class. it can be overridden to compute this common super type in other ways, in particular without actually loading any class, or to take into account the class that is currently being generated by this classwriter, which can of course not be loaded since it is under construction.
constructs a new classwriter object and enables optimizations for "mostly add" bytecode transformations. these optimizations are the following:  the constant pool and bootstrap methods from the original class are copied as is in the new class, which saves time. new constant pool entries and new bootstrap methods will be added at the end if necessary, but unused constant pool entries or bootstrap methods won't be removed. methods that are not transformed are copied as is in the new class, directly from the original class bytecode (i.e. without emitting visit events for all the method instructions), which saves a lot of time. untransformed methods are detected by the fact that the classreader receives methodvisitor objects that come from a classwriter (and not from any other classvisitor instance).  copy the entire constant pool and bootstrap methods from the original class and also to copy other fragments of original bytecode where applicable. zero or more of #compute_maxs and #compute_frames. these option flags do not affect methods that are copied as is in the new class. this means that neither the maximum stack size nor the stack frames will be computed for these methods.
returns the prototypes of the attributes used by this class, its fields and its methods.
returns the content of the class file that was built by this classwriter.
adds a dynamic constant reference to the constant pool of the class being build. does nothing if the constant pool already contains a similar item. this method is intended for attribute sub classes, and is normally not needed by class generators or adapters.
adds an invokedynamic reference to the constant pool of the class being build. does nothing if the constant pool already contains a similar item. this method is intended for attribute sub classes, and is normally not needed by class generators or adapters.
enlarges this byte vector so that it can receive 'size' more bytes.
puts an utf8 string into this byte vector. the byte vector is automatically enlarged if necessary. the string length is encoded in two bytes before the encoded characters, if there is space for that (i.e. if this.length - offset - 2 >= 0). to have already been encoded, using only one byte per character. encoded characters.
removes the range between start and end from the handler list that begins with the given element.
puts the jvms exception_table corresponding to the handler list that begins with the given element. this includes the exception_table_length field.
reads a constant_dynamic constant pool entry in #b. pool table. large. it is not automatically resized.
reads a numeric or string constant pool entry in #b. this method is intended for constant_double, constant_class, constant_string, constant_methodtype, constant_methodhandle or constant_dynamic entry in the class's constant pool. large. it is not automatically resized. constant pool entry.
reads a jvms field_info structure and makes the given visitor visit it.
reads the element values of a jvms 'annotation' structure and makes the given visitor visit them. this method can also be used to read the values of the jvms 'array_value' field of an annotation's 'element_value'. field) or of an 'array_value' structure. of a jvms 'annotation' structure, and false to parse the jvms 'array_value' of an annotation's element_value.
reads the bootstrapmethods attribute to compute the offset of each bootstrap method. in the constant pool of the class.
parses a runtime[in]visibletypeannotations attribute to find the offset of each type_annotation entry it contains, to find the corresponding labels, and to visit the try catch block annotations. attribute, excluding the attribute_info's attribute_name_index and attribute_length fields. false it is a runtimeinvisibletypeannotations attribute. 'annotations' array field.
reads a jvms 'code' attribute and makes the given visitor visit it. attribute_name_index and attribute_length fields.
reads a jvms 'verification_type_info' structure and stores it at the given index in the given array. read. parsed type is an item_uninitialized, a new label for the corresponding new instruction is stored in this array if it does not already exist.
computes the implicit frame of the method currently being parsed (as defined in the given
reads a jvms 'element_value' structure and makes the given visitor visit it. read.
constructs a new classreader object. this internal constructor must not be exposed as a public api.
reads the module, modulepackages and modulemainclass attributes and visit them. attribute_name_index and attribute_length fields). attribute_info's attribute_name_index and attribute_length fields), or 0.
reads a jvms method_info structure and makes the given visitor visit it.
makes the given visitor visit the jvms classfile structure passed to the constructor of this the class. any attribute whose type is not equal to the type of one the prototypes will not be parsed: its byte array value will be passed unchanged to the classwriter. this may corrupt it if this value contains references to the constant pool, or has syntactic or semantic links with a class element that has been transformed by a class adapter between the reader and the writer. #skip_code, #skip_debug, #skip_frames or #expand_frames.
returns the internal names of the implemented interfaces (see type#getinternalname()). interfaces are not returned.
reads a jvms 'stack_map_frame' structure and stores the result in the given context object. this method can also be used to read a full_frame structure, excluding its frame_type field (this is used to parse the legacy stackmap attributes). structure to be read, or the start offset of a full_frame structure (excluding its frame_type field). structure without its frame_type field.
reads the given input stream and returns its content as a byte array.
reads a runtime[in]visibleparameterannotations attribute and makes the given visitor visit it. runtime[in]visibleparameterannotations attribute, excluding the attribute_info's attribute_name_index and attribute_length fields. attribute, false it is a runtimeinvisibleparameterannotations attribute.
returns the label corresponding to the given bytecode offset. the default implementation of this method creates a label for the given offset if it has not been already created. for bytecodeoffset this method must not create a new one. otherwise it must store the new label in this array.
reads an utf8 string in #b. large. it is not automatically resized.
reads a non standard jvms 'attribute' structure in #b. the class. any attribute whose type is not equal to the type of one the prototypes will not be parsed: its byte array value will be passed unchanged to the classwriter. header bytes (attribute_name_index and attribute_length) are not taken into account here. -1 if the attribute to be read is not a code attribute. the 6 attribute header bytes (attribute_name_index and attribute_length) are not taken into account here. is not a code attribute.
parses the header of a jvms type_annotation structure to extract its target_type, target_info and target_path (the result is stored in the given context), and returns the start offset of the rest of the type_annotation structure. target_type and target_path must be stored.
sets this currentframe to the input stack map frame of the next "current" instruction, i.e. the instruction just after the given one. it is assumed that the value of this object when this method is called is the stack map frame status just before the given instruction is executed.
determine the order value for the given object. the default implementation checks against the given ordersourceprovider using #findorder and falls back to a regular #getorder(object) call.
sort the given list with a default ordercomparator. optimized to skip sorting for lists with size 0 or 1, in order to avoid unnecessary array extraction.
determine the conventional variable name for the given parameter taking the generic collection type, if any, into account. as of 5.0 this method supports reactive types:
determine the conventional variable name for the return type of the given method, taking the generic collection type, if any, into account, falling back on the given return value if the method declaration is not specific enough, e.g. object return type or untyped collection. as of 5.0 this method supports reactive types:
determine the conventional variable name for the supplied object based on its concrete type. the convention used is to return the un-capitalized short name of the class, according to javabeans property naming rules. for example: for arrays the pluralized version of the array component type is used. for collections an attempt is made to 'peek ahead' to determine the component type and return its pluralized version.
retrieve the class of an element in the collection. the exact element for which the class is retrieved will depend on the concrete collection implementation.
return an attribute name qualified by the given enclosing class. for example the attribute name ' foo' qualified by class ' com.myapp.someclass' would be ' com.myapp.someclass.foo'
convert strings in attribute name format (e.g. lowercase, hyphens separating words) into property name format (camel-case). for example
determine the class to use for naming a variable containing the given value. will return the class of the given value, except when encountering a jdk proxy, in which case it will determine the 'primary' interface implemented by that proxy.
resolve the given generic type against the given context class, substituting type variables as far as possible. in which the target type appears in a method signature (can be null)
resolve the single type argument of the given generic interface against the given target method which is assumed to return the given interface or an implementation of it. if not resolvable or if the single argument is of type wildcardtype.
determine the target type for the given generic parameter type.
resolve the single type argument of the given generic interface against the given target class which is assumed to implement the generic interface and possibly declare a concrete type for its type variable.
build a mapping of typevariable#getname typevariable names to searches all super types, enclosing types and interfaces.
resolve the type arguments of the given generic interface against the given target class which is assumed to implement the generic interface and possibly declare concrete types for its type variables. number of actual type arguments, or null if not resolvable
create a new methodparameter for the given method or constructor. this is a convenience factory method for scenarios where a method or constructor reference is treated in a generic fashion.
obtain the (lazily constructed) type-indexes-per-level map.
return the parameter descriptor for methodconstructor parameter.
create a new methodparameter for the given method or constructor. this is a convenience factory method for scenarios where a method or constructor reference is treated in a generic fashion.
return the generic type of the methodconstructor parameter.
return the name of the methodconstructor parameter. parameter name metadata is contained in the class file or no has been set to begin with)
return the parameter annotation of the given type, if available.
return the type of the methodconstructor parameter.
check whether the specified methodparameter represents a nullable kotlin type or an optional parameter (with a default value in the kotlin declaration).
cast the given type to a subtype of enum.
create the most approximate map for the given map. warning: since the parameterized type k is not bound to the type of keys contained in the supplied map, type safety cannot be guaranteed if the supplied map is an ensuring that the key type in the supplied map is an enum type matching type k. as an alternative, the caller may wish to treat the return value as a raw map or map keyed by object.
create the most appropriate collection for the given collection type. warning: since the parameterized type e is not bound to the supplied elementtype, type safety cannot be guaranteed if the desired collectiontype is enumset. in such scenarios, the caller is responsible for ensuring that the supplied elementtype is an enum type matching type e. as an alternative, the caller may wish to treat the return value as a raw collection or collection of object. (note: only relevant for enumset creation) the supplied elementtype is not a subtype of enum
create the most appropriate map for the given map type. warning: since the parameterized type k is not bound to the supplied keytype, type safety cannot be guaranteed if the desired maptype is enummap. in such scenarios, the caller is responsible for ensuring that the keytype is an enum type matching type k. as an alternative, the caller may wish to treat the return value as a raw map or map keyed by desired maptype is multivaluemap. (note: only relevant for enummap creation) the supplied keytype is not a subtype of enum
create the most approximate collection for the given collection. warning: since the parameterized type e is not bound to the type of elements contained in the supplied is responsible for ensuring that the element type for the supplied alternative, the caller may wish to treat the return value as a raw collection or collection of object.
create a variant of java.util.properties that automatically adapts non-string values to string representations on properties#getproperty.
check whether this exception contains an exception of the given type: either it is of the given class itself or it contains a nested cause of the given type.
return a string representation of this type in its fully resolved form (including any generic parameters).
return a resolvabletype for the specified methodparameter with a given implementation type. use this variant when the class that declares the method includes generic parameter variables that are satisfied by the implementation type.
return a resolvabletype for the specified field with a given implementation. use this variant when the class that declares the field includes generic parameter variables that are satisfied by the implementation class.
resolve the top-level parameter type of the given methodparameter.
return true if this type contains unresolvable generics only, that is, no substitute for any of its declared type variables.
return this type as a resolvabletype of the specified class. searches hierarchies to find a match, returning #none if this type does not implement or extend the specified class. type, or #none if not resolvable as that type
return a resolvabletype for the specified methodparameter, overriding the target type to resolve with a specific given type.
return a resolvabletype for the specified field with a given implementation. use this variant when the class that declares the field includes generic parameter variables that are satisfied by the implementation type.
return a resolvabletype for the specified method parameter with a given implementation. use this variant when the class that declares the method includes generic parameter variables that are satisfied by the implementation class.
return a resolvabletype representing the generic parameter for the given indexes. indexes are zero based; for example given the type for example getgeneric(1, 0) will access the string from the nested list. for convenience, if no indexes are specified the first generic is returned. if no generic is available at the specified indexes #none is returned. (may be omitted to return the first generic)
return a resolvabletype as a array of the specified componenttype.
convenience method that will #getgenerics() get and #resolve() resolve generic parameters, using the specified fallback if any type cannot be resolved.
return an array of resolvabletype resolvabletypes representing the generic parameters of this type. if no generics are available an empty array is returned. if you need to access a specific generic consider using the #getgeneric(int...) method as it allows access to nested generics and protects against (never null)
return a resolvabletype representing the direct supertype of this type. if no supertype is available this method returns #none. note: the resulting resolvabletype instance may not be serializable.
adapts this resolvabletype to a variableresolver.
return a resolvabletype for the specified method return type. use this variant when the class that declares the method includes generic parameter variables that are satisfied by the implementation class.
get a wildcardbounds instance for the specified type, returning
convenience method that will #getgenerics() get and will never be null, but it may contain null elements)
return a resolvabletype for the specified class with pre-declared generics.
return a resolvabletype for the specified field with a given implementation and the given nesting level. use this variant when the class that declares the field includes generic parameter variables that are satisfied by the implementation class. generic type; etc)
determine whether the underlying type has any unresolvable generics: either through an unresolvable type variable on the type itself or through implementing a generic interface in a raw fashion, i.e. without substituting that interface's type variables. the result will be true only in those two scenarios.
return a resolvabletype for the specified class, doing assignability checks against the raw class only (analogous to for example: resolvabletype.forrawclass(list.class). equivalent to object.class for typical use cases here
determine whether the underlying type represents a wildcard without specific bounds (i.e., equal to ? extends object).
return a resolvabletype for the specified constructor parameter with a given implementation. use this variant when the class that declares the constructor includes generic parameter variables that are satisfied by the implementation class.
return a resolvabletype array representing the direct interfaces implemented by this type. if this type does not implement any interfaces an empty array is returned. note: the resulting resolvabletype instances may not be serializable.
return a resolvabletype for the specified type backed by the given owner type. note: the resulting resolvabletype instance may not be serializable.
return a resolvabletype for the specified class with pre-declared generics.
return a resolvabletype for the specified type backed by a given
return a resolvabletype for the specified nesting level. the nesting level refers to the specific generic parameter that should be returned. a nesting level of 1 indicates this type; 2 indicates the first nested generic; 3 the second; and so on. for example, given list> level 1 refers to the list, level 2 the set, and level 3 the integer. the typeindexesperlevel map can be used to reference a specific generic for the given level. for example, an index of 0 would refer to a map key; whereas, 1 would refer to the value. if the map does not contain a value for a specific level the last generic will be used (e.g. a map value). nesting levels may also apply to array types; for example given if a type does not #hasgenerics() contain generics the current type, 2 for the first nested generic, 3 for the second and so on nesting level (may be null)
determine whether the underlying type is a type variable that cannot be resolved through the associated variable resolver.
build a message for the given base message and root cause.
retrieve the innermost cause of the given exception, if any.
return the full version string of the present spring codebase, or null if it cannot be determined.
inspects the target class. exceptions will be logged and a maker map returned to indicate the lack of debug information.
load the defining bytes for the given class, to be turned into a class object through a #defineclass call. the default implementation delegates to #openstreamforclass and #transformifnecessary. or null if no class defined for that name
open an inputstream for the specified class. the default implementation loads a standard class file through the parent classloader's getresourceasstream method.
determine whether the specified class is excluded from decoration by this class loader. the default implementation checks against excluded packages and classes.
returns true if the type signature of both the supplied are equal after resolving all types against the declaringtype, otherwise returns false.
compare the signatures of the bridge method and the method which it bridges. if the parameter and return types are the same, it is a 'visibility' bridge method introduced in java 6 to fix http:bugs.sun.comview_bug.do?bug_id=6342411. see also http:stas-blogspot.blogspot.com201003java-bridge-methods-explained.html
find the original method for the supplied method bridge method. it is safe to call this method passing in a non-bridge method instance. in such a case, the supplied method instance is returned directly to the caller. callers are not required to check for bridging before calling this method. if no more specific one could be found)
returns true if the supplied ' candidatemethod' can be consider a validate candidate for the method that is method#isbridge() bridged by the supplied method bridge method. this method performs inexpensive checks and can be used quickly filter for a set of possible matches.
if the supplied class has a declared method whose signature matches that of the supplied method, then this matching method is returned, otherwise null is returned.
searches for the generic method declaration whose erased signature matches that of the supplied bridge method.
searches for the bridged method in the given candidates.
select an invocable method on the target type: either the given method itself if actually exposed on the target type, or otherwise a corresponding method on one of the target type's interfaces or on the target type itself. matches on user-declared interfaces will be preferred since they are likely to contain relevant metadata that corresponds to the method on the target class. (typically an interface-based jdk proxy) target type (typically due to a proxy mismatch)
select methods on the given target type based on the lookup of associated metadata. callers define methods of interest through the metadatalookup parameter, allowing to collect the associated metadata into the result map. returning non-null metadata to be associated with a given method if there is a match, or null for no match or an empty map in case of no match
copy the attributes from the supplied attributeaccessor to this accessor.
obtain the closest match from the given exception types for the given target exception.
return a serializable type backed by a typeprovider . if type artifacts are generally not serializable in the current runtime environment, this delegate will simply return the original type as-is.
check whether this exception contains an exception of the given type: either it is of the given class itself or it contains a nested cause of the given type.
get the adapter for the given reactive type. or if a "source" object is provided, its actual type is used instead. (may be null if a concrete source object is given) (i.e. to adapt from; may be null if the reactive type is specified)
register a reactive type along with functions to adapt to and from a reactive streams publisher. the functions can assume their input is never be null nor optional.
return a shared default reactiveadapterregistry instance, lazily building it once needed. note: we highly recommend passing a long-lived, pre-configured this accessor is only meant as a fallback for code paths that want to fall back on a default instance if one isn't provided.
create a registry and auto-register default adapters.
look up the given value within the given group of constants. will return the first match.
parse the given string (upper or lower case accepted) and return the appropriate value if it's the name of a constant field in the class that we're analysing.
return all names of the given group of constants. note that this method assumes that constants are named in accordance with the standard java convention for constant values (i.e. all uppercase). the supplied namesuffix will be uppercased (in a locale-insensitive fashion) prior to the main logic of this method kicking in.
return a constant value cast to a number. or if the type wasn't compatible with number
return all values of the given group of constants. note that this method assumes that constants are named in accordance with the standard java convention for constant values (i.e. all uppercase). the supplied namesuffix will be uppercased (in a locale-insensitive fashion) prior to the main logic of this method kicking in.
convert the given bean property name to a constant name prefix. uses a common naming idiom: turning all lower case characters to upper case, and prepending upper case characters with an underscore. example: "imagesize" -> "image_size" example: "imagesize" -> "imagesize". example: "imagesize" -> "_image_size". example: "imagesize" -> "_i_m_a_g_e_s_i_z_e"
create a new constants converter class wrapping the given class. all public static final variables will be exposed, whatever their type.
look up the given value within the given group of constants. will return the first match.
return all names of the given group of constants. note that this method assumes that constants are named in accordance with the standard java convention for constant values (i.e. all uppercase). the supplied nameprefix will be uppercased (in a locale-insensitive fashion) prior to the main logic of this method kicking in.
return all values of the given group of constants. note that this method assumes that constants are named in accordance with the standard java convention for constant values (i.e. all uppercase). the supplied nameprefix will be uppercased (in a locale-insensitive fashion) prior to the main logic of this method kicking in.
retrieve the property value for the given key, checking local spring properties first and falling back to jvm-level system properties.
programmatically set a local property, overriding an entry in the
check whether the given name points back to the given alias as an alias in the other direction already, catching a circular reference upfront and throwing a corresponding illegalstateexception.
determine whether the given name has the given alias registered.
resolve all alias target names and aliases registered in this factory, applying the given stringvalueresolver to them. the value resolver may for example resolve placeholders in target bean names and even in alias names.
resolve the given path, replacing placeholders with corresponding property values from the environment if necessary.
this implementation throws illegalstateexception if attempting to read the underlying stream multiple times.
create a new filesystemresource from a file path. note: when building relative resources via #createrelative, it makes a difference whether the specified resource base path here ends with a slash or not. in the case of "c:dir1", relative paths will be built underneath that root: e.g. relative path "dir2" -> "c:dir1dir2". in the case of "c:dir1", relative paths will apply at the same directory level: relative path "dir2" -> "c:dir2".
this implementation opens a filechannel for the underlying file.
create a new filesystemresource from a file handle. note: when building relative resources via #createrelative, the relative path will apply at the same directory level: e.g. new file("c:dir1"), relative path "dir2" -> "c:dir2"! if you prefer to have relative paths built underneath the given root directory, use the #filesystemresource(string) constructor with a file path to append a trailing slash to the root path: "c:dir1", which indicates this directory as root for all relative paths.
this implementation returns the underlying filepath last-modified time.
create a new filesystemresource from a filesystem handle, locating the specified path. this is an alternative to #filesystemresource(string), performing all file system interactions via nio.2 instead of file.
this implementation returns the underlying filepath length.
this implementation opens a nio file stream for the underlying file.
this implementation creates a filesystemresource, applying the given path relative to the path of the underlying file of this resource descriptor.
resolve resource paths as file system paths. note: even if a given path starts with a slash, it will get interpreted as relative to the current vm working directory.
create a new classpathresource for classloader usage. a leading slash will be removed, as the classloader resource access methods will not accept it. or null for the thread context class loader
this implementation creates a classpathresource, applying the given path relative to the path of the underlying resource of this descriptor.
this implementation returns a url for the underlying class path resource, if available.
this implementation opens an inputstream for the given class path resource.
this implementation returns a description that includes the class path location.
this implementation returns a file reference for the given uri-identified resource, provided that it refers to a file in the file system.
this implementation returns a file reference for the given uri-identified resource, provided that it refers to a file in the file system.
this implementation returns a file reference for the underlying class path resource, provided that it refers to a file in the file system.
this implementation determines the underlying file (or jar file, in case of a resource in a jarzip).
this implementation builds a uri based on the url returned by #geturl().
this implementation checks the timestamp of the underlying file, if available.
this implementation reads the entire inputstream to calculate the content length. subclasses will almost always be able to provide a more optimal version of this, e.g. checking a file length.
this implementation opens an inputstream for the given url. it sets the usecaches flag to false, mainly to avoid jar file locking on windows.
create a new urlresource based on a uri specification. the given parts will automatically get encoded if necessary. also known as "scheme" also known as "scheme-specific part" as following after a "#" separator)
create a new urlresource based on the given uri object.
this implementation creates a urlresource, applying the given path relative to the path of the underlying url of this resource descriptor.
create a new urlresource based on a url path. note: the given path needs to be pre-encoded if necessary.
determine a cleaned url for the given original url.
this implementation opens a channel for the underlying file.
this implementation returns the underlying file reference.
this implementation opens a outputstream for the underlying file.
this implementation opens a inputstream for the underlying file.
find the most specific localized resource for the given name, extension and locale: the file will be searched with locations in the following order, similar to java.util.resourcebundle's search order:  [name]_[language]_[country]_[variant][extension] [name]_[language]_[country][extension] [name]_[language][extension] [name][extension]  if none of the specific files can be found, a resource descriptor for the default location will be returned.
treat the given text as a location pattern and convert it to a resource array.
resolve the given path, replacing placeholders with corresponding system property values if necessary.
treat the given value as a collection or array and convert it to a resource array. considers string elements as location patterns and takes resource elements as-is.
open a java.io.reader for the specified resource, using the specified (if any).
return whether the given resource location is a url: either a special "classpath" or "classpath" pseudo url or a standard url.
return a default resourcepatternresolver for the given resourceloader. this might be the resourceloader itself, if it implements the resourcepatternresolver extension, or a pathmatchingresourcepatternresolver built on the given resourceloader. (may be null to indicate a default resourceloader)
load and instantiate the factory implementations of the given type from @value #factories_resource_location, using the given class loader. the returned factories are sorted through annotationawareordercomparator. if a custom instantiation strategy is required, use #loadfactorynames to obtain all registered factory names. be loaded or if an error occurs while instantiating any factory
fill the given properties from the given resource (in iso-8859-1 encoding).
load all properties from the specified class path resource (in iso-8859-1 encoding), using the given class loader. merges properties if more than one resource of the same name found in the class path. (or null to use the default class loader)
fill the given properties from the given encodedresource, potentially defining a specific encoding for the properties file.
load properties from the given resource (in iso-8859-1 encoding).
actually load properties from the given encodedresource into the given properties instance.
load properties from the given encodedresource, potentially defining a specific encoding for the properties file.
return a potentially adapted variant of this resourcepropertysource, overriding the previously given name (if any) with the original resource name (equivalent to the name generated by the name-less constructor variants).
return a potentially adapted variant of this resourcepropertysource, overriding the previously given (or derived) name with the specified name.
return the description for the given resource; if the description is empty, return the class name of the resource plus its identity hash code.
return a merged properties instance containing both the loaded properties and properties set on this factorybean.
load properties into the given instance.
check whether the given file path has a duplicate but differently structured entry in the existing result, i.e. with or without a leading slash.
determine a sorted list of files in the given directory.
find all class location resources with the given location via the classloader. delegates to #dofindallclasspathresources(string).
find all resources in the file system that match the given location pattern via the ant-style pathmatcher.
resolve the given jar file url into a jarfile object.
find all class location resources with the given path via the classloader. called by #findallclasspathresources(string).
retrieve files that match the given path pattern, checking the given directory and its subdirectories. relative to the root directory
find all resources in jar files that match the given location pattern via the ant-style pathmatcher.
find all resources in the file system that match the given location pattern via the ant-style pathmatcher.
determine jar file references from the "java.class.path." manifest property and add them to the given set of resources in the form of pointers to the root of the jar file content.
search all urlclassloader urls for jar file references and add them to the given set of resources in the form of pointers to the root of the jar file content.
find all resources that match the given location pattern via the ant-style pathmatcher. supports resources in jar files and zip files and in the file system.
determine the root directory for the given location. used for determining the starting point for file matching, resolving the root directory location to a java.io.file and passing it into retrievematchingfiles, with the remainder of the location as pattern. will return "web-inf" for the pattern "web-inf.xml", for example.
recursively retrieve files that match the given pattern, adding them to the given result list. with prepended root directory path
write the given charsequence using the given charset, starting at the current writing position.
this implementation creates a single defaultdatabuffer to contain the data in databuffers.
relay buffers from the given publisher until the total the given maximum byte count, or until the publisher is complete.
read the given resource into a flux of databuffers starting at the given position. if the resource is a file, it is read into an fall back on #readbytechannel(callable, databufferfactory, int). closes the channel when the flux is terminated.
write the given stream of databuffer databuffers to the given writablebytechannel. does not close the channel when the flux is terminated, and does not #release(databuffer) release the data buffers in the source. if releasing is required, then subscribe to the returned flux with a note that the writing process does not start until the returned flux is subscribed to. process when subscribed to, and that publishes any writing errors and the completion signal
obtain a readablebytechannel from the given supplier, and read it into a
skip buffers from the given publisher until the total the given maximum byte count, or until the publisher is complete.
write the given stream of databuffer databuffers to the given asynchronousfilechannel. does not close the channel when the flux is terminated, and does not #release(databuffer) release the data buffers in the source. if releasing is required, then subscribe to the returned flux with a note that the writing process does not start until the returned flux is subscribed to. process when subscribed to, and that publishes any writing errors and the completion signal
obtain a asynchronousfilechannel from the given supplier, and read it into a channel when the flux is terminated.
release the given data buffer, if it is a pooleddatabuffer and has been pooleddatabuffer#isallocated() allocated.
this implementation uses netty's compositebytebuf.
add an option argument for the given option name and add the given value to the list of values associated with this option (of which there may be zero or more). the given value may be null, indicating that the option was specified without an associated value (e.g. "--foo" vs. "--foo=bar").
customize the set of property sources with those appropriate for any standard java environment:  @value #system_properties_property_source_name @value #system_environment_property_source_name  properties present in @value #system_properties_property_source_name will take precedence over those in @value #system_environment_property_source_name.
returns the value to which the specified key is mapped, or null if this map contains no mapping for the key.
parse the given string array based on the rules described simplecommandlineargsparser above, returning a fully-populated
assert that the named property source is present and return its index.
ensure that the given property source is not being added relative to itself.
this implementation first checks to see if the name specified is the special and if so delegates to the abstract #getnonoptionargs() method. if so and the collection of non-option arguments is empty, this method returns null. if not empty, it returns a comma-separated string of all non-option arguments. otherwise delegates to and returns the result of the abstract #getoptionvalues(string) method.
add the given propertysource to the start of the chain.
produce concise output (type and name) if the current log level does not include debug. if debug is enabled, produce verbose output including the hash code of the propertysource instance and every namevalue property pair. this variable verbosity is useful as a property source such as system properties or environment variables may contain an arbitrary number of property pairs, potentially leading to difficult to read exception and log messages.
validate the given profile, called internally prior to adding to the set of active or default profiles. subclasses may override to impose further restrictions on profile syntax. begins with the profile not operator (!).
return whether the given profile is active, or if active profiles are empty whether the profile should be active by default.
convert the given value to the specified target type, if necessary. is necessary
this implementation returns true if a property with the given name or any underscoreuppercase variant thereof exists in this property source.
check to see if this property source contains a property with the given name, or any underscore uppercase variation thereof. return the resolved name if one is found or otherwise the original name. never returns null.
format the given value via tostring(), quoting it if it is a set to true.
use this to log a message with different levels of detail (or different messages) at trace vs debug log levels. effectively, a substitute for:  class="code"> if (logger.isdebugenabled()) string str = logger.istraceenabled() ? "..." : "..."; if (logger.istraceenabled()) logger.trace(str); else logger.debug(str);  of log#istraceenabled()
create a composite logger that delegates to a primary or falls back on a secondary logger if logging for the primary logger is not enabled. this may be used for fallback logging from lower level packages that logically should log together with some higher level package but the two don't happen to share a suitable parent package (e.g. logging for the web and lower level http and codec packages). for such cases the primary, class-based logger can be wrapped with a shared fallback logger.
create a new type descriptor from the given type. use this to instruct the conversion system to convert an object to a specific target type, when no type location such as a method parameter or field is available to provide additional conversion context. generally prefer use of #forobject(object) for constructing type descriptors from source objects, as it handles the null object case.
if this type is an array, returns the array's component type. if this type is a stream, returns the stream's component type. if this type is a collection and it is parameterized, returns the collection's element type. if the collection is not parameterized, returns null indicating the element type is not declared. collection but its element type is not parameterized
returns true if an object of this type descriptor can be assigned to the location described by the given type descriptor. for example, valueof(string.class).isassignableto(valueof(charsequence.class)) returns true because a string value can be assigned to a charsequence variable. on the other hand, valueof(number.class).isassignableto(valueof(integer.class)) returns false because, while all integers are numbers, not all numbers are integers. for arrays, collections, and maps, element and keyvalue types are checked if declared. for example, a list<string> field value is assignable to a collection<charsequence> field, but list<number> is not assignable to list<integer>. type descriptor
create a new type descriptor from a methodparameter. use this constructor when a source or target conversion point is a constructor parameter, method parameter, or method return value.
create a type descriptor for a nested type declared within the method parameter. for example, if the methodparameter is a list and the nesting level is 1, the nested type descriptor will be string.class. if the methodparameter is a list> and the nesting level is 2, the nested type descriptor will also be a string.class. if the methodparameter is a map string> and the nesting level is 1, the nested type descriptor will be string, derived from the map value. if the methodparameter is a list string>> and the nesting level is 2, the nested type descriptor will be string, derived from the map value. returns null if a nested type cannot be obtained because it was not declared. for example, if the method parameter is a list, the nested type descriptor returned will be null. map keyvalue declaration within the method parameter or null if it could not be obtained specified nesting level are not of collection, array, or map types
create a new type descriptor from a java.util.collection type. useful for converting to typed collections. for example, a list could be converted to a the method call to construct such a typedescriptor would look something like: collection(list.class, typedescriptor.valueof(emailaddress.class)); used to convert collection elements
create a new type descriptor from a resolvabletype. this constructor is used internally and may also be used by subclasses that support non-java languages with extended type systems. it is public as of 5.1.4 whereas it was protected before.
create a new type descriptor from a java.util.map type. useful for converting to typed maps. for example, a map<string, string> could be converted to a map<id, emailaddress> by converting to a targettype built with this method: the method call to construct such a typedescriptor would look something like:  class="code"> map(map.class, typedescriptor.valueof(id.class), typedescriptor.valueof(emailaddress.class)); 
narrows this typedescriptor by setting its type to the class of the provided value. if the value is null, no narrowing is performed and this typedescriptor is returned unchanged. designed to be called by binding frameworks when they read property, field, or method return values. allows such frameworks to narrow a typedescriptor built from a declared property, field, or method return value type. for example, a field declared as java.lang.object would be narrowed to java.util.hashmap if it was set to a java.util.hashmap value. the narrowed typedescriptor can then be used to convert the hashmap to some other type. annotation and nested type context is preserved by the narrowed copy. class of the provided value)
create a new type descriptor from a field. use this constructor when a source or target conversion point is a field.
create a new type descriptor from a property. use this constructor when a source or target conversion point is a property on a java class.
cast this typedescriptor to a superclass or implemented interface preserving annotations and nested type context.
create a new type descriptor as an array of the specified type. for example to create a map[] use:  class="code"> typedescriptor.array(typedescriptor.map(map.class, typedescriptor.value(string.class), typedescriptor.value(string.class))); 
add common collection converters. (must also be castable to conversionservice, e.g. being a configurableconversionservice)
return a shared default conversionservice instance, lazily building it once needed. note: we highly recommend constructing individual this accessor is only meant as a fallback for code paths which need simple type coercion but cannot access a longer-lived
add converters appropriate for most environments. (must also be castable to conversionservice, e.g. being a configurableconversionservice)
returns an ordered class hierarchy for the given type.
find a genericconverter given a source and target type. this method will attempt to match all possible converters by working through the class and interface hierarchy of the types.
template method to convert a null source. the default implementation returns null or the java 8 custom null objects for specific target types.
hook method to lookup the converter for a given sourcetypetargettype pair. first queries this conversionservice's converter cache. on a cache miss, then performs an exhaustive search for a matching converter. if no converter matches, returns the default converter. or null if no suitable converter was found
create a new convertingpropertyeditoradapter for a given and the given target type.
register the given converter objects with the given target converterregistry.
create a new convertingcomparator instance.
read from the supplied inputstream and deserialize the contents into an object.
writes the source object to an output stream using java serialization. the source object must implement serializable.
serializes the source object and returns the byte array result.
create a new standardannotationmetadata wrapper for the given class, providing the option to return any nested annotations or annotation arrays in the form of org.springframework.core.annotation.annotationattributes instead of actual annotation instances. with asm-based annotationmetadata implementations
retrieve the merged attributes of the annotation of the given type, if any, from the supplied attributesmap. annotation attribute values appearing lower in the annotation hierarchy (i.e., closer to the declaring class) will override those defined higher in the annotation hierarchy. annotation type name keyed by annotation type name type to look for matching annotation is present in the attributesmap
specify the maximum number of entries for the metadatareader cache. default is 256 for a local cache, whereas a shared cache is typically unbounded. this method enforces a local resource cache, even if the resourceloader supports a shared resource cache.
find the given delimiter in the given data buffer.
join the given list of buffers into a single buffer.
split the given data buffer on delimiter boundaries. the returned flux contains an #end_frame buffer after each delimiter.
determine, if possible, the contentlength of the given resource without reading it.
merge two maps of hints, creating and copying into a new map if both have values, or returning the non-empty map, or an empty map if both are empty.
obtain the value for a required hint.
merge a single hint into a map of hints, possibly creating and copying all hints into a new map, or otherwise if the map of hints is empty, creating a new single entry map.
perform the search algorithm for the #searchwithgetsemantics method, avoiding endless recursion by tracking which annotated elements have already been visited. the metadepth parameter is explained in the type to find (as an alternative to annotationtype) annotations, or null if the annotation is not repeatable
search for annotations of the specified annotationname or find semantics. type to find (as an alternative to annotationtype) annotations, or null if the annotation is not repeatable
build an adapted annotatedelement for the given annotations, typically for use with other methods on annotatedelementutils.
get the first annotation of the specified annotationtype within the annotation hierarchy above the supplied element and merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy.  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within the annotation hierarchy. this method delegates to #getmergedannotationattributes(annotatedelement, string).
find all repeatable annotations of the specified annotationtype within the annotation hierarchy above the supplied element; and for each annotation found, merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy and synthesize the results back into an annotation of the specified  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within annotation hierarchies. this method follows find semantics as described in the may be null if the container type should be looked up via or an empty set if none were found is null, or if the container type cannot be resolved is not a valid container annotation for the supplied annotationtype
find all annotations of the specified annotationtype within the annotation hierarchy above the supplied element; and for each annotation found, merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy and synthesize the results back into an annotation of the specified  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within annotation hierarchies. this method follows find semantics as described in the or an empty set if none were found
find all annotations of the specified annotationtypes within the annotation hierarchy above the supplied element; and for each annotation found, merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy and synthesize the results back into an annotation of the corresponding annotationtype.  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within annotation hierarchies. this method follows find semantics as described in the or an empty set if none were found
find the first annotation of the specified annotationtype within the annotation hierarchy above the supplied element and merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy. attributes from lower levels in the annotation hierarchy override attributes of the same name from higher levels, and within a single annotation and within the annotation hierarchy. in contrast to #getallannotationattributes, the search algorithm used by this method will stop searching the annotation hierarchy once the first annotation of the specified annotationtype has been found. as a consequence, additional annotations of the specified this method follows find semantics as described in the strings or to preserve them as class references
determine if an annotation of the specified annotationtype is available on the supplied annotatedelement or within the annotation hierarchy above the specified element. if this method returns true, then #findmergedannotationattributes will return a non-null value. this method follows find semantics as described in the
find the first annotation of the specified annotationname within the annotation hierarchy above the supplied element and merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy. attributes from lower levels in the annotation hierarchy override attributes of the same name from higher levels, and within a single annotation and within the annotation hierarchy. in contrast to #getallannotationattributes, the search algorithm used by this method will stop searching the annotation hierarchy once the first annotation of the specified annotations of the specified annotationname will be ignored. this method follows find semantics as described in the preserve them as class references
find the first annotation of the specified annotationtype within the annotation hierarchy above the supplied element, merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy, and synthesize the result back into an annotation of the specified annotationtype.  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within the annotation hierarchy. this method follows find semantics as described in the
resolve the container type for the supplied repeatable annotationtype. delegates to annotationutils#resolvecontainerannotationtype(class).
post-process the aggregated results into a set of synthesized annotations.
get all repeatable annotations of the specified annotationtype within the annotation hierarchy above the supplied element; and for each annotation found, merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy and synthesize the results back into an annotation of the specified  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within annotation hierarchies. this method follows get semantics as described in the may be null if the container type should be looked up via or an empty set if none were found is null, or if the container type cannot be resolved is not a valid container annotation for the supplied annotationtype
get all annotations of the specified annotationtype within the annotation hierarchy above the supplied element; and for each annotation found, merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy and synthesize the results back into an annotation of the specified  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within annotation hierarchies. this method follows get semantics as described in the or an empty set if none were found
determine if an annotation of the specified annotationtype is present on the supplied annotatedelement or within the annotation hierarchy above the specified element. if this method returns true, then #getmergedannotationattributes will return a non-null value. this method follows get semantics as described in the
get all annotations of the specified annotationtypes within the annotation hierarchy above the supplied element; and for each annotation found, merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy and synthesize the results back into an annotation of the corresponding annotationtype.  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within annotation hierarchies. this method follows get semantics as described in the or an empty set if none were found
perform the search algorithm for the #searchwithfindsemantics method, avoiding endless recursion by tracking which annotated elements have already been visited. the metadepth parameter is explained in the type to find (as an alternative to annotationtype) annotations, or null if the annotation is not repeatable
get the first annotation of the specified annotationname within the annotation hierarchy above the supplied element and merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy. attributes from lower levels in the annotation hierarchy override attributes of the same name from higher levels, and aliasfor @aliasfor semantics are fully supported, both within a single annotation and within the annotation hierarchy. in contrast to #getallannotationattributes, the search algorithm used by this method will stop searching the annotation hierarchy once the first annotation of the specified annotationname has been found. as a consequence, additional annotations of the specified annotationname will be ignored. this method follows get semantics as described in the preserve them as class references into annotationattributes maps or to preserve them as annotation instances
get the first annotation of the specified annotationtype within the annotation hierarchy above the supplied element, merge that annotation's attributes with matching attributes from annotations in lower levels of the annotation hierarchy, and synthesize the result back into an annotation of the specified annotationtype.  aliasfor @aliasfor semantics are fully supported, both within a single annotation and within the annotation hierarchy. this method delegates to #getmergedannotationattributes(annotatedelement, class) and annotationutils#synthesizeannotation(map, class, annotatedelement).
validate that the supplied containertype is a proper container annotation for the supplied repeatable annotationtype (i.e. that it declares a value attribute that holds an array of the is not a valid container annotation for the supplied annotationtype
search for annotations of the specified annotationname or get semantics. type to find (as an alternative to annotationtype) annotations, or null if the annotation is not repeatable
get the annotation attributes of all annotations of the specified annotationname in the annotation hierarchy above the supplied annotatedelement and store the results in a note: in contrast to #getmergedannotationattributes(annotatedelement, string), this method does not support attribute overrides. this method follows get semantics as described in the preserve them as class references attributes from all annotations found, or null if not found
this method is invoked by #searchwithgetsemantics to perform the actual search within the supplied list of annotations. this method should be invoked first with locally declared annotations and then subsequently with inherited annotations, thereby allowing local annotations to take precedence over inherited annotations. the metadepth parameter is explained in the annotations, used for contextual logging; may be null if unknown type to find (as an alternative to annotationtype) annotations, or null if the annotation is not repeatable
this implementation checks for order @order or elements, in addition to the org.springframework.core.ordered check in the superclass.
sort the given list with a default annotationawareordercomparator. optimized to skip sorting for lists with size 0 or 1, in order to avoid unnecessary array extraction.
get the value stored under the specified attributename, ensuring that the value is of the expectedtype. if the expectedtype is an array and the value stored under the specified attributename is a single element of the component type of the expected array type, the single element will be wrapped in a single-element array of the appropriate type before returning it. never null or empty if it is not of the expected type
create a new, empty annotationattributes instance for the specified annotationtype.
return an annotationattributes instance based on the given map. if the map is already an annotationattributes instance, it will be cast and returned immediately without creating a new instance. otherwise a new instance will be created by passing the supplied map to the #annotationattributes(map) constructor.
enrich and validate the supplied attributes map by ensuring that it contains a non-null entry for each annotation attribute in the specified annotationtype and that the type of the entry matches the return type for the corresponding annotation attribute. if an entry is a map (presumably of annotation attributes), an attempt will be made to synthesize an annotation from it. similarly, if an entry is an array of maps, an attempt will be made to synthesize an array of annotations from those maps. if an attribute is missing in the supplied map, it will be set either to the value of its alias (if an alias exists) or to the value of the attribute's default value (if defined), and otherwise an illegalargumentexception will be thrown.
return the order on the specified type. takes care of order @order and @javax.annotation.priority.
return the value of the javax.annotation.priority annotation declared on the specified type, or null if none.
create a new synthesizingmethodparameter for the given method or constructor. this is a convenience factory method for scenarios where a method or constructor reference is treated in a generic fashion.
see annotation#hashcode() for a definition of the required algorithm.
see annotation#tostring() for guidelines on the recommended format.
handle the supplied annotation introspection exception. if the supplied exception is an annotationconfigurationexception, it will simply be thrown, allowing it to propagate to the caller, and nothing will be logged. otherwise, this method logs an introspection failure (in particular class values were not resolvable within annotation attributes and thereby effectively pretending there were no annotations on the specified element.
perform the search algorithm for #findannotation(annotatedelement, class) avoiding endless recursion by tracking which annotations have already been visited.
determine if this descriptor and the supplied descriptor both effectively represent aliases for the same attribute in the same target annotation, either explicitly or implicitly. this method searches the attribute override hierarchy, beginning with this descriptor, in order to detect implicit and transitively implicit aliases. effectively alias the same annotation attribute
create an aliasdescriptor from the declaration of @aliasfor on the supplied annotation attribute and validate the configuration of @aliasfor. is not annotated with @aliasfor
adapt the given value according to the given class and nested annotation settings. nested annotations will be logging; may be null if unknown compatibility with org.springframework.core.type.annotationmetadata) or to preserve them as class references
get the repeatable annotation annotations of such annotations are either present, indirectly present, or meta-present on the element. this method mimics the functionality of java 8's with additional support for meta-annotations. handles both single annotations and annotations nested within a container annotation. correctly handles bridge methods generated by the compiler if the supplied element is a method. meta-annotations will be searched if the annotation is not present on the supplied element. the annotations; may be null if a container is not supported or if it should be looked up via @ java.lang.annotation.repeatable when running on java 8 or higher
post-process the supplied annotationattributes. specifically, this method enforces attribute alias semantics for annotation attributes that are annotated with aliasfor @aliasfor and replaces default value placeholders with their original default values. annotation hierarchy from which the supplied attributes were created; may be null if unknown compatibility with org.springframework.core.type.annotationmetadata) or to preserve them as class references
get a single annotation of annotationtype from the supplied meta-present on the annotatedelement. note that this method supports only a single level of meta-annotations. for support for arbitrary levels of meta-annotations, use
find a single annotation of annotationtype on the supplied interfaces) if the annotation is not directly present on the given method itself. correctly handles bridge method methods generated by the compiler. meta-annotations will be searched if the annotation is not directly present on the method. annotations on methods are not inherited by default, so we need to handle this explicitly.
get the name of the aliased attribute configured via the supplied or the original attribute if no aliased one specified (indicating that the reference goes to a same-named attribute on a meta-annotation). this method returns the value of either the attribute or value attribute of @aliasfor, ensuring that only one of the attributes has been declared while simultaneously ensuring that at least one of the attributes has been declared. the aliased attribute name
get a single annotation of annotationtype from the supplied annotation: either the given annotation itself or a direct meta-annotation thereof. note that this method supports only a single level of meta-annotations. for support for arbitrary levels of meta-annotations, use one of the
resolve the container type for the supplied repeatable annotationtype. automatically detects a container annotation declared via is not annotated with @repeatable, this method simply returns
retrieve the given annotation's attributes as an annotationattributes map. this method provides fully recursive annotation reading capabilities on par with the reflection-based org.springframework.core.type.standardannotationmetadata. note: this variant of getannotationattributes() is only intended for use within the framework. the following special rules apply:  default values will be replaced with default value placeholders. the resulting, merged annotation attributes should eventually be ensure that placeholders have been replaced by actual default values and in order to enforce @aliasfor semantics.  may be null if unknown compatibility with org.springframework.core.type.annotationmetadata) or to preserve them as class references and corresponding attribute values as values (never null)
get the name of the overridden attribute configured via (never null) overridden attribute is allowed to be declared found or not applicable for the specified meta-annotation type type is null or annotation
does the given method override the given candidate method?
check the declared attributes of the given annotation, in particular covering google app engine's late arrival of typenotpresentexceptionproxy for this method not failing indicates that #getannotationattributes(annotation) won't failure either (when attempted later on).
retrieve a potentially cached array of declared annotations for the given element. (only for internal iteration purposes, not for external exposure)
get all methods declared in the supplied annotationtype that match java's requirements for annotation attributes. all methods in the returned list will be (never null) type (never null, though potentially empty)
get the declared repeatable annotation annotations of annotationtype from the supplied annotatedelement, where such annotations are either directly present, indirectly present, or meta-present on the element. this method mimics the functionality of java 8's with additional support for meta-annotations. handles both single annotations and annotations nested within a container annotation. correctly handles bridge methods generated by the compiler if the supplied element is a method. meta-annotations will be searched if the annotation is not present on the supplied element. the annotations; may be null if a container is not supported or if it should be looked up via @ java.lang.annotation.repeatable when running on java 8 or higher
perform the actual work for #findannotation(annotatedelement, class), honoring the synthesize flag.
determine if an annotation of type metaannotationtype is meta-present on the supplied annotationtype.
determine whether an annotation of the specified annotationtype is declared locally (i.e. directly present) on the supplied the supplied class may represent any type. meta-annotations will not be searched. note: this method does not determine if the annotation is java.lang.annotation.inherited inherited. for greater clarity regarding inherited annotations, consider using is directly present
synthesize an annotation from the supplied map of annotation attributes by wrapping the map in a dynamic proxy that implements an annotation of the specified annotationtype and transparently enforces attribute alias semantics for annotation attributes that are annotated with aliasfor @aliasfor. the supplied map must contain a key-value pair for every attribute defined in the supplied annotationtype that is not aliased or does not have a default value. nested maps and nested arrays of maps will be recursively synthesized into nested annotations or nested arrays of annotations, respectively. note that annotationattributes is a specialized type of corresponding to the supplied attributes; may be null if unknown attribute is not of the correct type
determine if the given annotated element is defined in a package or in the org.springframework.lang package
register the annotation-declared default values for the given attributes, if available.
determine whether the specified method has searchable annotations, i.e. not just java.lang or org.springframework.lang annotations such as deprecated and nullable. @ 5.0.5
clear the internal annotation metadata cache.
determine if annotations of the supplied annotationtype are synthesizable (i.e. in need of being wrapped in a dynamic proxy that provides functionality above that of a standard jdk annotation). specifically, an annotation is synthesizable if it declares any attributes that are configured as aliased pairs via annotation declare such aliased pairs.
determine the methods on the given type with searchable annotations on them.
get a map of all attribute aliases declared via @aliasfor in the supplied annotation type. the map is keyed by attribute name with each value representing a list of names of aliased attributes. for explicit alias pairs such as x and y (i.e. where x is an @aliasfor("y") and y is an @aliasfor("x"), there will be two entries in the map: x -> (y) and y -> (x). for implicit aliases (i.e. attributes that are declared as attribute overrides for the same attribute in the same meta-annotation), there will be n entries in the map. for example, if x, y, and z are implicit aliases, the map will contain the following entries: an empty return value implies that the annotation does not declare any attribute aliases.
find a single annotation of annotationtype on the supplied annotatedelement. meta-annotations will be searched if the annotation is not directly present on the supplied element. warning: this method operates generically on annotated elements. in other words, this method does not execute specialized search algorithms for classes or methods. if you require the more specific semantics of #findannotation(class, class) or #findannotation(method, class), invoke one of those methods instead.
retrieve the value of a named attribute, given an annotation instance. value cannot be retrieved due to an annotationconfigurationexception, in which case such an exception will be rethrown
retrieve the default value of a named attribute, given the
get the annotation with the supplied annotationname on the supplied element. type to find
perform the search algorithm for #findannotation(class, class), avoiding endless recursion by tracking which annotations have already been visited.
executes the given task synchronously, through direct invocation of it's runnable#run() run() method.
executes the given task, within a concurrency throttle if configured (through the superclass's settings). executes urgent tasks (with 'immediate' timeout) directly, bypassing the concurrency throttle (if active). all other tasks are subject to throttling.
delegates to the specified jdk concurrent executor.
actually execute the given runnable (which may be a user-supplied task or a wrapper around a user-supplied task) with the given executor.
copy the contents of the given inputstream to the given outputstream. leaves both streams open when done.
copy the contents of the given string to the given output outputstream. leaves the stream open when done.
copy a range of content of the given inputstream to the given outputstream. if the specified range exceeds the length of the inputstream, this copies up to the end of the stream and returns the actual number of copied bytes. leaves both streams open when done.
drain the remaining content of the given inputstream. leaves the inputstream open when done.
copy the contents of the given inputstream into a new byte array. leaves the stream open when done.
copy the contents of the given byte array to the given outputstream. leaves the stream open when done.
copy the contents of the given inputstream into a string. leaves the stream open when done.
copy the contents of the given byte array to the given output file.
copy the contents of the given inputstream into a new byte array. closes the stream when done.
copy the contents of the given byte array to the given outputstream. closes the stream when done.
copy the contents of the given reader to the given writer. closes both when done.
copy the contents of the given reader into a string. closes the reader when done.
copy the contents of the given string to the given output writer. closes the writer when done.
copy the contents of the given inputstream to the given outputstream. closes both streams when done.
create a deep copy of this map. (consistently using an independent modifiable linkedlist for each entry) along the lines of multivaluemap.addall semantics
return whether the given resource location is a url: either a special "classpath" pseudo url or a standard url.
extract the url for the actual jar file from the given url (which may point to a resource in a jar file or to a jar file itself).
extract the url for the outermost archive from the given jarwar url (which may point to a resource in a jar file or to a jar file itself). in the case of a jar file nested within a war file, this will return a url to the war file since that is the one resolvable in the file system.
resolve the given resource url to a java.io.file, i.e. to a file in the file system. the url was created for (for example, a class path location) a file in the file system
determine whether the given url points to a resource in a jar file. i.e. has protocol "jar", "war, ""zip", "vfszip" or "wsjar".
resolve the given resource location to a java.net.url. does not check whether the url actually exists; simply returns the url that the given location would correspond to. "classpath:" pseudo url, a "file:" url, or a plain file path
resolve the given resource uri to a java.io.file, i.e. to a file in the file system. the uri was created for (for example, a class path location) a file in the file system
determine whether the given url points to a resource in the file system, i.e. has protocol "file", "vfsfile" or "vfs".
resolve the given resource location to a java.io.file, i.e. to a file in the file system. does not check whether the file actually exists; simply returns the file that the given location would correspond to. "classpath:" pseudo url, a "file:" url, or a plain file path a file in the file system
assert a boolean expression, throwing an illegalstateexception if the expression evaluates to false. call #istrue if you wish to throw an illegalargumentexception on an assertion failure.  class="code">assert.state(id == null, "the id property must not already be initialized");
assert that an object is not null.  class="code"> assert.notnull(clazz, () -> "the class '" + clazz.getname() + "' must not be null");  assertion fails
assert that an array contains elements; that is, it must not be  class="code"> assert.notempty(array, () -> "the " + arraytype + " array must contain elements");  assertion fails
assert that supertype.isassignablefrom(subtype) is true.  class="code">assert.isassignable(number.class, myclass, "number expected"); if it is empty or ends in ":" or ";" or "," or ".", a full exception message will be appended. if it ends in a space, the name of the offending sub type will be appended. in any other case, a ":" with a space and the name of the offending sub type will be appended.
assert that an object is null.  class="code"> assert.isnull(value, () -> "the value '" + value + "' must be null");  assertion fails
assert that a collection contains elements; that is, it must not be  class="code">assert.notempty(collection, "collection must contain elements"); contains no elements
assert that the given string is not empty; that is, it must not be null and not the empty string.  class="code">assert.haslength(name, "name must not be empty");
assert that the given string contains valid text content; that is, it must not be null and must contain at least one non-whitespace character.  class="code">assert.hastext(name, "'name' must not be empty");
assert that an object is not null.  class="code">assert.notnull(clazz, "the class must not be null");
assert that a map contains entries; that is, it must not be null and must contain at least one entry.  class="code">assert.notempty(map, "map must contain entries");
assert that the given text does not contain the given substring.  class="code">assert.doesnotcontain(name, "rod", "name must not contain 'rod'");
assert that an array contains no null elements. note: does not complain if the array is empty!  class="code">assert.nonullelements(array, "the array must contain non-null elements");
assert that an object is null.  class="code">assert.isnull(value, "the value must be null");
assert that the provided object is an instance of the provided class.  class="code">assert.instanceof(foo.class, foo, "foo expected"); if it is empty or ends in ":" or ";" or "," or ".", a full exception message will be appended. if it ends in a space, the name of the offending object's type will be appended. in any other case, a ":" with a space and the name of the offending object's type will be appended.
assert that the given string contains valid text content; that is, it must not be null and must contain at least one non-whitespace character.  class="code"> assert.hastext(name, () -> "name for account '" + account.getid() + "' must not be empty");  assertion fails
assert that the given text does not contain the given substring.  class="code"> assert.doesnotcontain(name, forbidden, () -> "name must not contain '" + forbidden + "'");  assertion fails
assert that an array contains elements; that is, it must not be  class="code">assert.notempty(array, "the array must contain elements");
assert that a map contains entries; that is, it must not be null and must contain at least one entry.  class="code"> assert.notempty(map, () -> "the " + maptype + " map must contain entries");  assertion fails
assert that supertype.isassignablefrom(subtype) is true.  class="code"> assert.isassignable(number.class, myclass, () -> "processing " + myattributename + ":");  assertion fails. see #isassignable(class, class, string) for details.
assert a boolean expression, throwing an illegalstateexception if the expression evaluates to false. call #istrue if you wish to throw an illegalargumentexception on an assertion failure.  class="code"> assert.state(id == null, () -> "id for " + entity.getname() + " must not already be initialized");  assertion fails
assert a boolean expression, throwing an illegalargumentexception if the expression evaluates to false.  class="code">assert.istrue(i > 0, "the value must be greater than zero");
assert a boolean expression, throwing an illegalargumentexception if the expression evaluates to false.  class="code"> assert.istrue(i > 0, () -> "the value '" + i + "' must be greater than zero");  assertion fails
assert that an array contains no null elements. note: does not complain if the array is empty!  class="code"> assert.nonullelements(array, () -> "the " + arraytype + " array must contain non-null elements");  assertion fails
assert that the provided object is an instance of the provided class.  class="code"> assert.instanceof(foo.class, foo, () -> "processing " + foo.class.getsimplename() + ":");  assertion fails. see #isinstanceof(class, object, string) for details.
assert that a collection contains elements; that is, it must not be  class="code"> assert.notempty(collection, () -> "the " + collectiontype + " collection must contain elements");  assertion fails contains no elements
assert that the given string is not empty; that is, it must not be null and not the empty string.  class="code"> assert.haslength(name, () -> "name for account '" + account.getid() + "' must not be empty");  assertion fails
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
unwrap the given object which is potentially a java.util.optional. if the optional is empty, or simply the given object as-is
return a string representation of an object's overall identity. or an empty string if the object was null
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
return as hash code for the given object; typically the value of this method will delegate to any of the nullsafehashcode methods for arrays in this class. if the object is null, this method returns 0.
convert the given array (which may be a primitive array) to an object array (if necessary of primitive wrapper objects). a null source value will be converted to an empty object array.
determine whether the given object is empty. this method supports the following object types.   optional: considered empty if optional#empty()  array: considered empty if its length is zero  charsequence: considered empty if its length is zero  collection: delegates to collection#isempty()  map: delegates to map#isempty()  if the given object is non-null and not one of the aforementioned supported types, this method returns false.
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
append the given object to the given array, returning a new array consisting of the input array contents plus the given object.
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
check whether the given array of enum constants contains a constant with the given name.
return a string representation of the specified object. builds a string representation of the contents in case of an array. returns "null" if obj is null.
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
check whether the given exception is compatible with the specified exception types, as declared in a throws clause.
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
determine if the given objects are equal, returning true if both are null or false if only one is null. compares arrays with arrays.equals, performing an equality check based on the array elements rather than the array reference.
case insensitive alternative to enum#valueof(class, string). of enum values. use #containsconstant(enum[], string) as a guard to avoid this exception.
return a string representation of the contents of the specified array. the string representation consists of a list of the array's elements, enclosed in curly braces ( ""). adjacent elements are separated by the characters ", " (a comma followed by a space). returns
retrieve the last element of the given set, using sortedset#last() or otherwise iterating over all elements (assuming a linked set).
find the common element type of the given collection, if any. common type has been found (or the collection was empty)
check whether the given enumeration contains the given element.
marshal the elements from the given enumeration into an array of the given type. enumeration elements must be assignable to the type of the given array. the array returned will be a different instance than the array given.
find a single value of the given type in the given collection. or null if none or more than one such value found
merge the given properties instance into the given map, copying all properties (key-value pairs) over. uses properties.propertynames() to even catch default properties linked into the original properties instance.
retrieve the last element of the given list, accessing the highest index.
check whether the given iterator contains the given element.
merge the given array into the given collection.
return an unmodifiable view of the specified multi-value map.
return true if any element in ' candidates' is contained in ' source'; otherwise returns false.
return the first element in ' candidates' that is contained in ' source'. if no element in ' candidates' is present in ' source' returns null. iteration order is
creates a new propertyplaceholderhelper that uses the supplied prefix and suffix. and the associated default value, if any be ignored ( true) or cause an exception ( false)
create a new linkedcaseinsensitivemap that wraps a linkedhashmap with the given initial capacity and stores case-insensitive keys according to the given locale (by default in lower case).
resize the internal buffer size to a specified capacity. the actual size of the content stored in the buffer already
check if the right-hand side type may be assigned to the left-hand side type following the java generics rules.
unlike collection#contains(object) which relies on subtype, but otherwise ignores parameters.
indicate whether this mime type is compatible with the given mime type. for instance, text is compatible with textplain,
determine if the parameters in this mimetype and the supplied for charset charsets.
indicate whether this mime type includes the given mime type. for instance, text includes textplain and texthtml, and application+xml includes applicationsoap+xml, etc. this method is not symmetric.
checks the given token string for illegal characters, as defined in rfc 2616, section 2.2.
create a new mimetype for the given type, subtype, and parameters.
compares this mime type to another alphabetically.
base64-decode the given byte array from an utf-8 string.
base64-encode the given byte array to a string.
match a string against the given pattern, supporting the following simple pattern styles: "xxx", "xxx", "xxx" and "xxxyyy" matches (with an arbitrary number of pattern parts), as well as direct equality.
turn given source string array into sorted array.
delete any character in a given string. e.g. "az\n" will delete 'a's, 'z's and new lines.
replace all occurrences of a substring within a string with another string.
trim trailing whitespace from the given string.
convert a collection to a delimited string (e.g. csv). useful for tostring() implementations.
test whether the given string matches the given substring at the given index.
trim all occurrences of the supplied leading character from the given string.
take an array of strings and split each element based on the given delimiter. a properties instance is then generated, with the left of the delimiter providing the key, and the right of the delimiter providing the value. will trim both the key and value before adding them to the prior to attempting the split operation (typically the quotation mark symbol), or null if no removal should occur or null if the array to process was null or empty
take a string that is a delimited list and convert it into a string array. a single delimiter may consist of more than one character, but it will still be considered as a single delimiter string, rather than as bunch of potential delimiter characters, in contrast to rather than a bunch individual delimiter characters) line breaks: e.g. "\r\n\f" will delete all new lines and line feeds in a string
extract the filename extension from the given java resource path, e.g. "mypathmyfile.txt" -> "txt".
tokenize the given string into a string array via a the given delimiters string can consist of any number of delimiter characters. each of those characters can be used to separate tokens. a delimiter is always a single character; for multi-character delimiters, consider using #delimitedlisttostringarray. (each of the characters is individually considered as a delimiter) (only applies to tokens that are empty after trimming; stringtokenizer will not consider subsequent delimiters as token in the first place).
remove duplicate strings from the given array. as of 4.2, it preserves the original order, as it uses a linkedhashset.
trim the elements of the given string array, calling string.trim() on each of them.
normalize the path by suppressing sequences like "path.." and inner simple dots. the result is convenient for path comparison. for other uses, notice that windows separators ("\") are replaced by simple slashes.
concatenate the given string arrays into one, with overlapping array elements included twice. the order of elements in the original arrays is preserved.
merge the given string arrays into one, with overlapping array elements only included once. the order of elements in the original arrays is preserved (with the exception of overlapping elements, which are only included on their first occurrence). (with every entry included at most once, even entries within the first array)
parse the given string value into a locale, accepting the locale#tostring format as well as bcp 47 language tags. separators (as an alternative to underscores), or bcp 47 (e.g. "en-uk") as specified by locale#forlanguagetag on java 7+
trim leading and trailing whitespace from the given string.
trim leading whitespace from the given string.
append the given string to the given string array, returning a new array consisting of the input array contents plus the given string.
trim all whitespace from the given string: leading, trailing, and in between characters.
check whether the given charsequence contains any whitespace characters. contains at least 1 whitespace character
convert a string array into a delimited string (e.g. csv). useful for tostring() implementations.
extract the filename from the given java resource path, e.g. "mypathmyfile.txt" -> "myfile.txt".
apply the given relative path to the given java resource path, assuming standard java folder separation (i.e. "" separators). (relative to the full file path above)
strip the filename extension from the given java resource path, e.g. "mypathmyfile.txt" -> "mypathmyfile".
split a string at the first occurrence of the delimiter. does not include the delimiter in the result. index 1 being after the delimiter (neither element includes the delimiter); or null if the delimiter wasn't found in the given input string
trim all occurrences of the supplied trailing character from the given string.
parse the given timezonestring value into a timezone. but throwing illegalargumentexception in case of an invalid time zone specification
decode the given encoded uri component value. based on the following rules:  alphanumeric characters "a" through "z", "a" through "z", and "0" through "9" stay the same. special characters "-", "_", ".", and "" stay the same. a sequence " %xy" is interpreted as a hexadecimal representation of the character. 
count the occurrences of the substring sub in string str.
to be invoked after the main execution logic of concrete subclasses.
to be invoked before the main execution logic of concrete subclasses. this implementation applies the concurrency throttle.
tokenize the given path pattern into parts, based on this matcher's settings. performs caching based on #setcachepatterns, delegating to
actually match the given path against the given pattern. as far as the given base path goes is sufficient)
main entry point.
returns the length of the given pattern, where template variables are considered to be 1 long.
a convenient, alternative constructor to use with a custom path separator.
compare two patterns to determine which should match first, i.e. which is the most specific regarding the current path. more specific, equally specific, or less specific than pattern2.
build or retrieve an antpathstringmatcher for the given pattern. the default implementation checks this antpathmatcher's internal cache (see #setcachepatterns), creating a new antpathstringmatcher instance if no cached copy is found. when encountering too many patterns to cache at runtime (the threshold is 65536), it turns the default cache off, assuming that arbitrary permutations of patterns are coming in, with little chance for encountering a recurring pattern. this method may be overridden to implement a custom cache strategy.
given a pattern and a full path, determine the pattern-mapped part. for example:  ' docscvscommit.html' and ' docscvscommit.html -> '' ' docs' and ' docscvscommit -> ' cvscommit' ' docscvs.html' and ' docscvscommit.html -> ' commit.html' ' docs' and ' docscvscommit -> ' cvscommit' ' docs\.html' and ' docscvscommit.html -> ' cvscommit.html' ' .html' and ' docscvscommit.html -> ' docscvscommit.html' ' .html' and ' docscvscommit.html -> ' docscvscommit.html' ' ' and ' docscvscommit.html -> ' docscvscommit.html'  assumes that #match returns true for ' pattern' and ' path', but does not enforce this.
combine two patterns into a new pattern. this implementation simply concatenates the two patterns, unless the first pattern contains a file extension match (e.g., .html). in that case, the second pattern will be merged into the first. otherwise, an illegalargumentexception will be thrown. examples  border="1"> pattern 1pattern 2result  null null hotels nullhotels  nullhotelshotels hotelsbookingshotelsbookings hotelsbookingshotelsbookings hotelsbookingshotelsbookings hotels**bookingshotels**bookings hotelshotelhotelshotel hotelshotelhotelshotel hotels**hotelhotels**hotel .htmlhotels.htmlhotels.html .htmlhotelshotels.html .html.txt illegalargumentexception 
serialize the given object to a byte array.
deserialize the byte array into an object.
delete the supplied file - for directories, recursively delete any nested directories or files as well. or false it it did not exist
recursively copy the contents of the src filedirectory to the dest filedirectory.
perform the given callback operation on all matching methods of the given class, as locally declared or equivalent thereof (such as default methods on java 8 based interfaces that the given class implements).
invoke the given callback on all fields in the target class, going up the class hierarchy to get all declared fields.
get all declared methods on the leaf class and all superclasses. leaf class methods are included first.
rethrow the given throwable exception, which is presumably the target exception of an invocationtargetexception. should only be called if no checked exception is expected to be thrown by the target method. rethrows the underlying exception cast to an exception or
rethrow the given throwable exception, which is presumably the target exception of an invocationtargetexception. should only be called if no checked exception is expected to be thrown by the target method. rethrows the underlying exception cast to a runtimeexception or
determine whether the given method explicitly declares the given exception or one of its superclasses, which means that an exception of that type can be propagated as-is within a reflective invocation.
get the unique set of declared methods on the leaf class and all superclasses. leaf class methods are included first and while traversing the superclass hierarchy any methods found with signatures matching a method already included are filtered out.
set the field represented by the supplied field field object on the specified object target object to the specified value. in accordance with field#set(object, object) semantics, the new value is automatically unwrapped if the underlying field has a primitive type. thrown exceptions are handled via a call to #handlereflectionexception(exception).
attempt to find a field field on the supplied class with the supplied name andor class type. searches all superclasses up to object.
determine whether the given method is an "equals" method.
make the given method accessible, explicitly setting it accessible if necessary. the setaccessible(true) method is only called when actually necessary, to avoid unnecessary conflicts with a jvm securitymanager (if active).
invoke the given callback on all locally declared fields in the given class.
invoke the specified jdbc api method against the supplied target object with the supplied arguments.
invoke the specified method against the supplied target object with the supplied arguments. the target object can be null when invoking a static method. thrown exceptions are handled via a call to #handlereflectionexception.
determine whether the given method is a cglib 'renamed' method, following the pattern "cglib$methodname$0".
obtain an accessible constructor for the given class and parameters.
invoke the specified jdbc api method against the supplied target object with no arguments.
given the source object and the destination, which must be the same class or a subclass, copy all fields, including inherited fields. designed to work on objects with public no-arg constructors.
this variant retrieves class#getdeclaredfields() from a local cache in order to avoid the jvm's securitymanager check and defensive array copying.
get the field represented by the supplied field field object on the specified object target object. in accordance with field#get(object) semantics, the returned value is automatically wrapped if the underlying field has a primitive type. thrown exceptions are handled via a call to #handlereflectionexception(exception).
make the given constructor accessible, explicitly setting it accessible if necessary. the setaccessible(true) method is only called when actually necessary, to avoid unnecessary conflicts with a jvm securitymanager (if active).
make the given field accessible, explicitly setting it accessible if necessary. the setaccessible(true) method is only called when actually necessary, to avoid unnecessary conflicts with a jvm securitymanager (if active).
this variant retrieves class#getdeclaredmethods() from a local cache in order to avoid the jvm's securitymanager check and defensive array copying. in addition, it also includes java 8 default methods from locally implemented interfaces, since those are effectively to be treated just like declared methods.
handle the given reflection exception. should only be called if no checked exception is expected to be thrown by the target method. throws the underlying runtimeexception or error in case of an invocationtargetexception with such a root cause. throws an illegalstateexception with an appropriate message or undeclaredthrowableexception otherwise.
perform the given callback operation on all matching methods of the given class and superclasses (or given interface and super-interfaces). the same named method occurring on subclass and superclass will appear twice, unless excluded by the specified methodfilter.
attempt to find a method on the supplied class with the supplied name and parameter types. searches all superclasses up to object. returns null if no method can be found. (may be null to indicate any signature)
creates a newly allocated byte array. its size is the current size of this output stream and the valid contents of the buffer have been copied into it.
write the buffers content to the given outputstream.
resize the internal buffer size to a specified capacity. the actual size of the content stored in the buffer already
create a new buffer and store it in the linkedlist adds a new buffer that can store at least mincapacity bytes.
update the message digest with the next len bytes in this stream. avoids creating new byte arrays and use internal buffers for performance.
convert the stream's data to a byte array and return the byte array. also replaces the internal structures with the byte array to conserve memory: if the byte array is being made anyways, mind as well as use it. this approach also means that if this method is called twice without any writes in between, the second call is a no-op. this method is "unsafe" as it returns the internal buffer. callers should not modify the returned buffer.
return a string with a table describing all tasks performed. for custom reporting, call gettaskinfo() and use the task info directly.
return the name of the last task.
stop the current task. the results are undefined if timing methods are called without invoking at least one pair
start a named task. the results are undefined if #stop() or timing methods are called without invoking this method.
return the time taken by the last task.
return the last task as a taskinfo object.
return an informative string describing all tasks performed for custom reporting, call gettaskinfo() and use the task info directly.
return an array of the data for tasks performed.
find an available port for this sockettype, randomly selected from the range [ minport, maxport].
find the requested number of available ports for this sockettype, each randomly selected from the range [ minport, maxport].
sorts the given list of mimetype objects by specificity. given two mime types:  if either mime type has a mimetype#iswildcardtype() wildcard type, then the mime type without the wildcard is ordered before the other. if the two mime types have different mimetype#gettype() types, then they are considered equal and remain their current order. if either mime type has a mimetype#iswildcardsubtype() wildcard subtype , then the mime type without the wildcard is sorted before the other. if the two mime types have different mimetype#getsubtype() subtypes, then they are considered equal and remain their current order. if the two mime types have a different amount of parameters is ordered before the other.  for example: audiobasic < audio < / audiobasic;level=1 < audiobasic audiobasic == texthtml audiobasic == audiowave and content, section 5.3.2
parse the given string into a single mimetype.
tokenize the given comma-separated string of mimetype objects into a list. unlike simple tokenization by ",", this method takes into account quoted parameters.
generate a random mime boundary as bytes, often used in multipart mime types.
return a string representation of the given list of mimetype objects.
lazily initialize the securerandom for #generatemultipartboundary().
register the given common classes with the classutils cache.
determine the name of the package of the given fully-qualified class name, e.g. "java.lang" for the java.lang.string class name. is defined in the default package
check whether the given class is loadable in the given classloader.
build a string that consists of the names of the classesinterfaces in the given collection. basically like abstractcollection.tostring(), but stripping the "class ""interface " prefix before every class name.
determine whether the given class has a public constructor with the given signature, and return it if available (else return null). essentially translates nosuchmethodexception to null.
does the given class or one of its superclasses at least have one or more methods with the supplied name (with any argument types)? includes non-public methods.
override the thread context classloader with the environment's bean classloader if necessary, i.e. if the bean classloader is not equivalent to the thread context classloader already.
return all interfaces that the given class implements as a set, including ones implemented by superclasses. if the class itself is an interface, it gets returned as sole interface. (may be null when accepting all declared interfaces)
determine the common ancestor of the given classes, if any. extending the other), or null if none found. if any of the given classes is null, the other class will be returned.
return the user-defined class for the given class: usually simply the given class, but the original class in case of a cglib-generated subclass.
return a public static method of a class.
determine whether the class identified by the supplied name is present and can be loaded. will return false if either the class or one of its dependencies is not present or cannot be loaded. (may be null which indicates the default class loader) superclasses and interfaces) there was a readability mismatch in the inheritance hierarchy of the class (typically a missing dependency declaration in a jigsaw module definition for a superclass or interface implemented by the class to be checked here)
resolve the given class name into a class instance. supports primitives (like "int") and array class names (like "string[]"). this is effectively equivalent to the forname method with the same arguments, with the only difference being the exceptions thrown in case of class loading failure. (may be null, which indicates the default class loader) (that is, the class could not be found or the class file could not be loaded) there was a readability mismatch in the inheritance hierarchy of the class (typically a missing dependency declaration in a jigsaw module definition for a superclass or interface implemented by the class to be loaded here)
determine the name of the class file, relative to the containing package: e.g. "string.class"
given a method, which may come from an interface, and a target class used in the current reflective invocation, find the corresponding target method if there is one. e.g. the method may be ifoo.bar() and the target class may be defaultfoo. in this case, the method may be note: in contrast to org.springframework.aop.support.aoputils#getmostspecificmethod, this method does not resolve java 5 bridge methods automatically. call org.springframework.core.bridgemethodresolver#findbridgedmethod if bridge method resolution is desirable (e.g. for obtaining metadata from the original method definition). note: since spring 3.1.1, if java security settings disallow reflective access (e.g. calls to class#getdeclaredmethods etc, this implementation will fall back to returning the originally provided method. (may be null or may not even implement the method)
return a path suitable for use with classloader.getresource (also suitable for use with class.getresource by prepending a slash ('') to the return value). built by taking the package of the specified class file, converting all dots ('.') to slashes (''), adding a trailing slash if necessary, and concatenating the specified resource name to this. as such, this function may be used to build a path suitable for loading a resource file that is in the same package as a class file, although org.springframework.core.io.classpathresource is usually even more convenient.
return the short string name of a java class in uncapitalized javabeans property format. strips the outer class name in case of an inner class.
check whether the given class is cache-safe in the given context, i.e. whether it is loaded by the given classloader or a parent of it. (may be null which indicates the system class loader)
get the class name without the qualified package name.
resolve the given class name as primitive class, if appropriate, according to the jvm's naming rules for primitive classes. also supports the jvm's internal class names for primitive arrays. does not support the "[]" suffix notation for primitive arrays; this is only supported by #forname(string, classloader). a primitive class or primitive array class
determine a corresponding interface method for the given method handle, if possible. this is particularly useful for arriving at a public exported type on jigsaw which can be reflectively invoked without an illegal access warning.
return a descriptive name for the given object's type: usually simply the class name, but component type class name + "[]" for arrays, and an appended list of implemented interfaces for jdk proxies.
determine whether the given class has a public method with the given signature, and return it if available (else return null). in case of any signature specified, only returns the method if there is a unique candidate, i.e. a single public method with the specified name. essentially translates nosuchmethodexception to null. (may be null to indicate any signature)
return the number of methods with a given name (with any argument types), for the given class andor its superclasses. includes non-public methods.
check if the right-hand side type may be assigned to the left-hand side type, assuming setting by reflection. considers primitive wrapper classes as assignable to the corresponding primitive types.
replacement for class.forname() that also returns class instances for primitives (e.g. "int") and array class names (e.g. "string[]"). furthermore, it is also capable of resolving inner class names in java source style (e.g. "java.lang.thread.state" instead of "java.lang.thread$state"). (may be null, which indicates the default class loader)
determine whether the given class has a public method with the given signature, and return it if available (else throws an illegalstateexception). in case of any signature specified, only returns the method if there is a unique candidate, i.e. a single public method with the specified name. essentially translates nosuchmethodexception to illegalstateexception. (may be null to indicate any signature)
check whether the given class is visible in the given classloader. (may be null in which case this method will always return true)
given an input class object, return a string which consists of the class's package name as a pathname, i.e., all dots ('.') are replaced by slashes (''). neither a leading nor trailing slash is added. the result could be concatenated with a slash and the name of a resource and fed directly to classloader.getresource(). for it to be fed to to be prepended to the returned value. (empty) package will result in an empty string ("") being returned.
add given iterator to this composite.
update the message digest with the rest of the bytes in this stream. using this method is more optimized since it avoids creating new byte arrays for each call.
update the message digest with the next len bytes in this stream. using this method is more optimized since it avoids creating new byte arrays for each call.
check for a biginteger bigdecimal long overflow before returning the given number as a long value.
decode a java.math.biginteger from the supplied string value. supports decimal, hex, and octal notation.
convert the given number into an instance of the given target class. (i.e. not a standard number subclass as included in the jdk)
parse the given text into a number instance of the given target class, using the corresponding decode valueof method. trims all whitespace (leading, trailing, and in between characters) from the input string before attempting to parse the number. supports numbers in hex format (with leading "0x", "0x", or "#") as well. (i.e. not a standard number subclass as included in the jdk)
parse the given text into a number instance of the given target class, using the supplied numberformat. trims the input string before attempting to parse the number. (i.e. not a standard number subclass as included in the jdk)
template method for the creation of a new thread. the default implementation creates a new thread for the given
algorithm that judges the match between the declared parameter types of a candidate method and a specific list of arguments that this method is supposed to be invoked with. determines a weight that represents the class hierarchy difference between types and arguments. a direct match, i.e. type integer -> arg of class integer, does not increase the result - all direct matches means weight 0. a match between type object and arg of class integer would increase the weight by 2, due to the superclass 2 steps up in the hierarchy (i.e. object) being the last one that still matches the required type object. type number and class integer would increase the weight by 1 accordingly, due to the superclass 1 step up the hierarchy (i.e. number) still matching the required type number. therefore, with an arg of type integer, a constructor (integer) would be preferred to a constructor (number) which would in turn be preferred to a constructor (object). all argument weights get accumulated. note: this is the algorithm used by methodinvoker itself and also the algorithm used for constructor and factory method selection in spring's bean container (in case of lenient constructor resolution which is the default for regular bean definitions).
invoke the specified method. the invoker needs to have been prepared before. or null if the method has a void return type
prepare the specified method. the method can be invoked any number of times afterwards.
return the prepared method object that will be invoked. can for example be used to determine the return type.
set the target object on which to call the target method. only necessary when the target method is not static; else, a target class is sufficient.
find a matching method with the specified name for the specified arguments.
create a new messagedigest with the given algorithm. necessary because messagedigest is not thread-safe.
remove any entries that have been garbage collected and are no longer referenced. under normal circumstances garbage collected entries are automatically purged as items are added or removed from the map. this method can be used to force a purge, and is useful when the map is read frequently but updated less often.
apply an update operation to this segment. the segment will be locked during the update.
restructure the underlying data structure when it becomes necessary. this method can increase the size of the references table as well as purge any references that have been garbage collected.
create a new concurrentreferencehashmap instance. table exceeds this value, resize will be attempted. write to the map
get the hash for a given object, apply an additional hash function to reduce collisions. this implementation uses the same wangjenkins algorithm as
factory method used to create a new reference.
return the dataunit matching the specified suffix. of this enum's constants
obtain a datasize from a text string such as 12mb using the specified default dataunit if no unit is specified.  the string starts with a number followed optionally by a unit matching one of the supported dataunit suffixes.  examples:  "12kb" -- parses as "12 kilobytes" "5mb" -- parses as "5 megabytes" "20" -- parses as "20 kilobytes" (where the defaultunit is dataunit#kilobytes) 
add a comparator to the end of the chain. the comparator will default to ascending sort order, unless it is a invertiblecomparator.
replace the comparator at the given index. the comparator will default to ascending sort order, unless it is a invertiblecomparator.
invert the sort order of each sort definition contained by this compound comparator.
construct a compoundcomparator from the comparators in the provided array. all comparators will default to ascending sort order, unless they are invertiblecomparators.
return the xmleventwriter for the given stax result. or custom stax result
return the xmlstreamreader for the given stax source. or custom stax source
variant of #createdefensiveinputfactory() with a custom instance.
return the xmleventreader for the given stax source. or custom stax source
return the xmlstreamwriter for the given stax result. or custom stax result
check if the reader is closed, and throws a xmlstreamexception if so.
constructs a new instance of the staxeventxmlreader that reads from the given xmleventreader. the supplied event reader must be in
construct a new instance of the staxstreamxmlreader that reads from the given or xmlstreamconstants.start_element state.
convert a namespace uri and dom or sax qualified name to a qname. the qualified name can have the form prefix:localname or localname.
this implementation throws a saxnotrecognizedexception exception for any feature outside of the "http:xml.orgsaxfeatures" namespace and returns false for any feature within.
throws a saxnotrecognizedexception exception when the given property does not signify a lexical handler. the property name for a lexical handler is http:xml.orgsaxpropertieslexical-handler.
throws a saxnotrecognizedexception exception when the given property does not signify a lexical handler. the property name for a lexical handler is http:xml.orgsaxpropertieslexical-handler.
this implementation throws a saxnotrecognizedexception exception for any feature outside of the "http:xml.orgsaxfeatures" namespace and accepts a false value for any feature within.
create a new instance of the domcontenthandler with the given node.
start the prefix mapping for the given prefix.
convert a qname to a qualified name, as used by dom and sax. the returned string has a format of prefix:localname if the prefix is set, or just localname if not.
enable indenting for the supplied javax.xml.transform.transformer. if the underlying xslt engine is xalan, then the special output key indent-amount will be also be set to a value of #default_indent_amount characters.
consume the next comment token, update the "incomment" flag and return the remaining content.
try to consume the supplied token against the supplied content and update the in comment parse state to the supplied value. returns the index into the content which is after the token or -1 if the token is not found.
detect the validation mode for the xml document in the supplied inputstream. note that the supplied inputstream is closed by this method before returning.
does the supplied content contain an xml opening tag. if the parse state is currently in an xml comment then this method always returns false. it is expected that all comment tokens will have consumed for the supplied content before passing the remainder to this method.
consumes all the leading comment data in the given string and returns the remaining content, which may be empty since the supplied content might be all comment data. for our purposes it is only important to strip leading comment content on a line since the first piece of non comment content will be either the doctype declaration or the root element of the document.
remove the given prefix from this context.
bind the given prefix to the given namespace.
construct a new instance of the staxresult with the specified xmleventwriter.
construct a new instance of the staxresult with the specified xmlstreamwriter.
extracts the text value from the given dom element, ignoring xml comments. appends all characterdata nodes and entityreference nodes into a single string value, excluding comment nodes. only exposes actual user-specified text, no default values of any kind.
retrieves all child elements of the given dom element that match any of the given element names. only looks at the direct child level of the given element; do not go into further depth (in contrast to the dom api's getelementsbytagname method).
retrieves all child elements of the given dom element.
utility method that returns the first child element identified by its name.
expose this listenablefuture as a jdk completablefuture.
add the given failure callback to this registry.
add the given callback to this registry.
add the given success callback to this registry.
invoke the original (super) method on the specified object. argument to the methodinterceptor argument array as long as the types are compatible without wrapping in an invocationtargetexception
invoke the original method, on a different object of the same type. argument to the methodinterceptor (usually not what you want) argument array as long as the types are compatible without wrapping in an invocationtargetexception
return the methodproxy used when intercepting the method matching the given signature.
for internal use by enhancer only; see the org.springframework.cglib.reflect.fastmethod class for similar functionality.
helper method to create an intercepted object. for finer control over the generated instance, use a new instance of enhancer instead of this static method.
generate a new class if necessary and uses the specified callbacks (if any) to create a new object instance. uses the constructor of the superclass matching the argumenttypes parameter, with the given arguments.
filter the list of constructors from the superclass. the constructors which remain will be included in the generated class. the default implementation is to filter out all private constructors, but subclasses may extend enhancer to override this behavior.
helper method to create an intercepted object. for finer control over the generated instance, use a new instance of enhancer instead of this static method.
set the array of callback types to use. this may be used instead of #setcallbacks when calling an array of actual callback instances. you must use a callbackfilter to specify the index into this array for each method in the proxied class.
set the array of callbacks to use. ignored if you use #createclass. you must use a callbackfilter to specify the index into this array for each method in the proxied class.
helper method to create an intercepted object. for finer control over the generated instance, use a new instance of enhancer instead of this static method.
set the class which the generated class will extend. as a convenience, if the supplied superclass is actually an interface, setinterfaces will be called with the appropriate argument instead. a non-interface argument must not be declared as final, and must have an accessible constructor.
encode the given header field param as describe in rfc 5987. only the us-ascii, utf-8 and iso-8859-1 charsets are supported
decode the given header field param as describe in rfc 5987. only the us-ascii, utf-8 and iso-8859-1 charsets are supported.
return the header value for this content disposition as defined in rfc 2183.
parse a @literal content-disposition header value as defined in rfc 2183.
return the type of the request's body.
factory method to obtain a builder for a server-defined cookie that starts with a name-value pair and may also include attributes.
return the enum constant of this type with the corresponding series.
return the enum constant of this type with the specified numeric value.
return a replica of this instance with its quality value removed. or a new one otherwise
sorts the given list of mediatype objects by quality value. given two media types:  if the two media types have different #getqualityvalue() quality value, then the media type with the highest quality value is ordered before the other. if either media type has a #iswildcardtype() wildcard type, then the media type without the wildcard is ordered before the other. if the two media types have different #gettype() types, then they are considered equal and remain their current order. if either media type has a #iswildcardsubtype() wildcard subtype, then the media type without the wildcard is sorted before the other. if the two media types have different #getsubtype() subtypes, then they are considered equal and remain their current order. if the two media types have a different amount of #getparameter(string) parameters, then the media type with the most parameters is ordered before the other. 
sorts the given list of mediatype objects by specificity. given two media types:  if either media type has a #iswildcardtype() wildcard type, then the media type without the wildcard is ordered before the other. if the two media types have different #gettype() types, then they are considered equal and remain their current order. if either media type has a #iswildcardsubtype() wildcard subtype, then the media type without the wildcard is sorted before the other. if the two media types have different #getsubtype() subtypes, then they are considered equal and remain their current order. if the two media types have different #getqualityvalue() quality value, then the media type with the highest quality value is ordered before the other. if the two media types have a different amount of #getparameter(string) parameters, then the media type with the most parameters is ordered before the other.  for example: audiobasic < audio < / audio < audio;q=0.7; audio;q=0.3 audiobasic;level=1 < audiobasic audiobasic == texthtml audiobasic == audiowave and content, section 5.3.2
parse the given list of (potentially) comma-separated strings into a list of mediatype objects. this method can be used to parse an accept or content-type header.
parse the given string into a single mediatype.
return a replica of this instance with the quality value of the given mediatype. or a new one otherwise
sorts the given list of mediatype objects by specificity as the primary criteria and quality value the secondary.
re-create the given mime type as a media type.
parse the mime.types file found in the resources. format is:  # comments begin with a '#' # the format is <mime type> <space separated file extensions> # for example: textplain txt text # this would map file.txt and file.text to # the mime type "textplain" 
create a new httpentity with the given body and headers.
parse the given, comma-separated string into a list of httprange objects. this method can be used to parse an range header. the number of ranges is greater than 100.
turn a resource into a resourceregion using the range information contained in the current httprange.
convert each httprange into a resourceregion, selecting the appropriate segment of the given resource using http range information. resource length.
return a string representation of the given list of httprange objects. this method can be used to for an range header.
helps to format http header values, as http header values themselves can contain comma-separated values, can become confusing with regular
return all values of a given header name, even if this header is set multiple times.
turn the given list of header values into a comma-delimited result.
retrieve a combined result from the field values of the etag header.
parse the first header value for the given header name as a date, return null if there is no value or also in case of an invalid value (if rejectinvalid=false), or raise illegalargumentexception if the value cannot be parsed as a date. in that case ( false)
set the (new) value of the host header. if the given inetsocketaddress#getport() port is 0, the host header will only contain the
set the value of the #authorization authorization header to basic authentication based on the given username and password. sequence. defaults to standardcharsets#iso_8859_1 iso-8859-1. contains characters that cannot be encoded to the given charset
set the list of acceptable charset charsets, as specified by the accept-charset header.
set the acceptable language ranges, as specified by the @literal accept-language header.
return a httpheaders object that can be read and written to.
variant of #setacceptlanguage(list) using locale's.
return the value of the host header, if available. if the header value does not contain a port, the be 0.
return the set of allowed httpmethod http methods, as specified by the allow header. returns an empty set when the allowed methods are unspecified.
return a httpheaders object that can only be read, not written to.
set the content-disposition header when creating a applications typically would not set this header directly but rather prepare a multivaluemap object>, containing an object or a org.springframework.core.io.resource for each part, and then pass that to the resttemplate or webclient.
return the list of acceptable charset charsets, as specified by the accept-charset header.
return the value of the access-control-allow-methods response header.
a variant of #getacceptlanguage() that converts each
add a "max-age=" directive. this directive is well suited for publicly caching resources, knowing that they won't change within the configured amount of time. additional directives can be also used, in case resources shouldn't be cached ( #cacheprivate()) or transformed ( #notransform()) by shared caches. in order to prevent caches to reuse the cached response even when it has become stale (i.e. the "max-age" delay is passed), the "must-revalidate" directive should be set ( #mustrevalidate()
add a "no-store" directive. this directive is well suited for preventing caches (browsers and proxies) to cache the content of responses.
add a "no-cache" directive. this directive is well suited for telling caches that the response can be reused only if the client revalidates it with the server. this directive won't disable cache altogether and may result with clients sending conditional requests (with "etag", "if-modified-since" headers) and the server responding with "304 - not modified" status. in order to disable caching and minimize requestsresponses exchanges, the #nostore() directive should be used instead of #nocache().
return the "cache-control" header value.
use javax.servlet.servletrequest#getparametermap() to reconstruct the body of a form 'post' providing a predictable outcome as opposed to reading from the body, which can fail if any other code has used the servletrequest to access a parameter, thus causing the input stream to be "consumed".
construct a new instance of the servletserverhttpresponse based on the given httpservletresponse.
apply #beforecommit(supplier) beforecommit actions, apply the response status and headerscookies, and write the response body.
we cannot combine error_listener and handlerresultsubscriber due to: https:issues.jboss.orgbrowsewfly-8515.
completion signal from the upstream, write publisher. this is also used by sub-classes to delegate completion notifications from the container.
invoked when writing is possible, either in the same thread after a check via #iswritepossible(), or as a callback from the underlying container.
invoked during an error or completion callback from the underlying container to cancel the upstream subscription.
error signal from the upstream, write publisher. this is also used by sub-classes to delegate error notifications from the container.
template method invoked after a data item to write is received via data item for writing once that is possible.
completion signal from the upstream, write publisher. this is also used by sub-classes to delegate completion notifications from the container.
error signal from the upstream, write publisher. this is also used by sub-classes to delegate error notifications from the container.
invoked during an error or completion callback from the underlying container to cancel the upstream subscription.
invoke this to delegate an error signal to the subscriber.
invoke this to delegate a completion signal to the subscriber.
sub-classes can call this to delegate container error notifications.
read and publish data one at a time until there is no more data, no more demand, or perhaps we completed in the mean time. no more demand or we have completed.
a method for parsing of the query into name-value pairs. the return value is turned into an immutable map and cached. note that this method is invoked lazily on first access to parsing is thread-safe nevertheless.
read from the request body inputstream and return a databuffer. invoked only when servletinputstream#isready() returns "true". stream returned -1, or null if 0 bytes were read.
write the databuffer to the response body outputstream. invoked only when servletoutputstream#isready() returns "true" and the readable bytes in the databuffer is greater than 0.
template method for preparing the given httpurlconnection. the default implementation prepares the connection for input and output, and sets the http method.
opens and returns a connection to the given url. the default implementation uses the given #setproxy(java.net.proxy) proxy - if any - to open a connection.
setting the #settaskexecutor taskexecutor property is required before calling this method.
add the given headers to the given http connection.
return a multivaluemap with the configured parts.
variant of #part(string, object) that also accepts a mediatype which is used to determine how to encode the part.
variant of #asyncpart(string, publisher, class) that accepts a specifying generic type information.
add an asynchronous part with publisher-based content.
add the given headers to the given http request.
merge the given httpclient-level requestconfig with the factory-level requestconfig, if necessary.
create a commons httpmethodbase object for the given http method and uri specification.
create a new netty4clienthttprequestfactory with a default
template method for changing properties on the given socketchannelconfig. the default implementation sets the connect timeout based on the set property.
apply #beforecommit(supplier) beforecommit actions, apply the request headerscookies, and write the request body.
constructor with an jettyresourcefactory that will manage shared resources.
create a new clienthttprequest via this template's clienthttprequestfactory.
overridden to expose an interceptingclienthttprequestfactory if necessary.
create a new org.springframework.http.client.asyncclienthttprequest via this template's
get additional hints for encoding for example based on the server request or annotations from controller method parameters. by default, delegate to the encoder if it is an instance of httpmessageencoder.
get additional hints for decoding for example based on the server request or annotations from controller method parameters. by default, delegate to the decoder if it is an instance of httpmessagedecoder.
determine the content-type of the http message based on the "content-type" header or otherwise default to
determine the json encoding to use for the given mime type.
tokenize the given flux into flux. object is an array, each element is returned individually, immediately after it is received.
constructor with a jackson objectmapper to use.
constructor with a default list of part writers (string and resource).
create a new message.builder instance for the given class. this method uses a concurrenthashmap for caching method lookups.
parse message size as a varint from the input stream, updating messagebytestoread and inspired from codedinputstream#readrawvarint32(int, java.io.inputstream) truncated
create a new message.builder instance for the given class. this method uses a concurrenthashmap for caching method lookups.
returns the qualified name for the given class, according to the mapping rules in the jaxb specification.
return readers that need to be at the end, after all others.
return writers that support specific types. or for multipart requests only ("true"). generally the two sets are the same except for the multipart writer itself.
return object writers (json, xml, sse). or for multipart requests only ("true"). generally the two sets are the same except for the multipart writer itself.
return object readers (json, xml, sse).
return writers that need to be at the end, after all others.
return readers that support specific types.
internal method that returns the configured writers. or for multipart requests only ("true"). generally the two sets are the same except for the multipart writer itself.
return the list of supported charset charsets. by default, returns charset#availablecharsets(). can be overridden in subclasses.
sets the default content-type to be used for writing.
this implementation sets the default headers by calling #adddefaultheaders, and then calls #writeinternal.
this implementation sets the default headers by calling #adddefaultheaders, and then calls #writeinternal.
returns the default content type for the given type. called when #write is invoked without a specified content type parameter. by default, this returns the first element of the can be overridden in subclasses.
returns true if any of the #setsupportedmediatypes(list) supported media types mediatype#includes(mediatype) include the given media type. typically the value of a content-type header. or if the media type is null
returns true if the given media type includes any of the typically the value of an accept header. or if the media type is null
add default headers to the output message. this implementation delegates to #getdefaultcontenttype(object) if a content type was not provided, set if necessary the default character set, calls
a constructor accepting a conversionservice as well as a default charset.
return the filename of the given multipart part. this value will be used for the the default implementation returns resource#getfilename() if the part is a
apply the configured charset as a default to registered part converters.
configure custom deserializers. each deserializer is registered for the type returned by jsondeserializer#handledtype(), which must not be null.
configure custom serializers. each serializer is registered for the type returned by jsonserializer#handledtype(), which must not be null.
configure an existing objectmapper instance with this builder's settings. this can be applied to any number of objectmappers.
set a complete list of modules to be registered with the objectmapper. note: if this is set, no finding of modules is going to happen - not by jackson, and not by spring either (see #findmodulesviaserviceloader). as a consequence, specifying an empty list here will suppress any kind of module detection. specify either this or #modulestoinstall, not both.
build a new objectmapper instance. each build operation produces an independent objectmapper instance. the builder's settings can get modified, with a subsequent build operation then producing a new objectmapper based on the most recent settings.
determine the json encoding to use for the given content type.
determine whether to log the given exception coming from a (typically a jsonmappingexception)
obtain a gsonbuilder which base64-encodes byte[] properties when reading and writing json. a custom com.google.gson.typeadapter will be registered via serializes a byte[] property to and from a base64-encoded string instead of a json array.
construct a new protobufjsonformathttpmessageconverter with the given accepting an initializer that allows the registration of message extensions.
construct a new protobufjsonformathttpmessageconverter with the given accepting a registry that specifies protocol message extensions.
construct a new protobufhttpmessageconverter with an initializer that allows the registration of message extensions.
create a new message.builder instance for the given class. this method uses a concurrentreferencehashmap for caching method lookups.
jaxb2collectionhttpmessageconverter can read a generic
create a collection of the given type, with the given initial capacity (if supported by the collection type).
construct a new mappingjackson2xmlhttpmessageconverter with a custom objectmapper (must be a xmlmapper instance). you can use jackson2objectmapperbuilder to build it easily.
return a jaxbcontext for the given class.
create a new unmarshaller for the given class.
create a new marshaller for the given class.
return the actually supported http methods as httpmethod instances, or null if not known.
delegate the servletcontext to any webapplicationinitializer implementations present on the application classpath. because this class declares @ handlestypes(webapplicationinitializer.class), servlet 3.0+ containers will automatically scan the classpath for implementations of spring's webapplicationinitializer interface and provide the set of all such types to the webappinitializerclasses parameter of this method. if no webapplicationinitializer implementations are found on the classpath, this method is effectively a no-op. an info-level log message will be issued notifying the user that the servletcontainerinitializer has indeed been invoked but that no webapplicationinitializer implementations were found. assuming that one or more webapplicationinitializer types are detected, they will be instantiated (and sorted if the @ org.springframework.core.annotation.order @order annotation is present or the org.springframework.core.ordered ordered interface has been implemented). then the webapplicationinitializer#onstartup(servletcontext) method will be invoked on each instance, delegating the servletcontext such that each instance may register and configure servlets such as spring's or any other servlet api componentry such as filters.
constructor for a 500 error with a methodparameter and an optional cause.
this method must be invoked after all properties have been set to complete initialization.
register a servletcontextlistener that closes the given application context when the servlet context is destroyed. closed when servletcontext is destroyed
refresh the given application context, if necessary.
return the spring configuration that contains application beans including the ones detected by webhttphandlerbuilder#applicationcontext.
manipulate the "live" list of currently configured filters.
build the httphandler.
static factory method to create a new builder instance by detecting beans in an applicationcontext. the following are detected:   webhandler [1] -- looked up by the name  webfilter [0..n] -- detected by type and ordered, see annotationawareordercomparator.  webexceptionhandler [0..n] -- detected by type and ordered.  websessionmanager [0..1] -- looked up by the name  servercodecconfigurer [0..1] -- looked up by the name  localecontextresolver [0..1] -- looked up by the name 
whether the request has any forwarded headers.
apply and remove, or remove forwarded type headers.
find the names of beans annotated with applicationcontext and wrap them as controlleradvicebean instances.
create an instance from a bean instance, method name, and parameter types.
if the provided instance contains a bean name rather than an object instance, the bean name is resolved before a handlermethod is created and returned.
assert that the target bean class is an instance of the class where the given method is declared. in some cases the actual controller instance at request- processing time may be a jdk dynamic proxy (lazy initialization, prototype beans, and others). @controller's that require proxying should prefer class-based proxy mechanisms.
create an instance from a bean name, a method, and a beanfactory. the method #createwithresolvedbean() may be used later to re-create the handlermethod with an initialized bean.
iterate over registered invoke the one that supports it.
find a registered handlermethodargumentresolver that supports the given method parameter.
iterate over registered handlermethodreturnvaluehandler handlermethodreturnvaluehandlers and invoke the one that supports it.
get the method argument values for the current request, checking the provided argument values and falling back to the configured argument resolvers. the resulting array will be passed into #doinvoke.
invoke the handler method with the given argument values.
create an instance from a collection of uricomponentscontributor uricomponentscontributors or by the same class, the most convenient option is to obtain the configured and provide that to this constructor. if the conversionservice argument is null, will be used by default. or handlermethodargumentresolver handlermethodargumentresolvers. need to be formatted as strings before being added to the uri
return diagnostic information.
return the model to use -- either the "default" or the "redirect" model. the default model is used if redirectmodelscenario=false or there is no redirect model (i.e. redirectattributes was not declared as a method argument) and ignoredefaultmodelonredirect=false.
remove the given attributes from the model.
return the method mapped to the given exception type, or null if none.
find a method to handle the given throwable. use exceptiondepthcomparator if more than one match is found.
extract exception mappings from the @exceptionhandler annotation first, and then as a fallback from the method signature itself.
construct a new attribute instance with the given constructor. called from after constructor resolution.
determine any validation triggered by the given annotation. or null if this annotation does not trigger any validation
validate the specified candidate value if applicable. the default implementation checks for @javax.validation.valid, spring's org.springframework.validation.annotation.validated, and custom annotations whose name starts with "valid".
returns true if the parameter is annotated with method parameter that is not a simple type.
validate the model attribute if applicable. the default implementation checks for @javax.validation.valid, spring's org.springframework.validation.annotation.validated, and custom annotations whose name starts with "valid".
add non-null return values to the modelandviewcontainer.
whether to raise a fatal bind exception on validation errors.
return true if there is a method-level @modelattribute or, in default resolution mode, for any return value type that is not a simple type.
extension point to create the model attribute if not found in the model, with subsequent parameter binding through bean properties (unless suppressed). the default implementation typically uses the unique public no-arg constructor if available but also handles a "primary constructor" approach for data classes: it understands the javabeans constructorproperties annotation as well as runtime-retained parameter names in the bytecode, associating request parameters with constructor arguments by name. if no such constructor is found, the default constructor will be used (even if not public), assuming subsequent bean property bindings through setter methods.
resolve the argument from the model or if not found instantiate it with its default if it is available. the model attribute is then populated with request values via data binding and optionally validated if @java.validation.valid is present on the argument. and the next method parameter is not of type errors
supports the following:  @requestparam-annotated method arguments. this excludes map params where the annotation does not specify a name. see requestparammapmethodargumentresolver instead for such params. arguments of type multipartfile unless annotated with @ requestpart. arguments of type part unless annotated with @ requestpart. in default resolution mode, simple type arguments even if not with @ requestparam. 
initialize a webdatabinder with @initbinder methods. if the @initbinder annotation specifies attributes names, it is invoked only if the names include the target object name.
determine whether the given @initbinder method should be used to initialize the given webdatabinder instance. by default we check the specified attribute names in the annotation value, if any.
delegate to the webargumentresolver instance. to the method parameter.
actually resolve the value and check the resolved value is not
find @modelattribute arguments also listed as @sessionattributes.
invoke model attribute methods to populate the model. attributes are added only if not already present in the model.
add bindingresult attributes to the model for attributes that require it.
whether the given attribute requires a bindingresult in the model.
create a new instance with the given @modelattribute methods.
promote model attributes listed as @sessionattributes to the session. add bindingresult attributes where necessary.
populate the model in the following order:  retrieve "known" session attributes listed as @sessionattributes. invoke @modelattribute methods find @modelattribute method arguments also listed as an exception if necessary. 
derive the model attribute name for the given method parameter based on a @modelattribute parameter annotation (if present) or falling back on parameter type based conventions.
derive the model attribute name for the given return value. results will be based on:  the method modelattribute annotation value the declared return type if it is more specific than object the actual return value type 
retrieve "known" attributes from the session, i.e. attributes listed by name in @sessionattributes or attributes previously stored in the model that matched by type.
store a subset of the given attributes in the session. attributes not declared as session attributes via @sessionattributes are ignored.
create a new namedvalueinfo based on the given namedvalueinfo with sanitized values.
a null results in a false value for booleans or an exception for other primitives.
create a new abstractnamedvaluemethodargumentresolver instance. and #... spel expressions in default values, or null if default values are not expected to contain expressions
resolve the given annotation-specified value, potentially containing placeholders and expressions.
create a new requestpartservletserverhttprequest instance.
look for a multipartresolver bean in the root web application context. supports a "multipartresolverbeanname" filter init param; the default bean name is "filtermultipartresolver". this can be overridden to use a custom multipartresolver instance, for example if not using a spring web application context.
check for a multipart request via this filter's multipartresolver, and wrap the original request with a multiparthttpservletrequest if appropriate. all later elements in the filter chain, most importantly servlets, benefit from proper parameter extraction in the multipart case, and are able to cast to multiparthttpservletrequest if they need to.
determine the encoding for the given request. can be overridden in subclasses. the default implementation checks the request encoding, falling back to the default encoding specified for this resolver.
parse the given servlet request, resolving its multipart elements.
set the temporary directory where uploaded files get stored. default is the servlet container's temporary directory for the web application.
parse the given list of commons fileitems into a spring multipartparsingresult, containing spring multipartfile instances and a map of multipart parameter.
cleanup the spring multipartfiles created during multipart parsing, potentially holding temporary data on disk. deletes the underlying commons fileitem instances.
determine an appropriate fileupload instance for the given encoding. default implementation returns the shared fileupload instance if the encoding matches, else creates a new fileupload instance with the same configuration other than the desired encoding.
create a commonsmultipartfile wrapper for the given commons fileitem.
find the root webapplicationcontext for this web app, typically loaded via org.springframework.web.context.contextloaderlistener. will rethrow an exception that happened on root context startup, to differentiate between a failed context startup and no context at all.
return the best available mutex for the given session: that is, an object to synchronize on for the given session. returns the session mutex attribute if available; usually, this means that the httpsessionmutexlistener needs to be defined in web.xml. falls back to the session reference itself if no mutex attribute found. the session mutex is guaranteed to be the same object during the entire lifetime of the session, available under the key defined by the session_mutex_attribute constant. it serves as a safe reference to synchronize on for locking on the current session. in many cases, the session reference itself is a safe mutex as well, since it will always be the same object reference for the same active logical session. however, this is not guaranteed across different servlet containers; the only 100% safe way is a session mutex.
find the root webapplicationcontext for this web app, typically loaded via org.springframework.web.context.contextloaderlistener. will rethrow an exception that happened on root context startup, to differentiate between a failed context startup and no context at all.
handle the navigation request implied by the specified parameters, through delegating to the target bean in the spring application context. the target bean needs to extend the jsf navigationhandler class. if it extends spring's decoratingnavigationhandler, the overloaded as argument will be used. else, the standard handlenavigation method will be called.
method to be called by subclasses when intending to delegate to the next handler in the navigationhandler chain. will always call the most appropriate next handler, either the decorated navigationhandler passed in as constructor argument or the original navigationhandler as passed into this method - according to the position of this instance in the chain. will call the decorated navigationhandler specified as constructor argument, if any. in case of a decoratingnavigationhandler as target, the original navigationhandler as passed into this method will be passed on to the next element in the chain: this ensures propagation of the original handler that the last element in the handler chain might delegate back to. in case of a standard navigationhandler as target, the original handler will simply not get passed on; no delegating back to the original is possible further down the chain in that scenario. if no decorated navigationhandler specified as constructor argument, this instance is the last element in the chain. hence, this method will call the original navigationhandler as passed into this method. if no original navigationhandler has been passed in (for example if this instance is the last element in a chain with standard navigationhandlers as earlier elements), this method corresponds to a no-op. specified outcome, or null if the outcome was acquired by some other means (which may be null) or null if none
check the given property values for field markers, i.e. for fields that start with the field marker prefix. the existence of a field marker indicates that the specified field existed in the form. if the property values do not contain a corresponding field value, the field will be considered as empty and will be reset appropriately.
determine an empty value for the specified field. the default implementation returns:   boolean.false for boolean fields an empty array for array types collection implementations for collection types map implementations for map types else, null is used as default 
check the given property values for field defaults, i.e. for fields that start with the field default prefix. the existence of a field defaults indicates that the specified value should be used if the field is otherwise not present.
bind all multipart files contained in the given request, if any (in case of a multipart request). to be called by subclasses. multipart files will only be added to the property values if they are not empty or if we're configured to bind empty multipart files too.
get an array of int parameters, throwing an exception if not found or one is not a number.. so it doesn't need to be caught
get an array of string parameters, return an empty array if not found.
get a float parameter, with a fallback value. never throws an exception. can pass a distinguished value as default to enable checks of whether it was supplied.
get a boolean parameter, throwing an exception if it isn't found or isn't a boolean. accepts "true", "on", "yes" (any case) and "1" as values for true; treats every other non-empty value as false (i.e. parses leniently). so it doesn't need to be caught
get a double parameter, throwing an exception if it isn't found or isn't a number. so it doesn't need to be caught
get a float parameter, throwing an exception if it isn't found or isn't a number. so it doesn't need to be caught
get an array of long parameters, return an empty array if not found.
get a boolean parameter, with a fallback value. never throws an exception. can pass a distinguished value as default to enable checks of whether it was supplied. accepts "true", "on", "yes" (any case) and "1" as values for true; treats every other non-empty value as false (i.e. parses leniently).
get a long parameter, throwing an exception if it isn't found or isn't a number. so it doesn't need to be caught
get a double parameter, with a fallback value. never throws an exception. can pass a distinguished value as default to enable checks of whether it was supplied.
get an array of boolean parameters, return an empty array if not found. accepts "true", "on", "yes" (any case) and "1" as values for true; treats every other non-empty value as false (i.e. parses leniently).
get an int parameter, throwing an exception if it isn't found or isn't a number. so it doesn't need to be caught
get an array of double parameters, return an empty array if not found.
get a string parameter, or null if not present. so it doesn't need to be caught
get a boolean parameter, or null if not present. throws an exception if it the parameter value isn't a boolean. accepts "true", "on", "yes" (any case) and "1" as values for true; treats every other non-empty value as false (i.e. parses leniently). so it doesn't need to be caught
get an array of float parameters, return an empty array if not found.
get a double parameter, or null if not present. throws an exception if it the parameter value isn't a number. so it doesn't need to be caught
get an int parameter, with a fallback value. never throws an exception. can pass a distinguished value as default to enable checks of whether it was supplied.
get a float parameter, or null if not present. throws an exception if it the parameter value isn't a number. so it doesn't need to be caught
get an array of int parameters, return an empty array if not found.
get an array of string parameters, throwing an exception if not found. so it doesn't need to be caught
get a long parameter, or null if not present. throws an exception if it the parameter value isn't a number. so it doesn't need to be caught
get an integer parameter, or null if not present. throws an exception if it the parameter value isn't a number. so it doesn't need to be caught
get an array of boolean parameters, throwing an exception if not found or one isn't a boolean. accepts "true", "on", "yes" (any case) and "1" as values for true; treats every other non-empty value as false (i.e. parses leniently). so it doesn't need to be caught
get an array of double parameters, throwing an exception if not found or one is not a number. so it doesn't need to be caught
get an array of float parameters, throwing an exception if not found or one is not a number. so it doesn't need to be caught
get a long parameter, with a fallback value. never throws an exception. can pass a distinguished value as default to enable checks of whether it was supplied.
get an array of long parameters, throwing an exception if not found or one is not a number. so it doesn't need to be caught
get a string parameter, throwing an exception if it isn't found. so it doesn't need to be caught
bind the parameters of the given request to this binder's target, also binding multipart files in case of a multipart request. this call can create field errors, representing basic binding errors like a required field (code "required"), or type mismatch between value and bean property (code "typemismatch"). multipart files are bound via their parameter name, just like normal http parameters: i.e. "uploadedfile" to an "uploadedfile" bean property, invoking a "setuploadedfile" setter method. the type of the target property for a multipart file can be multipartfile, byte[], or string. the latter two receive the contents of the uploaded file; all metadata like original file name, content type, etc are lost in those cases.
treats errors as fatal. use this method only if it's an error if the input isn't valid. this might be appropriate if all input is from dropdowns, for example.
extension point to create the webdatabinder instance. by default this is webrequestdatabinder.
retrieve the spring webapplicationcontext to use. the default implementation returns the current webapplicationcontext as registered for the thread context class loader.
treats errors as fatal. use this method only if it's an error if the input isn't valid. this might be appropriate if all input is from dropdowns, for example.
bind the parameters of the given request to this binder's target, also binding multipart files in case of a multipart request. this call can create field errors, representing basic binding errors like a required field (code "required"), or type mismatch between value and bean property (code "typemismatch"). multipart files are bound via their parameter name, just like normal http parameters: i.e. "uploadedfile" to an "uploadedfile" bean property, invoking a "setuploadedfile" setter method. the type of the target property for a multipart file can be part, multipartfile, byte[], or string. the latter two receive the contents of the uploaded file; all metadata like original file name, content type, etc are lost in those cases.
combine query params and form data for multipart form data from the body of the request into a map object> of values to use for data binding purposes.
bind query params, form data, and or multipart form data to the binder target.
returns diagnostic information about the errors held in this object.
handle the given request.
add a response header to expose. note that "" is not a valid exposed header value.
check the origin of the request against the configured allowed origins. means the request origin is not allowed
add an actual request header to allow.
add an origin to allow.
by default a newly created corsconfiguration does not permit any cross-origin requests and must be configured explicitly to indicate what should be allowed. use this method to flip the initialization model to start with open defaults that permit all cross-origin requests for get, head, and post requests. note however that this method will not override any existing values already set. the following defaults are applied if not already set:  allow all origins. allow "simple" methods get, head and post. allow all headers. set max age to 1800 seconds (30 minutes). 
set the list of response headers other than simple headers (i.e. actual response might have and can be exposed. note that "" is not a valid exposed header value. by default this is not set.
check the supplied request headers (or the headers listed in the the configured allowed headers. request, or null if none of the supplied request headers is allowed
add an http method to allow.
set the http methods to allow, e.g. "get", "post", the special value "" allows all methods. if not set, only "get" and "head" are allowed. by default this is not set. note: cors checks use values from "forwarded" ( href="http:tools.ietf.orghtmlrfc7239">rfc 7239), "x-forwarded-host", "x-forwarded-port", and "x-forwarded-proto" headers, if present, in order to reflect the client-originated address. consider using the forwardedheaderfilter in order to choose from a central place whether to extract and use, or to discard such headers. see the spring framework reference for more on this filter.
combine the non-null properties of the supplied when combining single values like allowcredentials or combining lists like allowedorigins, allowedmethods, way. for example, combining ["get", "post"] with in mind that combining ["get", "post"] with [""] results in [""]. notice that default permit values set by any value explicitly defined. configuration if the supplied configuration is null
set cors configuration based on url patterns.
check the headers and determine the headers for the response of a pre-flight request. the default implementation simply delegates to
handle the given request.
check if the request is a same-origin one, based on origin, and note: as of 5.1 this method ignores client-originated address. consider using the forwardedheaderfilter to extract and use, or to discard such headers. of a cross-origin request
create a new instance of the resttemplate using default settings. default httpmessageconverter httpmessageconverters are initialized.
handle the given response, performing appropriate logging and invoking the responseerrorhandler if necessary. can be overridden in subclasses.
execute the given method on the provided uri. the clienthttprequest is processed using the requestcallback; the response with the responseextractor.
configure default uri variable values. this is a shortcut for:  class="code"> defaulturibuilderfactory factory = new defaulturibuilderfactory(); handler.setdefaulturivariables(...); resttemplate resttemplate = new resttemplate(); resttemplate.seturitemplatehandler(handler); 
determine the content-type of the response based on the "content-type" header or otherwise default to mediatype#application_octet_stream.
construct a new instance of with the given response data.
return the response body as a string.
creates a new instance of the asyncresttemplate using the given asynchronous and synchronous request factories.
execute the given method on the provided uri. the is processed using the requestcallback; the response with the responseextractor. be null)
configure default uri variable values. this is a shortcut for:  class="code"> defaulturitemplatehandler handler = new defaulturitemplatehandler(); handler.setdefaulturivariables(...); asyncresttemplate resttemplate = new asyncresttemplate(); resttemplate.seturitemplatehandler(handler); 
create a new instance of the asyncresttemplate using the given this constructor uses a simpleclienthttprequestfactory in combination with the given asynctaskexecutor for asynchronous execution.
read the body of the given response (for inclusion in a status exception). or an empty byte array if the body could not be read
delegates to #handleerror(clienthttpresponse, httpstatus) with the response status code.
handle the error in the given response with the given resolved status code. the default implementation throws an httpclienterrorexception if the status code is httpstatus.series#client_error, an and an unknownhttpstatuscodeexception in other cases.
determine the charset of the response (for inclusion in a status exception).
delegates to #haserror(httpstatus) (for a standard status enum value) or
determine the http status of the given response. that cannot be represented with the httpstatus enum
create httpclienterrorexception or an http status specific sub-class.
create an httpservererrorexception or an http status specific sub-class.
indicates whether the response has an empty message body. implementation tries to read the first bytes of the response stream:  if no bytes are available, the message body is empty otherwise it is not empty and the stream is reset to its start for further reading 
indicates whether the response has a message body. implementation returns false for:  a response status of 1xx, 204 or 304 a content-length header of 0 
override to provide handling when a key is not resolved via. determine the media type(s). if a mediatype is returned from this method it will be added to the cache in the base class.
find a contentnegotiationstrategy of the given type.
at startup this method returns extensions explicitly registered with either pathextensioncontentnegotiationstrategy or "path extension" strategy and its useregisteredextensionsonly property is set to "false", the list of extensions may increase as file extensions are resolved via
add a mapping from a key, extracted from a path extension or a query parameter, to a mediatype. this is required in order for the parameter strategy to work. any extensions explicitly registered here are also whitelisted for the purpose of reflected file download attack detection (see spring framework reference documentation for more details on rfd attack protection). the path extension strategy will also try to use
actually build the contentnegotiationmanager.
a public method exposing the knowledge of the path extension strategy to resolve file extensions to a mediatype in this case for a given file extensions first and then falls back on mediatypefactory if available.
create an instance with the given map of file extensions and media types.
register a contextloaderlistener against the given servlet context. the from the #createrootapplicationcontext() template method.
find all servletcontext attributes which implement disposablebean and destroy them, removing all affected servletcontext attributes eventually.
return the applicationcontextinitializer implementation classes to use if any have been specified by #context_initializer_classes_param.
obtain the spring root web application context for the current thread (i.e. for the current thread's context classloader, which needs to be the web application's classloader). if none found
return the webapplicationcontext implementation class to use, either the default xmlwebapplicationcontext or a custom context class if specified.
customize the configurablewebapplicationcontext created by this contextloader after config locations have been supplied to the context but before the context is refreshed. the default implementation #determinecontextinitializerclasses(servletcontext) determines what (if any) context initializer classes have been specified through given web application context. any applicationcontextinitializers implementing org.springframework.core.annotation.order order will be sorted appropriately.
close spring's web application context for the given servlet context. if overriding #loadparentcontext(servletcontext), you may have to override this method as well.
instantiate the root webapplicationcontext for this loader, either the default context class or a custom context class if specified. this implementation expects custom contexts to implement the can be overridden in subclasses. in addition, #customizecontext gets called prior to refreshing the context, allowing subclasses to perform custom modifications to the context.
initialize spring's web application context for the given servlet context, using the application context provided at construction time, or creating a new one according to the " #context_class_param contextclass" and " #config_location_param contextconfiglocation" context-params.
bind the given requestattributes to the current thread. or null to reset the thread-bound context for child threads (using an inheritablethreadlocal)
return the requestattributes currently bound to the thread. exposes the previously bound requestattributes instance, if any. falls back to the current jsf facescontext, if any. is bound to the current thread
return the requestattributes currently bound to the thread. or null if none bound
update all accessed session attributes through session.setattribute calls, explicitly indicating to the container that they might have been modified.
register the given callback as to be executed after session termination. note: the callback object should be serializable in order to survive web app restarts.
exposes the httpsession that we're wrapping.
execute all callbacks that have been registered for execution after request completion.
obtain the webasyncmanager for the current request, or if not found, create and associate it with the request.
obtain the webasyncmanager for the current request, or if not found, create and associate it with the request.
start concurrent request processing and initialize the given the result and dispatches the request to resume processing of that result. the asyncwebrequest is also updated with a completion handler that expires the deferredresult and a timeout handler assuming the deferredresult has a default timeout result. via #getconcurrentresultcontext()
start concurrent request processing and execute the given task with an from the task execution is saved and the request dispatched in order to resume processing of that result. if the task raises an exception then the saved result will be the raised exception. via #getconcurrentresultcontext()
register one or more deferredresultprocessinginterceptor deferredresultprocessinginterceptors without a specified key. the default key is derived from the interceptor class name and hash code.
use the given webasynctask to configure the task executor as well as the timeout value of the asyncwebrequest before delegating to via #getconcurrentresultcontext()
register a callableprocessinginterceptor without a key. the key is derived from the class name and hashcode.
provide a handler to use to handle the result value.
determine if further error handling should be bypassed. error handling
customize the set of property sources with those contributed by superclasses as well as those appropriate for standard servlet-based environments:  @value #servlet_config_property_source_name @value #servlet_context_property_source_name @value #jndi_property_source_name  properties present in @value #servlet_config_property_source_name will take precedence over those in @value #servlet_context_property_source_name, and properties found in either of the above take precedence over those found in @value #jndi_property_source_name. properties in any of the above will take precedence over system properties and environment variables contributed by the standardenvironment superclass. the servlet-related property sources are added as once the actual servletcontext object becomes available.
return a full description of this event, involving all available context data.
return a short description of this event, only involving the most important context data.
this implementation delegates to servletcontext.getresource, but throws a filenotfoundexception if no resource found.
this implementation delegates to servletcontext.getresourceasstream, but throws a filenotfoundexception if no resource found.
create a new servletcontextresource. the servlet spec requires that resource paths start with a slash, even if many containers accept paths without leading slash too. consequently, the given path will be prepended with a slash if it doesn't already start with one.
this implementation creates a servletcontextresource, applying the given path relative to the path of the underlying file of this resource descriptor.
this implementation resolves "file:" urls or alternatively delegates to if not found or not resolvable.
this implementation delegates to servletcontext.getresourceasstream, which returns null in case of a non-readable resource (e.g. a directory).
register a org.springframework.beans.factory.config.beandefinition for any classes specified by #register(class...) and scan any packages specified by #scan(string...). for any values specified by #setconfiglocation(string) or class, registering a beandefinition if class loading is successful, and if class loading fails (i.e. a classnotfoundexception is raised), assume the value is a package and attempt to scan it for annotated classes. enables the default set of annotation configuration post processors, such that configuration class bean definitions are registered with generated bean definition names unless the value attribute is provided to the stereotype annotation.
load the bean definitions with the given xmlbeandefinitionreader. the lifecycle of the bean factory is handled by the refreshbeanfactory method; therefore this method is just supposed to load andor register bean definitions. delegates to a resourcepatternresolver for resolving location patterns into resource instances.
loads the bean definitions via an xmlbeandefinitionreader.
the default location for the root context is "web-infapplicationcontext.xml", and "web-inftest-servlet.xml" for a context with the namespace "test-servlet" (like for a dispatcherservlet instance with the servlet-name "test").
register requestsession scopes, a servletcontextawareprocessor, etc.
this implementation supports file paths beneath the root of the servletcontext.
process @autowired injection for the given target object, based on the current web application context. intended for use as a delegate.
process @autowired injection for the given target object, based on the current root web application context as stored in the servletcontext. intended for use as a delegate.
register servletcontextawareprocessor.
this implementation supports file paths beneath the root of the servletcontext.
register web-specific environment beans ("contextparameters", "contextattributes") with the given beanfactory, as used by the webapplicationcontext.
find the root webapplicationcontext for this web app, typically loaded via org.springframework.web.context.contextloaderlistener. will rethrow an exception that happened on root context startup, to differentiate between a failed context startup and no context at all.
return the current requestattributes instance as servletrequestattributes.
find a unique webapplicationcontext for this web app: either the root web app context (preferred) or a unique webapplicationcontext among the registered servletcontext attributes (typically coming from a single dispatcherservlet in the current web application). note that dispatcherservlet's exposure of its context can be controlled through its publishcontext property, which is true by default but can be selectively switched to only publish a single context despite multiple dispatcherservlet registrations in the web app.
register web-specific scopes ("request", "session", "globalsession", "application") with the given beanfactory, as used by the webapplicationcontext.
replace servlet-based stubpropertysource stub property sources with actual instances populated with the given servletcontext and this method is idempotent with respect to the fact it may be called any number of times but will perform replacement of stub property sources with their corresponding actual property sources once and only once. be null) or if the standardservletenvironment#servlet_context_property_source_name servlet context property source has already been initialized) or if the standardservletenvironment#servlet_config_property_source_name servlet config property source has already been initialized)
find a custom webapplicationcontext for this web app.
load the bean definitions with the given groovybeandefinitionreader. the lifecycle of the bean factory is handled by the refreshbeanfactory method; therefore this method is just supposed to load andor register bean definitions. delegates to a resourcepatternresolver for resolving location patterns into resource instances.
the default location for the root context is "web-infapplicationcontext.groovy", and "web-inftest-servlet.groovy" for a context with the namespace "test-servlet" (like for a dispatcherservlet instance with the servlet-name "test").
loads the bean definitions via an groovybeandefinitionreader.
overridden version which checks for servletcontextresource and uses servletcontext.getresourcepaths to find matching resources below the web application root directory. in case of other resources, delegates to the superclass version.
extract entries from the given jar by pattern.
recursively retrieve servletcontextresources that match the given pattern, adding them to the given result set. with preprended root directory path
return the current servletcontext.
return the current application context as webapplicationcontext. note: only use this if you actually need to access webapplicationcontext-specific functionality. preferably use else, to be able to run in non-webapplicationcontext environments as well.
invoke all registered destruction callbacks. to be called on servletcontext shutdown.
register requestsession scopes, a servletcontextawareprocessor, etc.
this implementation supports file paths beneath the root of the servletcontext.
make the servletcontext of this filter available to subclasses. analogous to genericservlet's getservletcontext(). takes the filterconfig's servletcontext by default. if initialized as bean in a spring application context, it falls back to the servletcontext that the bean factory runs in.
standard way of initializing this filter. map config parameters onto bean properties of this filter, and invoke subclass initialization. properties are missing), or if subclass initialization fails.
create new filterconfigpropertyvalues. we can't accept default values
this dofilter implementation stores a request attribute for "already filtered", proceeding without filtering again if the attribute is already there.
return the webapplicationcontext passed in at construction time, if available. otherwise, attempt to retrieve a webapplicationcontext from the configured name if set. otherwise look up a webapplicationcontext under the well-known "root" application context attribute. the subclasses may override this method to provide a different
actually invoke the delegate filter with the given request and response.
destroy the filter delegate. default implementation simply calls filter.destroy on it.
initialize the filter delegate, defined as bean the given spring application context. the default implementation fetches the bean from the application context and calls the standard filter.init method on it, passing in the filterconfig of this filter proxy.
clean up all the filters supplied, calling each one's destroy method in turn, but in reverse order.
forms a temporary chain from the list of delegate filters supplied ( #setfilters) and executes them in order. each filter delegates to the next one in the list, achieving the normal behavior of a filterchain, despite the fact that this is a filter.
initialize all the filters, calling each one's init method in turn in the order supplied.
indicates whether the given request and response are eligible for etag generation. the default implementation returns true if all conditions match:  response status codes in the 2xx series request method is a get response cache-control header is not set or does not contain a "no-store" directive 
generate the etag header value from the given response body byte array. the default implementation generates an md5 hash.
constructor with required information. may change during a forward (e.g. tomcat.
create a log message for the given request, prefix and suffix. if includequerystring is true, then the inner part of the log message will take the form request_uri?query_string; otherwise the message will simply be of the form request_uri. the final message is composed of the inner part as described and the supplied prefix and suffix.
forwards the request to the next filter in the chain and delegates down to the subclasses to perform the actual request logging both before and after the request is processed.
extracts the message payload portion of the message created by
transform an http post into another method based on methodparamname.
remove nested "" such as in uri vars with regular expressions.
return an appropriate response object of the specified type, if available, unwrapping the given response as far as necessary. of that type is available
check the given request origin against a list of allowed origins. a list containing "" means that all origins are allowed. an empty list means only same origin is allowed. note: as of 5.1 this method ignores client-originated address. consider using the forwardedheaderfilter to extract and use, or to discard such headers.
return whether default html escaping is enabled for the web application, i.e. the value of the "defaulthtmlescape" context-param in web.xml (if any). this method differentiates between no param specified at all and an actual boolean value specified, allowing to have a context-specific default in case of no setting at the global level. ( null = no explicit default)
return the best available mutex for the given session: that is, an object to synchronize on for the given session. returns the session mutex attribute if available; usually, this means that the httpsessionmutexlistener needs to be defined in web.xml. falls back to the httpsession itself if no mutex attribute found. the session mutex is guaranteed to be the same object during the entire lifetime of the session, available under the key defined by the session_mutex_attribute constant. it serves as a safe reference to synchronize on for locking on the current session. in many cases, the httpsession reference itself is a safe mutex as well, since it will always be the same object reference for the same active logical session. however, this is not guaranteed across different servlet containers; the only 100% safe way is a session mutex.
return whether response encoding should be used when html escaping characters, thus only escaping xml markup significant characters with utf- encodings. this option is enabled for the web application with a servletcontext param, i.e. the value of the "responseencodedhtmlescape" context-param in web.xml (if any). this method differentiates between no param specified at all and an actual boolean value specified, allowing to have a context-specific default in case of no setting at the global level. ( null = no explicit default)
return an appropriate request object of the specified type, if available, unwrapping the given request as far as necessary. of that type is available
clear the servlet spec's error attributes as javax.servlet.http.httpservletrequest attributes under the keys defined in the servlet 2.3 specification:
return a map containing all parameters with the given prefix. maps single values to string and multiple values to string array. for example, with a prefix of "spring_", "spring_param1" and "spring_param2" result in a map with "param1" and "param2" as keys. (if this is null or the empty string, all parameters will match) containing either a string or a string array as values
retrieve the first cookie with the given name. note that multiple cookies can have the same name but different paths or domains.
determine the session id of the given request, if any.
check the given request for a session attribute of the given name. returns null if there is no session or if the session has no such attribute. does not create a new session if none has existed before!
check the given request for a session attribute of the given name. throws an exception if there is no session or if the session has no such attribute. does not create a new session if none has existed before!
return the real path of the given path within the web application, as provided by the servlet container. prepends a slash if the path does not already start with a slash, and throws a filenotfoundexception if the path cannot be resolved to a resource (in contrast to servletcontext's getrealpath, which returns null).
expose the specified request attribute if not already present.
set the session attribute with the given name to the given value. removes the session attribute if value is null, if a session existed at all. does not create a new session if not necessary!
remove the system property that points to the web app root directory. to be called on shutdown of the web application.
check if the request is a same-origin one, based on origin, host, note: as of 5.1 this method ignores client-originated address. consider using the forwardedheaderfilter to extract and use, or to discard such headers. of cross-origin request
check if a specific input type="submit" parameter was sent in the request, either via a button (directly with name) or via an image (name + ".x" or name + ".y").
set a system property to the web application root directory. the key of the system property can be defined with the "webapprootkey" context-param in web.xml. default is "webapp.root". can be used for tools that support substitution with system.getproperty values, like log4j's "$key" syntax within log file locations. or if the war file is not expanded
obtain a named parameter from the given request parameters. this method will try to obtain a parameter value using the following algorithm:  try to get the parameter value using just the given logical name. this handles parameters of the form logicalname = value. for normal parameters, e.g. submitted using a hidden html form field, this will return the requested value. try to obtain the parameter value from the parameter name, where the parameter name in the request is of the form logicalname_value = xyz with "_" being the configured delimiter. this deals with parameter values submitted using an html form submit button. if the value obtained in the previous step has a ".x" or ".y" suffix, remove that. this handles cases where the value was submitted using an html form image button. in this case the parameter in the request would actually be of the form logicalname_value.x = 123.   if the parameter does not exist in given request
parse the given string with matrix variables. an example string would look like this "q1=a;q1=b;q2=a,b,c". the resulting map would contain keys "q1" and "q2" with values ["a","b"] and
copy the cached body content to the response. for the complete cached body content
identical to #encode() but skipping over uri variable placeholders. also #variableencoder is initialized with the given charset for use later when uri variables are expanded.
encode the given source into an encoded string using the rules specified by the given component and with the given options.
create a uricomponentsbuilder from the uri template string. this implementation also breaks up the path into path segments depending on whether #setparsepath parsepath is enabled.
determine whether the supplied tag has any ancestor tag of the supplied type. of the supplied type or if the supplied ancestortagclass is not type-assignable to the tag class
determine whether the supplied tag has any ancestor tag of the supplied type, throwing an illegalstateexception if not. have a tag of the supplied parenttagclass as an ancestor or in the case of the string-typed arguments, is composed wholly of whitespace; or if the supplied ancestortagclass is not type-assignable to the tag class
determines the scope for a given input string. if the string does not match 'request', 'session', 'page' or 'application', the method will return pagecontext#page_scope.
convenience method to apply #encode(string, charset) to all given uri variable values.
extract the file extension from the given uri path.
resolve $... placeholders in the given text, replacing them with corresponding servlet context init parameter or system property values. unresolvable placeholders with no default value are ignored and passed through unchanged if the flag is set to true.
return the query string part of the given request's url. if this is a forwarded request, correctly resolves to the query string of the original request.
return the servlet path for the given request, detecting an include request url if called within a requestdispatcher include.
sanitize the given path. uses the following rules:  replace all "" by "" 
return the path within the servlet mapping for the given request, i.e. the part of the request's url beyond the part that called the servlet, or "" if the whole url has been used to identify the servlet. detects include request url if called within a requestdispatcher include. e.g.: servlet mapping = ""; request uri = "testa" -> "testa". e.g.: servlet mapping = ""; request uri = "testa" -> "testa". e.g.: servlet mapping = "test"; request uri = "testa" -> "a". e.g.: servlet mapping = "test"; request uri = "test" -> "". e.g.: servlet mapping = ".test"; request uri = "a.test" -> "".
return the context path for the given request, detecting an include request url if called within a requestdispatcher include. as the value returned by request.getcontextpath() is not decoded by the servlet container, this method will decode it.
decode the given matrix variables via assumed the url path from which the variables were extracted is already decoded through a call to
return the request uri for the given request, detecting an include request url if called within a requestdispatcher include. as the value returned by request.getrequesturi() is not decoded by the servlet container, this method will decode it. the uri that the web container resolves should be correct, but some containers like jbossjetty incorrectly include ";" strings like ";jsessionid" in the uri. this method cuts off such incorrect appendices.
return the context path for the given request, detecting an include request url if called within a requestdispatcher include. as the value returned by request.getcontextpath() is not decoded by the servlet container, this method will decode it.
decode the given uri path variables via assumed the url path from which the variables were extracted is already decoded through a call to
determine the encoding for the given request. can be overridden in subclasses. the default implementation checks the request encoding, falling back to the default encoding specified for this resolver.
return the request uri for the given request. if this is a forwarded request, correctly resolves to the request uri of the original request.
match the given "mapping" to the start of the "requesturi" and if there is a match return the extra part. this method is needed because the context path and the servlet path returned by the httpservletrequest are stripped of semicolon content unlike the requesuri.
return the servlet path for the given request, regarding an include request url if called within a requestdispatcher include. as the value returned by request.getservletpath() is already decoded by the servlet container, this method will not attempt to decode it.
create a uri components builder from the given http url string. note: the presence of reserved characters can prevent correct parsing of the uri string. for example if a query parameter contains '=' or  characters, the query string cannot be parsed unambiguously. such values should be substituted for uri variables to enable correct parsing:  class="code"> string urlstring = "https:example.comhotels42?filter=value"; uricomponentsbuilder.fromhttpurl(urlstring).buildandexpand("hot&cold"); 
create a builder that is initialized with the given uri.
adapt this builder's scheme+host+port from the given headers, specifically "forwarded" ( href="http:tools.ietf.orghtmlrfc7239">rfc 7239, or "x-forwarded-host", "x-forwarded-port", and "x-forwarded-proto" if "forwarded" is not found. note: this method uses values from forwarded headers, if present, in order to reflect the client-originated protocol and address. consider using the forwardedheaderfilter in order to choose from a central place whether to extract and use, or to discard such headers. see the spring framework reference for more on this filter.
initialize components of this builder from components of the given uri.
set the path of this builder overriding all existing path and path segment values.
create a builder that is initialized with the given path.
append the given query to the existing query of this builder. the given query may contain uri template variables. note: the presence of reserved characters can prevent correct parsing of the uri string. for example if a query parameter contains '=' or  characters, the query string cannot be parsed unambiguously. such values should be substituted for uri variables to enable correct parsing:  class="code"> uricomponentsbuilder.fromuristring("hotels42") .query("filter=value") .buildandexpand("hot&cold"); 
create a builder that is initialized with the given uri string. note: the presence of reserved characters can prevent correct parsing of the uri string. for example if a query parameter contains '=' or  characters, the query string cannot be parsed unambiguously. such values should be substituted for uri variables to enable correct parsing:  class="code"> string uristring = "hotels42?filter=value"; uricomponentsbuilder.fromuristring(uristring).buildandexpand("hot&cold"); 
create an instance by parsing the "origin" header of an http request.
append the given query parameter to the existing query parameters. the given name or any of the values may contain uri template variables. if no values are given, the resulting uri will contain the query parameter name only (i.e. ?foo instead of ?foo=bar).
set or append individual uri components of this builder from the values of the given uricomponents instance. for the semantics of each component (i.e. set vs append) check the builder methods on this class. for example #host(string) sets while #path(string) appends.
insert a base url (if configured) unless the given url has a host already.
turn special characters into html character references. handles complete character set defined in html 4.01 recommendation. escapes all special characters to their corresponding numeric reference in hex format (&#xhex;) at least as required by the specified encoding. in other words, if a special character does not have to be escaped for the given encoding, it may not be. reference:  href="http:www.w3.orgtrhtml4sgmlentities.html"> http:www.w3.orgtrhtml4sgmlentities.html 
turn special characters into html character references. handles complete character set defined in html 4.01 recommendation. escapes all special characters to their corresponding entity reference (e.g. <) at least as required by the specified encoding. in other words, if a special character does not have to be escaped for the given encoding, it may not be. reference:  href="http:www.w3.orgtrhtml4sgmlentities.html"> http:www.w3.orgtrhtml4sgmlentities.html 
turn special characters into html character references. handles complete character set defined in html 4.01 recommendation. escapes all special characters to their corresponding numeric reference in decimal format (&#decimal;) at least as required by the specified encoding. in other words, if a special character does not have to be escaped for the given encoding, it may not be. reference:  href="http:www.w3.orgtrhtml4sgmlentities.html"> http:www.w3.orgtrhtml4sgmlentities.html 
create a cookie with the given value, using the cookie descriptor settings of this generator (except for "cookiemaxage").
add a cookie with the given value to the response, using the cookie descriptor settings of this generator. delegates to #createcookie for cookie creation.
remove the cookie that this generator describes from the response. will generate a cookie with empty value and max age 0. delegates to #createcookie for cookie creation.
create a new contentcachingrequestwrapper for the given servlet request.
create a new contentcachingrequestwrapper for the given servlet request.
construct a new uritemplate with the given uri string.
match the given uri to a map of variable values. keys in the returned map are variable names, values are variable values, as occurred in the given uri. example:  class="code"> uritemplate template = new uritemplate("http:example.comhotelshotelbookingsbooking"); system.out.println(template.match("http:example.comhotels1bookings42"));  will print:  hotel=1, booking=42
indicate whether the given uri matches this template.
given the map of variables, expands this template into a uri. the map keys represent variable names, the map values variable values. the order of variables is not significant. example:  class="code"> uritemplate template = new uritemplate("http:example.comhotelshotelbookingsbooking"); map<string, string> urivariables = new hashmap<string, string>(); urivariables.put("booking", "42"); urivariables.put("hotel", "rest  relax"); system.out.println(template.expand(urivariables));  will print:  http:example.comhotelsrest%20%26%20relaxbookings42 or if it does not contain values for all the variable names
given an array of variables, expand this template into a full uri. the array represent variable values. the order of variables is significant. example:  class="code"> uritemplate template = new uritemplate("http:example.comhotelshotelbookingsbooking"); system.out.println(template.expand("rest  relax", 42));  will print:  http:example.comhotelsrest%20%26%20relaxbookings42 or if it does not contain sufficient variables
return the reference mapped to the given character, or null if none found.
returns a new set of character entity references reflecting the html 4.0 character set.
turn javascript special characters into escaped characters.
matching a separator is easy, basically the character at candidateindex must be the separator.
matching on a wildcardpathelement is quite straight forward. scan the candidate from the candidateindex onwards for the next separator or the end of the candidate.
used the knowledge built up whilst processing since the last path element to determine what kind of path element to create.
record a new captured variable. if it clashes with an existing one then report an error.
package private delegate for pathpatternparser#parse(string).
push a path element to the chain being build.
just hit a ':' and want to jump over the regex specification for this variable. pos will be pointing at the ':', we want to skip until the .  nested ... pairs don't have to be escaped: abcvar:x1,2def an escaped will not be treated as the end of the regex: abcvar:x\\y:def a separator that should not indicate the end of the regex can be escaped:
create a new capturevariablepathelement instance.
match the beginning of the given path and return the remaining portion not covered by this pattern. this is useful for matching nested routes where the path is matched incrementally at each level.
whether this pattern matches the given path.
return the string form of the pattern built from walking the path element chain.
match this pattern to the given uri path and return extracted uri template variables as well as path parameters (matrix variables).
determine the pattern-mapped part for the given path. for example:  ' docscvscommit.html' and ' docscvscommit.html -> '' ' docs' and ' docscvscommit' -> ' cvscommit' ' docscvs.html' and ' docscvscommit.html -> ' commit.html' ' docs' and ' docscvscommit -> ' cvscommit'  notes:  assumes that #matches returns true for the same path but does not enforce this. duplicate occurrences of separators within the returned result are removed leading and trailing separators are removed from the returned result  of it is matched by pattern elements
join two paths together including a separator if necessary. extraneous separators are removed (if the first path ends with one and the second path starts with one).
combine this pattern with another. currently does not produce a new pathpattern, just produces a new string.
compare this pattern with a supplied pattern: return -1,0,+1 if this pattern is more specific, the same or less specific than the supplied pattern. the aim is to sort more specific patterns first.
return a detailed message that includes the original pattern text with a pointer to the error position, as well as the error message.
calculate the full endpoint address for the given endpoint.
create a jax-ws service according to the parameters of this factory.
allow map access to the custom properties to be set on the stub, with the option to add or override specific entries. useful for specifying entries directly, for example via "customproperties[mykey]". this is particularly useful for adding or overriding entries in child bean definitions.
initialize the jax-ws port for this interceptor.
initialize this client interceptor's properties from the given webservice annotation, if necessary and possible (i.e. if "wsdldocumenturl", "namespaceuri", "servicename" and "portname" haven't been set but corresponding values are declared at the annotation level of the specified service interface).
perform a jax-ws service invocation on the given port stub.
obtain the port stub from the given jax-ws service.
perform a jax-ws service invocation based on the given method invocation.
prepare the given jax-ws port stub, applying properties to it. called by #prepare.
build the httpcontext for the given endpoint.
stops all published endpoints, taking the web services offline.
publish all javax.jws.webservice annotated beans in the containing beanfactory.
obtains all web service beans and publishes them as jax-ws endpoints.
reads a remote invocation from the request, executes it, and writes the remote invocation result to the response.
write the given remoteinvocationresult to the given http response.
serialize the given remoteinvocation to the given outputstream. the default implementation gives #decorateoutputstream a chance to decorate the stream first (for example, for custom encryption or compression). creates an java.io.objectoutputstream for the final stream and calls can be overridden for custom serialization of the invocation.
deserialize a remoteinvocation object from the given inputstream. gives #decorateinputstream a chance to decorate the stream first (for example, for custom encryption or compression). creates a and calls #doreadremoteinvocation to actually read the object. can be overridden for custom serialization of the invocation.
return the httpinvokerrequestexecutor used by this remote accessor. creates a default simplehttpinvokerrequestexecutor if no executor has been initialized already.
convert the given http invoker access exception to an appropriate spring remoteaccessexception. original exception propagated to the caller
serialize the given remoteinvocation to the given outputstream. the default implementation gives decorateoutputstream a chance to decorate the stream first (for example, for custom encryption or compression). creates an objectoutputstream for the final stream and calls can be overridden for custom serialization of the invocation.
serialize the given remoteinvocation into a bytearrayoutputstream.
deserialize a remoteinvocationresult object from the given inputstream. gives decorateinputstream a chance to decorate the stream first (for example, for custom encryption or compression). creates an calls doreadremoteinvocationresult to actually read the object. can be overridden for custom serialization of the invocation.
perform the actual reading of an invocation object from the given objectinputstream. the default implementation simply calls readobject. can be overridden for deserialization of a custom wrapper object rather than the plain invocation, for example an encryption-aware holder. couldn't get resolved
validate the given response as contained in the httppost object, throwing an exception if it does not correspond to a successful http response. default implementation rejects any http status code beyond 2xx, to avoid parsing the response body and trying to deserialize from a corrupted stream.
create a httppost for the given configuration. the default implementation creates a standard httppost with "applicationx-java-serialized-object" as "content-type" header. target service
execute the given request through the httpclient. this method implements the basic processing workflow: the actual work happens in this class's template methods.
determine whether the given response indicates a gzip response. the default implementation checks whether the http "content-encoding" header contains "gzip" (in any casing).
extract the response body from the given executed remote invocation request. the default implementation simply fetches the httppost's response body stream. if the response is recognized as gzip response, the inputstream will get wrapped in a gzipinputstream.
set the given serialized remote invocation as request body. the default implementation simply sets the serialized invocation as the httppost's request body. this can be overridden, for example, to write a specific encoding and to potentially set appropriate http request headers. remoteinvocation object
execute the given httppost instance.
open an httpurlconnection for the given remote invocation request. target service
prepare the given http connection. the default implementation specifies post as method, "applicationx-java-serialized-object" as "content-type" header, and the given content length as "content-length" header.
set the given serialized remote invocation as request body. the default implementation simply write the serialized invocation to the httpurlconnection's outputstream. this can be overridden, for example, to write a specific encoding and potentially set appropriate http request headers. remoteinvocation object
validate the given response as contained in the httpurlconnection object, throwing an exception if it does not correspond to a successful http response. default implementation rejects any http status code beyond 2xx, to avoid parsing the response body and trying to deserialize from a corrupted stream.
extract the response body from the given executed remote invocation request. the default implementation simply reads the serialized invocation from the httpurlconnection's inputstream. if the response is recognized as gzip response, the inputstream will get wrapped in a gzipinputstream.
reads a remote invocation from the request, executes it, and writes the remote invocation result to the response.
write the given remoteinvocationresult to the given http response.
serialize the given remoteinvocation to the given outputstream. the default implementation gives #decorateoutputstream a chance to decorate the stream first (for example, for custom encryption or compression). creates an java.io.objectoutputstream for the final stream and calls can be overridden for custom serialization of the invocation.
processes the incoming hessian request and creates a hessian response.
initialize this exporter.
actually invoke the skeleton with the given streams.
convert the given hessian access exception to an appropriate spring remoteaccessexception.
initialize the hessian proxy for this interceptor.
processes the incoming hessian request and creates a hessian response.
trigger cancellation of this scheduled task.
add a fixed-delay intervaltask.
add a triggertask.
schedule the specified fixed-delay task, either right away if possible or on initialization of the scheduler. (or null if processing a previously registered task)
schedule all registered tasks against the underlying
schedule the specified trigger task, either right away if possible or on initialization of the scheduler.
set the taskscheduler to register scheduled tasks with, or a
add a crontask.
schedule the specified cron task, either right away if possible or on initialization of the scheduler. (or null if processing a previously registered task)
schedule the specified fixed-rate task, either right away if possible or on initialization of the scheduler. (or null if processing a previously registered task)
schedule the specified fixed-rate task, either right away if possible or on initialization of the scheduler. (or null if processing a previously registered task)
schedule the specified fixed-delay task, either right away if possible or on initialization of the scheduler. (or null if processing a previously registered task)
add a fixed-rate intervaltask.
decorate the task for error handling. if the provided errorhandler is not null, it will be used. otherwise, repeating tasks will have errors suppressed by default whereas one-shot tasks will have errors propagated by default since those errors may be expected through the returned future. in both cases, the errors will be logged.
determine whether the specified expression represents a valid cron pattern.
reset the calendar setting all the fields provided to zero.
search the bits provided for the next set bit after the value provided, and reset the calendar. ones of lower significance than the field of interest)
get the next date in the sequence matching the cron pattern and after the value provided. the return value will have a whole number of seconds, and will be after the input value.
parse the given pattern expression.
returns the time after which a task should run again.
determine the next execution time according to the given trigger context. next execution times are calculated based on the previous execution; therefore, overlapping executions won't occur.
register the specified scheduledexecutortask scheduledexecutortasks on the given scheduledexecutorservice.
determine the actual runnable to schedule for the given task. wraps the task's runnable in a that will catch and log the exception. if necessary, it will suppress the exception according to the flag.
create a new scheduledexecutorservice instance. the default implementation creates a scheduledthreadpoolexecutor. can be overridden in subclasses to provide custom scheduledexecutorservice instances.
perform a shutdown on the underlying executorservice.
set up the executorservice.
wait for the executor to terminate, according to the value of the
create the blockingqueue to use for the threadpoolexecutor. a linkedblockingqueue instance will be created for a positive capacity value; a synchronousqueue else.
create a new instance of threadpoolexecutor or a subclass thereof. the default implementation creates a standard threadpoolexecutor. can be overridden to provide custom threadpoolexecutor subclasses.
create a new scheduledexecutorservice instance. the default implementation creates a scheduledthreadpoolexecutor. can be overridden in subclasses to provide custom scheduledexecutorservice instances.
create the blockingqueue to use for the threadpoolexecutor. a linkedblockingqueue instance will be created for a positive capacity value; a synchronousqueue else.
note: this method exposes an executorservice to its base class but stores the actual threadpoolexecutor handle internally. do not override this method for replacing the executor, rather just for decorating its executorservice handle or storing custom state.
return the qualifier or bean name of the executor to be used when executing the given method, specified via async#value at the method or declaring class level. if @async is specified at both the method and class level, the method's #value takes precedence (even if empty string, indicating that the default executor should be used preferentially).
process the given @scheduled method declaration on the given bean.
return all currently scheduled tasks, from scheduled methods as well as from programmatic schedulingconfigurer interaction.
create a runnable for the given bean instance, calling the specified scheduled method. the default implementation creates a scheduledmethodrunnable.
collect any asyncconfigurer beans through autowiring.
returns proxyasyncconfiguration or aspectjasyncconfiguration for proxy and aspectj values of enableasync#mode(), respectively.
set the 'async' annotation type. the default async annotation type is the async annotation, as well as the ejb 3.1 javax.ejb.asynchronous annotation (if present). this setter property exists so that developers can provide their own (non-spring-specific) annotation type to indicate that a method is to be executed asynchronously.
calculate a pointcut for the given async annotation types, if any.
create a new asyncannotationadvisor for the given task executor. (can be null to trigger default executor resolution) handle unexpected exception thrown by asynchronous method executions
determine the exposed exception: either the cause of a given
create a new simpleloadtimeweaver for the given class loader. weaving (must support the required weaving methods). does not support the required weaving methods
apply transformation on a given class byte definition. the method will always return a non-null byte array (if no transformation has taken place the array content will be identical to the original one).
create a new instance of the tomcatloadtimeweaver class using the supplied classloader.
create a new instance of the glassfishloadtimeweaver class using the supplied classloader.
create a new websphereclasspredefineplugin. (must not be null)
create a new instance of the jbossloadtimeweaver class using the supplied classloader.
determine the caching attribute for this method invocation. defaults to the class's caching attribute if no method attribute is found. is not cacheable
create an evaluationcontext.
return an identifying description for this caching operation. available to subclasses, for inclusion in their tostring() result.
create a new cacheoperation instance from the given builder.
create a new simplekey instance.
generate a key based on the specified parameters.
load the param information only when needed.
add an attribute for a cacheable method. method names can be exact matches, or of the pattern "xxx", "xxx" or "xxx" for matching multiple methods.
execute cache#evict(object) on the specified cache and invoke the error handler if an exception occurs.
execute cache#get(object) on the specified cache and invoke the error handler if an exception occurs. return null if the handler does not throw any exception, which simulates a cache miss in case of error.
execute cache#clear() on the specified cache and invoke the error handler if an exception occurs.
execute cache#put(object, object) on the specified cache and invoke the error handler if an exception occurs.
return a bean with the specified name and type. used to resolve services that are referenced by name in a cacheoperation.
configure this aspect with the given error handler, key generator and cache resolvermanager suppliers, applying the corresponding default if a supplier is not resolvable.
find a cached item only for cacheableoperation that passes the condition. or null if none is found
return the cacheoperationmetadata for the specified operation. resolve the cacheresolver and the keygenerator to be used for the operation.
compute the key for the given caching operation.
collect the cacheputrequest for all cacheoperation using the specified result item.
set one or more cache operation sources which are used to find the cache attributes. if more than one source is provided, they will be aggregated using a compositecacheoperationsource.
registers a cache aspect.  class="code"> <bean id="cacheaspect" class="org.springframework.cache.aspectj.annotationcacheaspect" factory-method="aspectof"> <property name="cachemanager" ref="cachemanager"> <property name="keygenerator" ref="keygenerator"> <bean> 
parses the ' ' tag. will register an autoproxycreator with the container as necessary.
parse the cache resolution strategy to use. if a 'cache-resolver' attribute is set, it is injected. otherwise the 'cache-manager' is set. if setboth is true, both service are actually injected.
convert the given user value, as passed into the put method, to a value in the internal store (adapting null).
dynamically register an additional cache with this manager.
update the exposed #cachenames set with the given name. this will always be called within a full #cachemap lock and effectively behaves like a copyonwritearrayset with preserved order but exposed as an unmodifiable reference.
initialize the static configuration of caches. triggered on startup through #afterpropertiesset(); can also be called to re-initialize at runtime.
this implementation always returns a cache implementation that will not store items. additionally, the request cache will be remembered by the manager for consistency.
create a new concurrentmapcache instance for the specified cache name.
return the imports to use if the advicemode is set to advicemode#aspectj. take care of adding the necessary jsr-107 import if it is available.
return the imports to use if the advicemode is set to advicemode#proxy. take care of adding the necessary jsr-107 import if it is available.
apply the defaults to the specified cacheoperation.builder.
validates the specified cacheoperation. throws an illegalstateexception if the state of the operation is invalid. as there might be multiple sources for default values, this ensure that the operation is in a proper state before being returned.
determine the cache operation(s) for the given cacheoperationprovider. this implementation delegates to configured for parsing known annotations into spring's metadata attribute class. can be overridden to support custom annotations that carry caching metadata.
create a custom annotationcacheoperationsource.
sets managed resource to expose and stores its classloader.
register the configured notificationlistener notificationlisteners with the mbeanserver.
set the autodetection mode to use by name. to one of the autodetect_ constants or is null
replace any bean names used as keys in the notificationlistener mappings with their corresponding objectname values. with the mbeanserver
notifies all registered mbeanexporterlistener mbeanexporterlisteners of the registration of the mbean identified by the supplied objectname.
set the autodetection mode to use. one of the autodetect_ constants
register an individual bean with the #setserver mbeanserver. this method is responsible for deciding how a bean should be exposed to the mbeanserver. specifically, if the supplied mapvalue is the name of a bean that is configured for lazy initialization, then a proxy to the resource is registered with the mbeanserver so that the lazy load behavior is honored. if the bean is already an mbean then it will be registered directly with the mbeanserver without any intervention. for all other beans or bean names, the resource itself is registered with the mbeanserver directly. may be either the string name of a bean, or the bean itself
performs the actual autodetection process, delegating to an given bean. whether to include a bean or not
notifies all registered mbeanexporterlistener mbeanexporterlisteners of the unregistration of the mbean identified by the supplied objectname.
build an adapted mbean for the given bean instance, if possible. the default implementation builds a jmx 1.2 standardmbean for the target's mbeanmxbean interface in case of an aop proxy, delegating the interface's management operations to the proxy.
register beans that are configured for lazy initialization with the with the mbeanserver
if the supplied managed resource implements the notificationpublisheraware an instance of
set the notificationlistener notificationlisteners to register with the javax.management.mbeanserver. the key of each entry in the map is a string representation of the javax.management.objectname or the bean name of the mbean the listener should be registered for. specifying an asterisk ( ) for a key will cause the listener to be associated with all mbeans registered by this class at startup time. the value of each entry is the advanced options such as registering handback objects see #setnotificationlisteners(notificationlistenerbean[]).
register the defined beans with the mbeanserver. each bean is exposed to the mbeanserver via a the modelmbeanprovider interface that is configured. by default the requiredmodelmbean class that is supplied with all jmx implementations is used. the management interface produced for each bean is dependent on the implementation of the objectnamingstrategy interface being used.
indicates whether or not a particular bean name is present in the excluded beans list.
creates an mbean that is configured with the appropriate management interface for the supplied managed resource.
replaces the notification source if necessary to do so. from the notification javadoc: "it is strongly recommended that notification senders use the object name rather than a reference to the mbean object as the source."
send the supplied notification using the wrapped
reads mbeanparameterinfo from the managedoperationparameter attributes attached to a method. returns an empty array of mbeanparameterinfo if no attributes are found.
creates a description for the attribute corresponding to this property descriptor. attempts to create the description using metadata from either the getter or setter attributes, otherwise uses the property name.
throws an illegalargumentexception if it encounters a jdk dynamic proxy. metadata can only be read from target classes and cglib proxies!
adds descriptor fields from the managedresource attribute to the mbean descriptor. specifically, adds the currencytimelimit, and persistname descriptor fields if they are present in the metadata.
reads the managednotification metadata from the class of the managed resource and generates and returns the corresponding modelmbeannotificationinfo metadata.
retrieves the description for the supplied method from the metadata. uses the method name is no description is present in the metadata.
reads managed resource description from the source level metadata. returns an empty string if no description can be found.
resolve the given class names into class objects.
checks to see if the given method is declared in a managed interface for the given bean.
set the array of interfaces to use for creating the management info. these interfaces will be used for a bean if no entry corresponding to that bean is found in the interfacemappings property. each entry must be an interface.
resolve the given interface mappings, turning class names into class objects.
determine whether the given method is supposed to be included, that is, not configured as to be ignored. of the mbeanexporter
set the mappings of bean keys to a comma-separated list of method names. these method names are ignored when creating the management interface. the property key must match the bean key and the property value must match the list of method names. when searching for method names to ignore for a bean, spring will check these mappings first.
set the mappings of bean keys to a comma-separated list of method names. the property key should match the bean key and the property value should match the list of method names. when searching for method names for a bean, spring will check these mappings first.
create parameter info for the given method. the default implementation returns an empty array of mbeanparameterinfo. of the mbeanexporter
creates an instance of modelmbeanoperationinfo for the given method. populates the parameter info for the operation. not used by the default implementation but possibly by subclasses of the mbeanexporter
set the currencytimelimit field to the specified "defaultcurrencytimelimit", if any (by default none).
iterate through all methods on the mbean class and gives subclasses the chance to vote on their inclusion. if a particular method corresponds to the accessor or mutator of an attribute that is included in the management interface, then the corresponding operation is exposed with the "role" descriptor field set to the appropriate value. of the mbeanexporter
iterate through all properties on the mbean class and gives subclasses the chance to vote on the inclusion of both the accessor and mutator. if a particular accessor or mutator is voted for inclusion, the appropriate metadata is assembled and passed to the subclass for descriptor population. of the mbeanexporter
apply the given jmx "currencytimelimit" value to the given descriptor. the default implementation sets a value >0 as-is (as number of cache seconds), turns a value of 0 into integer.max_value ("always cache") and sets the "defaultcurrencytimelimit" (if any, indicating "never cache") in case of a value <0. this follows the recommendation in the jmx 1.2 specification.
create an instance of the modelmbeaninfosupport class supplied with all jmx implementations and populates the metadata through calls to the subclass.
reads the objectname from the source-level metadata associated with the managed resource's class.
returns an instance of objectname based on the identity of the managed resource.
merges the properties configured in the mappings and used for objectname resolution.
convert the supplied managednotification into the corresponding
return the java 6 mxbean interface exists for the given class, if any (that is, an interface whose name ends with "mxbean" andor carries an appropriate mxbean annotation).
create a string[] representing the argument signature of a method. each element in the array is the fully qualified class name of the corresponding argument in the methods signature.
return the jmx attribute name to use for the given javabeans property. when using strict casing, a javabean property with a getter method such as getfoo() translates to an attribute called would translate to just foo.
return the standard mbean interface for the given class, if any (that is, an interface whose name matches the class name of the given class but with suffix "mbean").
convert an array of mbeanparameterinfo into an array of
attempt to find a locally running mbeanserver. fails if no if this parameter is null, all registered mbeanservers are considered. if the empty string is given, the platform mbeanserver will be returned.
append an additional keyvalue pair to an existing objectname with the key being the static value identity and the value being the identity hash code of the managed resource being exposed on the supplied objectname. this can be used to provide a unique objectname for each distinct instance of a particular bean or class. useful when generating objectname objectnames at runtime for a set of managed resources based on the template value supplied by a
return the list of javax.management.objectname string representations for which the encapsulated #getnotificationfilter() notificationfilter will be registered as a listener for javax.management.notification notifications.
creates a jmxconnector for the given settings and exposes the associated mbeanserverconnection.
creates lazy proxies for the jmxconnector and mbeanserverconnection.
creates the mbeanserver instance.
start the connector server. if the threaded flag is set to true, the jmxconnectorserver will be started in a separate thread. if the daemon flag is set to true, that thread will be started as a daemon thread. with the mbeanserver
unregisters all beans that have been registered by an instance of this class.
actually unregister the specified mbean from the server.
return the objectname objectnames of all registered beans.
actually register the mbean with the server. the behavior when encountering an existing mbean can be configured using #setregistrationpolicy.
retrieve the objectname instance corresponding to the supplied name.
unregisters the specified notificationlistener.
registers the specified notificationlistener. ensures that an mbeanserverconnection is configured and attempts to detect a local connection if one is not supplied.
closes any jmxconnector that may be managed by this interceptor.
connects to the remote mbeanserver using the configured jmxserviceurl: to the specified jmx service, or to a local mbeanserver if no service url specified.
prepares the mbeanserverconnection if the "connectonstartup" is turned on (which it is by default).
refresh the connection and retry the mbean invocation if possible. if not configured to refresh on connect failure, this method simply rethrows the original exception. if it failed as well
routes a method invocation (not a property getset) to the corresponding operation on the managed resource.
convert the given result object (from attribute access or operation invocation) to the specified target class for returning from the proxy method. is necessary
ensures that an mbeanserverconnection is configured and attempts to detect a local connection if one is not supplied.
loads the management interface info for the configured mbean into the caches. this information is used by the proxy when determining whether an invocation matches a valid operation or attribute on the management interface of the managed resource.
route the invocation to the configured managed resource. correctly routes javabean property access to mbeanserverconnection.getsetattribute and method invocation to
checks that the proxyinterface has been specified and then generates the proxy for the target mbean.
add both keyed and non-keyed entries for the supplied field to the supplied field list.
concatenate the given elements, delimiting each with null elements altogether.
build the code list for the given code and field: an objectfield-specific code, a field-specific code, a plain error code. arrays, lists and maps are resolved both for specific elements and the whole collection. see the defaultmessagecodesresolver class level javadoc for details on the generated codes.
actually set the nested path. delegated to by setnestedpath and pushnestedpath.
check whether the given fielderror matches the given field.
transform the given field into its full path, regarding the nested path of this instance.
unwrap the source behind this error: possibly an exception (typically org.springframework.beans.propertyaccessexception) or a bean validation javax.validation.constraintviolation. the cause of the outermost exception will be introspected as well, e.g. the underlying conversion exception or exception thrown from a setter (instead of having to unwrap the propertyaccessexception in turn). (i.e. none specified or not available anymore after deserialization)
preserve the source behind this error: possibly an exception (typically org.springframework.beans.propertyaccessexception) or a bean validation javax.validation.constraintviolation. note that any such source object is being stored as transient: that is, it won't be part of a serialized error representation.
find the bindingresult for the given name in the given model.
find a required bindingresult for the given name in the given model.
this implementation exposes a propertyeditor adapter for a formatter, if applicable.
formats the field value based on registered propertyeditors.
invoke the given validator smartvalidator for the supplied object and arguments is null, or if the supplied validator does not
reject the given field with the given error code, error arguments and default message if the value is empty or just contains whitespace. an 'empty' value in this context means either null, the empty string "", or consisting wholly of whitespace. the object whose field is being validated does not need to be passed in because the errors instance can resolve field values by itself (it will usually hold an internal reference to the target object). (can be null)
reject the given field with the given error code, error arguments and default message if the value is empty. an 'empty' value in this context means either null or the empty string "". the object whose field is being validated does not need to be passed in because the errors instance can resolve field values by itself (it will usually hold an internal reference to the target object). (can be null)
check the given property values against the required fields, generating missing field errors where appropriate.
add a custom formatter, applying it to the specified field types only, if any, or otherwise to all fields matching the formatter-declared type. registers a corresponding propertyeditor adapter underneath the covers. field type if field types are explicitly specified as parameters) derived from the given formatter implementation class
invoke the specified validators, if any.
add a custom formatter for the field type specified in formatter class, applying it to the specified fields only, if any, or otherwise to all fields. registers a corresponding propertyeditor adapter underneath the covers.
close this databinder, which may result in throwing a bindexception if it encountered any errors.
create the abstractpropertybindingresult instance using direct field access.
return this binder's underlying simpletypeconverter.
check the given property values against the allowed fields, removing values for fields that are not allowed.
create the abstractpropertybindingresult instance using standard javabean property access.
apply given property values to the target object. default implementation applies all of the supplied property values as bean property values. by default, unknown fields will be ignored.
invoke the specified validators, if any, with the given validation hints. note: validation hints may get ignored by the actual target validator.
register fields that are required for each binding process. if one of the specified fields is not contained in the list of incoming property values, a corresponding "missing field" error will be created, with error code "required" (by the default binding error processor).
bind the given property values to this binder's target. this call can create field errors, representing basic binding errors like a required field (code "required"), or type mismatch between value and bean property (code "typemismatch"). note that the given propertyvalues should be a throwaway instance: for efficiency, it will be modified to just contain allowed fields if it implements the mutablepropertyvalues interface; else, an internal mutable copy will be created for this purpose. pass in a copy of the propertyvalues if you want your original instance to stay unmodified in any case.
return a model map for the obtained state, exposing an errors instance as ' #model_key_prefix model_key_prefix + objectname' and the object itself. note that the map is constructed every time you're calling this method. adding things to the map and then re-calling this method will not work. the attributes in the model map returned by this method are usually included in the modelandview for a form view that uses spring's bind tag, which needs access to the errors instance.
this default implementation determines the type based on the actual field value, if any. subclasses should override this to determine the type from a descriptor, even for null values.
this implementation delegates to the editor lookup facility, if available.
create a new beanwrapper for the underlying target object.
create a new directfieldaccessor for the underlying target object.
validate the supplied value for the specified field on the target type, reporting the same validation errors as if the value would be bound to the field on an instance of the target class.
determine the validation groups to validate against for the given method invocation. default are the validation groups as specified in the validated annotation on the containing target class of the method.
determine a field for the given constraint violation. the default implementation returns the stringified property path.
turn the specified validation hints into jsr-303 validation groups.
return fielderror arguments for a validation error on the given field. invoked for each violated constraint. the default implementation returns a first argument indicating the field name (see #getresolvablefield). afterwards, it adds all actual constraint annotation attributes (i.e. excluding "message", "groups" and "payload") in alphabetical order of their attribute names. can be overridden to e.g. add further attributes from the constraint descriptor.
extract the rejected value behind the given constraint violation, for exposure through the spring errors representation. which contains the current field's value
process the given jsr-303 constraintviolations, adding corresponding errors to the provided spring errors object.
perform validation of the given bean.
set the jsr-303 validator to delegate to for validating methods. default is the default validatorfactory's default validator.
look up the jndi object and store it.
look up the object with the given name in the current jndi context. jndi implementations returns null, a namingexception gets thrown) name bound to jndi
execute the given jndi context callback implementation.
release a jndi context as obtained from #getcontext().
look up the object with the given name in the current jndi context. superclass of the actual class, or null for any match. for example, if the value is object.class, this method will succeed whatever the class of the returned instance. jndi implementations returns null, a namingexception gets thrown) name bound to jndi
create a new jndi initial context. invoked by #getcontext. the default implementation use this template's environment settings. can be subclassed for custom contexts, e.g. for testing.
rebind the given object to the current jndi context, using the given name. overwrites any existing binding.
bind the given object to the current jndi context, using the given name.
remove the binding for the given name from the current jndi context.
this implementation looks up and returns the value associated with the given name from the underlying jndilocatordelegate. if a namingexception is thrown during the call to jndilocatordelegate#lookup(string), returns
check whether a default jndi environment, as in a java ee environment, is available on this jvm.
configure a jndilocatordelegate with its "resourceref" property set to
convert the given jndi name into the actual jndi name to use. the default implementation applies the "java:compenv" prefix if "resourceref" is "true" and no other scheme (e.g. "java:") is given.
perform an actual jndi lookup for the given name via the jnditemplate. if the name doesn't begin with "java:compenv", this prefix is added if "resourceref" is set to "true".
remove the given ejb instance. to be invoked by concrete remote slsb invoker subclasses.
refresh the ejb home object and retry the given invocation. called by invoke on connect failure.
invokes the create() method on the cached ejb home object.
determine the create method of the given ejb home object.
this implementation "creates" a new ejb instance for each invocation. can be overridden for custom invocation strategies. alternatively, override #getsessionbeaninstance and for example to hold a single shared ejb component instance.
remove the given ejb instance.
this implementation "creates" a new ejb instance for each invocation. can be overridden for custom invocation strategies. alternatively, override #getsessionbeaninstance and for example to hold a single shared ejb instance.
add date converters to the specified registry.
install the converters into the converter registry.
get the datetimeformatter with the this context's settings applied to the base formatter. formatting rules, generally context-independent
associate the given datetimecontext with the current thread. or null to reset the thread-bound context
obtain a datetimeformatter with user-specific settings applied to the given base formatter. (generally user independent)
factory method used to create a datetimeformatter.
create a new datetimeformatter using this factory. if no specific pattern or style has been defined, the supplied fallbackformatter will be used. when no specific factory properties have been set
factory method used to create a datetimeformatter.
install the converters into the converter registry.
associate the given jodatimecontext with the current thread. or null to reset the thread-bound context
obtain a datetimeformatter with user-specific settings applied to the given base formatter. (generally user independent)
get the datetimeformatter with the this context's settings applied to the base formatter. formatting rules, generally context-independent
create a new datetimeformatter using this factory. if no specific pattern or style has been defined, the supplied fallbackformatter will be used. when no specific factory properties have been set
add formatters appropriate for most environments: including number formatters, jsr-354 money  currency formatters, jsr-310 date-time andor joda-time formatters, depending on the presence of the corresponding api on the classpath.
loads and parses the groovy script via the groovyclassloader.
create a new groovyscriptfactory for the given script source, specifying a strategy interface that can customize groovy's compilation process within the underlying groovyclassloader. interpreted by the post-processor that actually creates the script. groovyclassloader compiler configuration
instantiate the given groovy script class and run it if necessary. or the result of running the script instance)
evaluate the specified beanshell script based on the given script source, returning the class defined by the script. the script may either declare a full class or return an actual instance of the scripted object (in which case the class of the object will be returned). in any other case, the returned class will be null.
evaluate the specified beanshell script based on the given script source, keeping a returned script class or script object as-is. the script may either be a simple script that needs a corresponding proxy generated (implementing the specified interfaces), or declare a full class or return an actual instance of the scripted object (in which case the specified interfaces, if any, need to be implemented by that classinstance). supposed to implement (may be null or empty if the script itself declares a full class or returns an actual instance of the scripted object)
create a new beanshell-scripted object from the given script source. the script may either be a simple script that needs a corresponding proxy generated (implementing the specified interfaces), or declare a full class or return an actual instance of the scripted object (in which case the specified interfaces, if any, need to be implemented by that classinstance). supposed to implement (may be null or empty if the script itself declares a full class or returns an actual instance of the scripted object)
load and parse the beanshell script via bshscriptutils.
register a scriptfactorypostprocessor bean definition in the supplied already been registered.
parses the dynamic object element and returns the resulting bean definition. registers a scriptfactorypostprocessor if needed.
resolves the script source from either the ' script-source' attribute or the ' inline-script' element. logs and xmlreadercontext#error and returns null if neither or both of these values are specified.
create a bean definition for the scripted object, based on the given script definition, extracting the definition data that is relevant for the scripted object (that is, everything but bean class and constructor arguments).
prepare the script beans in the internal beanfactory that this post-processor uses. each original bean definition will be split into a scriptfactory definition and a scripted object definition.
create a config interface for the given bean definition, defining setter methods for the defined property values as well as an init method and a destroy method (if defined). this implementation creates the interface via cglib's interfacemaker, determining the property types from the given interfaces (as far as possible). config interface for getters corresponding to the setters we're supposed to generate)
create a refreshable proxy for the given aop targetsource. indicate proxying of all interfaces implemented by the target class)
convert the given script source locator to a scriptsource instance. by default, supported locators are spring resource locations (such as "file:c:myscript.bsh" or "classpath:mypackagemyscript.bsh") and inline scripts ("inline:myscripttext...").
get the refresh check delay for the given scriptfactory beandefinition. if the beandefinition has a under the key #refresh_check_delay_attribute which is a valid number type, then this value is used. otherwise, the #defaultrefreshcheckdelay value is used.
create a scriptfactory bean definition based on the given script definition, extracting only the definition data that is relevant for the scriptfactory (that is, only bean class and constructor arguments).
set the globally scoped bindings on the underlying script engine manager, shared by all scripts, as an alternative to script argument bindings.
obtain the jsr-223 scriptengine to use for the given script.
retrieve the current last-modified timestamp of the underlying resource.
load and parse the script via jsr-223's scriptengine.
retrieve a scriptengine from the given scriptenginemanager by name, delegating to scriptenginemanager#getenginebyname but throwing a descriptive exception if not found or if initialization failed.
set a fresh script string, overriding the previous script.
initialize this service exporter, binding the specified service to jndi.
locate or create the rmi registry.
locate or create the rmi registry.
unexport the rmi registry on bean factory shutdown, provided that this bean actually created a registry.
locate or create the rmi registry. no implicit creation of a rmi registry will happen)
exposes the exporter's service interface, if any, as target interface.
wrap the given arbitrary exception that happened during remote access in either a remoteexception or a spring remoteaccessexception (if the method signature does not support remoteexception). only call this for remote access exceptions, not for exceptions thrown by the target service itself! remoteaccessexception or remoteexception remoteexception
determine whether the given rmi exception indicates a connect failure. treats rmi's connectexception, connectioexception, unknownhostexception, nosuchobjectexception and stubnotfoundexception as connect failure.
perform a raw method invocation on the given rmi stub, letting reflection exceptions through as-is.
convert the given remoteexception that happened during remote access to spring's remoteaccessexception if the method signature does not support remoteexception. else, return the original remoteexception. a connect failure
determine the object to export: either the service object itself or a rmiinvocationwrapper in case of a non-rmi service object.
create the rmi stub, typically by looking it up. called on interceptor initialization if "cachestub" is "true"; else called for each invocation by #getstub(). the default implementation retrieves the service from the jndi environment. this can be overridden in subclasses.
fetches an rmi stub and delegates to #doinvoke. if configured to refresh on connect failure, it will call
perform the given invocation on the given rmi stub.
apply the given aop method invocation to the given rmiinvocationhandler. the default implementation delegates to #createremoteinvocation.
perform the actual writing of the given invocation result object to the given objectoutputstream. the default implementation simply calls can be overridden for serialization of a custom wrapper object rather than the plain invocation, for example an encryption-aware holder.
perform the actual reading of an invocation result object from the given objectinputstream. the default implementation simply calls can be overridden for deserialization of a custom wrapper object rather than the plain invocation, for example an encryption-aware holder. being found in the local classloader
apply the given aop method invocation to the given rmiinvocationhandler. the default implementation delegates to #createremoteinvocation.
perform the given invocation on the given rmi stub.
create the rmi stub, typically by looking it up. called on interceptor initialization if "cachestub" is "true"; else called for each invocation by #getstub(). the default implementation looks up the service url via
initialize this service exporter, registering the service as rmi object. creates an rmi registry on the specified port if none exists.
create a new remoteinvocation for the given aop method invocation.
perform this invocation on the given target object. typically called when a remoteinvocation is received on the server.
add an additional invocation attribute. useful to add additional invocation context without having to subclass remoteinvocation. attribute keys have to be unique, and no overriding of existing attributes is allowed. the implementation avoids to unnecessarily create the attributes map, to minimize serialization size.
fill the current client-side stack trace into the given exception. the given exception is typically thrown on the server and serialized as-is, with the client wanting it to contain the client-side portion of the stack trace as well. what we can do here is to update the trace, provided that we run on jdk 1.4+.
check whether a service reference has been set, and whether it matches the specified service.
get a proxy for the given service object, implementing the specified service interface. used to export a proxy that does not expose any internals but just a specific interface intended for remote access. furthermore, a
apply the given remote invocation to the given target object, wrapping the invocation result in a serializable remoteinvocationresult object. the default implementation creates a plain remoteinvocationresult. can be overridden in subclasses for custom invocation behavior, for example to return additional context information. note that this is not covered by the remoteinvocationexecutor strategy!
return the candidate types that are associated with the specified stereotype. or an empty set if none has been found for the specified basepackage
load and instantiate the candidatecomponentsindex from @value #components_resource_location, using the given class loader. if no index is available, return null. be loaded or if an error occurs while creating candidatecomponentsindex
return the expression for the specified spel value parse the expression if it hasn't been already.
access the given target object by resolving the given property name against the given target environment.
set the application event class to publish. the event class must have a constructor with a single will pass in the invoked object. if it does not expose a constructor that takes a single object argument
add additional details such as the bean type and method signature to the given error message.
invoke the event listener method with the given argument values.
process the specified applicationevent, checking if the condition match and handling non-null result, if any.
assert that the target bean class is an instance of the class where the given method is declared. in some cases the actual bean instance at event- processing time may be a jdk dynamic proxy (lazy initialization, prototype beans, and others). event listener beans that require proxying should prefer class-based proxy mechanisms.
resolve the method arguments to use for the specified applicationevent. these arguments will be used to invoke the method handled by this instance. can return null to indicate that no suitable arguments could be resolved and therefore the method should not be invoked at all for the specified event.
invoke the given listener with the given event.
actually process the event, after having filtered according to the desired event source already. the default implementation invokes the specified delegate, if any.
create a sourcefilteringlistener for the given event source. only processing events from this source from the specified source
return a collection of applicationlisteners matching the given event type. non-matching listeners get excluded early. non-matching listeners early, based on cached matching information.
filter a listener early through checking its generically declared event type before trying to instantiate it. if this method returns true for a given listener as a first pass, the listener instance will get retrieved and fully evaluated through a for the given event type
determine whether the given listener supports the given event. the default implementation detects the smartapplicationlistener and genericapplicationlistener interfaces. in case of a standard will be used to introspect the generically declared type of the target listener. for the given event type
actually retrieve the application listeners for the given event and source type.
specify if the condition defined by the specified expression matches.
return the locale associated with the given user context, if any, or the system default locale otherwise. this is effectively a replacement for java.util.locale#getdefault(), able to optionally respect a user-level locale setting. specific locale has been associated with the current thread
return the localecontext associated with the current thread, if any.
associate the given timezone with the current thread, preserving any locale that may have been set already. will implicitly create a localecontext for the given locale. the time zone part of the thread-bound context for child threads (using an inheritablethreadlocal)
associate the given localecontext with the current thread. the given localecontext may be a timezoneawarelocalecontext, containing a locale with associated time zone information. or null to reset the thread-bound context for child threads (using an inheritablethreadlocal)
associate the given locale with the current thread, preserving any timezone that may have been set already. will implicitly create a localecontext for the given locale. the locale part of thread-bound context for child threads (using an inheritablethreadlocal)
retrieve all applicable lifecycle beans: all singletons that have already been created, as well as all smartlifecycle beans (even if they are marked as lazy-init).
stop the specified bean as part of the given set of lifecycle beans, making sure that any beans that depends on it are stopped first.
start the specified bean as part of the given set of lifecycle beans, making sure that any beans that it depends on are started first.
load the properties from the given resource.
resolves the given message code as key in the retrieved bundle files, using a cached messageformat instance per message code.
get a propertiesholder for the given filename, either from the cache or freshly loaded.
calculate all filenames for the given bundle basename and locale. will calculate filenames for the given locale, the system locale (if applicable), and the default file.
refresh the propertiesholder for the given bundle filename. the holder can be null if not cached before, or a timed-out cache entry (potentially getting re-validated against the current last-modified timestamp).
calculate the filenames for the given bundle basename and locale, appending language code, country code, and variant code. e.g.: basename "messages", locale "de_at_oo" -> "messages_de_at_oo", "messages_de_at", "messages_de". follows the rules defined by java.util.locale#tostring().
resolves the given message code as key in the retrieved bundle files, returning the value found in the bundle as-is (without messageformat parsing).
get a propertiesholder that contains the actually visible properties for a locale, after merging all specified resource bundles. either fetches the holder from the cache or freshly loads it. only used when caching resource bundle contents forever, i.e. with cacheseconds  0. therefore, merged properties are always cached forever.
build a default string representation for this messagesourceresolvable: including codes, arguments, and default message.
create a new classpathxmlapplicationcontext with the given parent, loading the definitions from the given xml files and automatically refreshing the context.
format the given message string, using cached messageformats. by default invoked for passed-in default messages, to resolve any argument placeholders found in them. the message, or null if none
return a messageformat for the given bundle and code, fetching already generated messageformats from the cache. defined for the given code
efficiently retrieve the string value for the specified key, or return null if not found. as of 4.2, the default implementation checks containskey before it attempts to call getstring (which would require catching missingresourceexception for key not found). can be overridden in subclasses.
return a resourcebundle for the given basename and code, fetching already generated messageformats from the cache. found for the given basename and locale
implemented for compatibility with
visit each bean definition in the given bean factory and attempt to replace $... property placeholders with values from the given properties.
processing occurs by replacing $... placeholders in bean definitions by resolving each against this configurer's set of propertysources, which includes:  all org.springframework.core.env.configurableenvironment#getpropertysources environment property sources, if an environment #setenvironment is present  #mergeproperties merged local properties, if #setlocation any any property sources set by calling #setpropertysources  if #setpropertysources is called, environment and local properties will be ignored. this method is designed to give the user fine-grained control over property sources, and once set, the configurer makes no assumptions about adding additional sources.
load bean definitions from the given groovy scripts or xml files. note that ".xml" files will be parsed as xml content; all other kinds of resources will be parsed as groovy scripts. loading each specified resource name
initialize the lifecycleprocessor. uses defaultlifecycleprocessor if none defined in the context.
initialize the messagesource. use parent's if none defined in this context.
return the internal applicationeventmulticaster used by the context.
instantiate and invoke all registered beanfactorypostprocessor beans, respecting explicit order if given. must be called before singleton instantiation.
return the internal messagesource used by the context.
return the internal lifecycleprocessor used by the context.
finish the initialization of this context's bean factory, initializing all remaining singleton beans.
configure the factory's standard context characteristics, such as the context's classloader and post-processors.
publish the given event to all listeners. or a payload object to be turned into a payloadapplicationevent)
register a shutdown hook with the jvm runtime, closing this context on jvm shutdown unless it has already been closed at that time. delegates to doclose() for the actual closing procedure.
set the parent of this application context. the parent applicationcontext#getenvironment() environment is this (child) application context environment if the parent is non- null and its environment is an instance of configurableenvironment.
finish the refresh of this context, invoking the lifecycleprocessor's onrefresh() method and publishing the
initialize the applicationeventmulticaster. uses simpleapplicationeventmulticaster if none defined in the context.
return information about this context.
prepare this context for refreshing, setting its startup date and active flag as well as performing any initialization of property sources.
actually performs context closing: publishes a contextclosedevent and destroys the singletons in the bean factory of this application context. called by both close() and a jvm shutdown hook, if any.
assert that this context's beanfactory is currently active, throwing an illegalstateexception if it isn't. invoked by all beanfactory delegation methods that depend on an active context, i.e. in particular all bean accessor methods. the default implementation checks the #isactive() 'active' status of this context overall. may be overridden for more specific checks, or for a no-op if #getbeanfactory() itself throws an exception in such a case.
register the given beanpostprocessor beans.
invoke the given beanfactorypostprocessor beans.
invoke the given beandefinitionregistrypostprocessor beans.
register a bean from the given bean class, using the given supplier for obtaining a new instance (typically declared as a lambda expression or method reference), optionally customizing its bean definition metadata (again typically declared as a lambda expression or method reference). this method can be overridden to adapt the registration mechanism for all registerbean methods (since they all delegate to this one). of null, resolving a public constructor to be autowired instead)
do nothing: we hold a single internal beanfactory and rely on callers to register beans through our public methods (or the beanfactory's).
set the config locations for this application context. if not set, the implementation may use a default as appropriate.
load bean definitions from the given xml resources. loading each specified resource name
get a default message for the given messagesourceresolvable. this implementation fully renders the default message if available, or just returns the plain default message string if the primary message code is being used as a default message.
subclasses can override this method to resolve a message without arguments in an optimized fashion, i.e. to resolve without involving a messageformat. the default implementation does use messageformat, through delegating to the #resolvecode method. subclasses are encouraged to replace this with optimized resolution. unfortunately, java.text.messageformat is not implemented in an efficient fashion. in particular, it does not detect that a message pattern doesn't contain argument placeholders in the first place. therefore, it is advisable to circumvent messageformat for messages without arguments. (subclasses are encouraged to support internationalization)
try to retrieve the given message from the parent messagesource, if any. within the message
searches through the given array of objects, finds any messagesourceresolvable objects and resolves them. allows for messages to have messagesourceresolvables as arguments.
resolve the given code and arguments as message in the given locale, returning null if not found. does not fall back to the code as default message. invoked by getmessage methods. within the message
loads the bean definitions via an xmlbeandefinitionreader.
load the bean definitions with the given xmlbeandefinitionreader. the lifecycle of the bean factory is handled by the #refreshbeanfactory method; hence this method is just supposed to load andor register bean definitions.
resolve resource paths as file system paths. note: even if a given path starts with a slash, it will get interpreted as relative to the current vm working directory. this is consistent with the semantics in a servlet container.
return a messagesourceaccessor for the application context used by this object, for easy message access.
return the applicationcontext that this object is associated with.
register a prototype bean with the underlying bean factory. for more advanced needs, register with the underlying beanfactory directly.
register a singleton bean with the underlying bean factory. for more advanced needs, register with the underlying beanfactory directly.
register a prototype bean with the underlying bean factory. for more advanced needs, register with the underlying beanfactory directly.
create a new staticapplicationcontext with the given parent.
register a singleton bean with the underlying bean factory. for more advanced needs, register with the underlying beanfactory directly.
this implementation performs an actual refresh of this context's underlying bean factory, shutting down the previous bean factory (if any) and initializing a fresh bean factory for the next phase of the context's lifecycle.
customize the internal bean factory used by this context. called for each #refresh() attempt. the default implementation applies this context's and #setallowcircularreferences "allowcircularreferences" settings, if specified. can be overridden in subclasses to customize any of
find all applicable applicationcontexts for the current application. called if no specific applicationcontext has been set for this livebeansview.
actually generate a json snapshot of the beans in the given applicationcontexts. this implementation doesn't use any json parsing libraries in order to avoid third-party library dependencies. it produces an array of context description objects, each containing a context and parent attribute as well as a beans attribute with nested bean description objects. each bean object contains a bean, scope, type and resource attribute, as well as a dependencies attribute with a nested array of bean names that the present bean depends on.
determine a resource description for the given bean definition and apply basic json escaping (backslashes, double quotes) to it.
register member (nested) classes that happen to be configuration classes themselves.
factory method to obtain a sourceclass from a class.
factory method to obtain sourceclass sourceclasss from class names.
factory method to obtain a sourceclass from a class name.
recursively collect all declared @import values. unlike most meta-annotations it is valid to have several @imports declared with different values; the usual process of returning values from the first meta-annotation on a class is not sufficient. for example, it is common for a @configuration class to declare direct annotation.
register default methods on interfaces implemented by the configuration class.
retrieve the metadata for all @bean methods.
handle the specified deferredimportselector. if deferred import selectors are being collected, this registers this instance to the list. if they are being processed, the deferredimportselector is also processed immediately according to its deferredimportselector.group.
given a stack containing (in order)  com.acme.foo com.acme.bar com.acme.baz  return "[foo->bar->baz]".
factory method to obtain a sourceclass from a configurationclass.
create a new configurationclassparser instance that will be used to populate the set of configuration classes.
apply processing and build a complete configurationclass by reading the annotations, members and methods from the source class. this method can be called multiple times as relevant sources are discovered.
process the given @propertysource annotation metadata.
validate each configurationclass object.
returns @import class, considering all meta-annotations.
create a new annotationconfigapplicationcontext with the given defaultlistablebeanfactory.
this implementation resolves the type of annotation from generic metadata and validates that (a) the annotation is in fact present on the importing the #selectimports(advicemode) method is then invoked, allowing the concrete implementation to choose imports in a safe and convenient fashion. on the importing @configuration class or if #selectimports(advicemode) returns null
register all relevant annotation post processors in the given registry. that this registration was triggered from. may be null. that have actually been registered by this call
derive further bean definitions from the configuration classes in the registry.
post-processes a beanfactory in search of configuration class beandefinitions; any candidates are then enhanced by a configurationclassenhancer. candidate status is determined by beandefinition attribute metadata.
prepare the configuration classes for servicing bean requests at runtime by replacing them with cglib-enhanced subclasses.
build and validate a configuration model based on the registry of
create a new conditionevaluator instance.
determine if an item should be skipped based on @conditional annotations.
check whether the given bean definition is a candidate for a configuration class (or a nested component class declared within a configurationcomponent class, to be auto-registered as well), and mark it accordingly.
determine the order for the given configuration class metadata. or ordered.lowest_precedence if none declared
check the given metadata for a lite configuration class candidate (e.g. a class annotated with @component or just having configuration class, just registering it and scanning it for @bean methods
get the environment from the given registry if possible, otherwise return a new standardenvironment.
perform a scan within the specified base packages, returning the registered bean definitions. this method does not register an annotation config processor but rather leaves this up to the caller.
apply further settings to the given bean definition, beyond the contents retrieved from scanning the component class.
check the given candidate's bean name, determining whether the corresponding bean definition needs to be registered or conflicts with an existing definition. existing, compatible bean definition for the specified name bean definition has been found for the specified name
determine whether the given new bean definition is compatible with the given existing bean definition. the default implementation considers them as compatible when the existing bean definition comes from the same source or from a non-scanning source. explicitly defined one or a previously generated one from scanning new definition to be skipped in favor of the existing definition
register, escalate, and configure the aspectj auto proxy creator based on the value of the @ enableaspectjautoproxy#proxytargetclass() attribute on the importing
determine whether the given class does not match any exclude filter and does match at least one include filter.
register the default filter for component @component. this will implicitly register all annotations that have the also supports java ee 6's javax.annotation.managedbean and jsr-330's javax.inject.named annotations, if available.
determine whether the given class is a candidate component based on any
determine whether the given bean definition qualifies as candidate. the default implementation checks whether the class is not an interface and not dependent on an enclosing class. can be overridden in subclasses.
set the resourceloader to use for resource locations. this will typically be a resourcepatternresolver implementation. default is a pathmatchingresourcepatternresolver, also capable of resource pattern resolving through the resourcepatternresolver interface.
return the metadatareaderfactory used by this component provider.
determine if the specified include typefilter is supported by the index.
check whether the given annotation is a stereotype that is allowed to suggest a component name through its annotation value().
derive a bean name from one of the annotations on the class.
derive a default bean name from the given bean definition. the default implementation simply builds a decapitalized version of the short class name: e.g. "mypackage.myjdbcdao" -> "myjdbcdao". note that inner classes will thus have names of the form "outerclassname.innerclassname", which because of the period in the name may be an issue if you are autowiring by name.
read a particular configurationclass, registering bean definitions for the class itself and all of its bean methods.
register the configuration class itself as a bean definition.
read configurationmodel, registering bean definitions with the registry based on its contents.
read the given beanmethod, registering bean definitions with the beandefinitionregistry based on its contents.
create a new configurationclassbeandefinitionreader instance that will be used to populate the given beandefinitionregistry.
create a new configurationclass with the given name.
create a new configurationclass representing a class that was imported using the import annotation or automatically processed as a nested configuration class (if imported is true).
create a new configurationclass with the given name.
create a new configurationclass with the given name.
create a new configurationclass representing a class that was imported using the import annotation or automatically processed as a nested configuration class (if importedby is not null).
create a new scannedgenericbeandefinition for the class that the given metadatareader describes.
register, escalate, and configure the standard auto proxy creator (apc) against the given registry. works by finding the nearest annotation declared on the importing attributes. if mode is set to proxy, the apc is registered; if subclass (cglib) proxying. several @enable annotations expose both mode and capabilities end up sharing a aopconfigutils#auto_proxy_creator_bean_name single apc. for this reason, this implementation doesn't "care" exactly which annotation it finds -- as long as it exposes the right mode and the same.
create a new annotatedbeandefinitionreader for the given registry and using the given environment. in the form of a beandefinitionregistry profiles.
register a bean from the given bean class, deriving its metadata from class-declared annotations. (may be null) in addition to qualifiers at the bean class level factory's beandefinition, e.g. setting a lazy-init or primary flag
get the environment from the given registry if possible, otherwise return a new standardenvironment.
build a dependencydescriptor for the underlying fieldmethod.
obtain a lazily resolving resource proxy for the given name and type, delegating to #getresource on demand once a method call comes in.
obtain a resource object for the given name and type through autowiring based on the given factory.
obtain the resource object for the given name and type.
uses enhancer to generate a subclass of superclass, ensuring that callbacks are registered for the new subclass.
check whether the given method corresponds to the container's currently invoked factory method. compares method name and parameter types only in order to work around a potential problem with covariant return types (currently only known to happen on groovy classes).
enhance a bean @bean method to check the supplied beanfactory for the existence of this bean object. super implementation of the proxied method i.e., the actual @bean method
loads the specified class and generates a cglib subclass of it equipped with container-aware callbacks capable of respecting scoping and other bean semantics.
create a subclass proxy that intercepts calls to getobject(), delegating to the current beanfactory instead of creating a new instance. these proxies are created only when calling a factorybean from within a bean method, allowing for proper scoping semantics even when working against the factorybean instance directly. if a factorybean instance is fetched through the container via  it will not be proxied. this too is aligned with the way xml configuration works.
creates a new cglib enhancer instance.
invoke beanclassloaderaware, beanfactoryaware, if implemented by the given object.
enable aspectj weaving with the given loadtimeweaver.
copy all attributes in the supplied map into this map, with existing objects of the same name taking precedence (i.e. not getting replaced).
copy all attributes in the supplied map into this map, with existing objects of the same name taking precedence (i.e. not getting replaced).
initialize the messagesource of the given theme with the one from the corresponding parent of this themesource.
create a messagesource for the given basename, to be used as messagesource for the corresponding theme. default implementation creates a resourcebundlemessagesource. for the given basename. a subclass could create a specifically configured reloadableresourcebundlemessagesource, for example.
this implementation returns a simpletheme instance, holding a resourcebundle-based messagesource whose basename corresponds to the given theme name (prefixed by the configured "basenameprefix"). simpletheme instances are cached per theme name. use a reloadable messagesource if themes should reflect changes to the underlying files.
initialize the themesource for the given application context, autodetecting a bean with the name "themesource". if no such bean is found, a default (empty) themesource will be used.
return the detail message, including the message from the linked exception if there is one.
this implementation delegates to the createtopicconnection(username, password) method of the target topicconnectionfactory, passing in the specified user credentials. if the specified username is empty, it will simply delegate to the standard
this implementation delegates to the createconnection(username, password) method of the target connectionfactory, passing in the specified user credentials. if the specified username is empty, it will simply delegate to the standard
this implementation delegates to the createqueueconnection(username, password) method of the target queueconnectionfactory, passing in the specified user credentials. if the specified username is empty, it will simply delegate to the standard
prepare the given connection before it is exposed. the default implementation applies exceptionlistener and client id. can be overridden in subclasses.
create a default session for this connectionfactory, adapting to jms 1.0.2 style queuetopic mode if necessary. ( session.transacted or one of the common modes)
wrap the given connection with a proxy that delegates every method call to it but suppresses close calls. this is useful for allowing application code to handle a special framework connection just like an ordinary connection from a jms connectionfactory.
close the given connection.
initialize the underlying shared connection. closes and reinitializes the connection if an underlying connection is present already.
exception listener callback that renews the underlying single connection.
make sure a connection or connectionfactory has been set.
release the given connection, stopping it (if necessary) and eventually closing it. checks smartconnectionfactory#shouldstop, if available. this is essentially a more sophisticated version of (if this is null, the call will be ignored) (may be null)
obtain a jms session that is synchronized with the current transaction, if any. (used as transactionsynchronizationmanager key) jms resources started in order to allow for receiving messages. note that a reused connection may already have been started before, even if this flag is false.
obtain a jms session that is synchronized with the current transaction, if any. (may be null) that is synchronized with a spring-managed transaction (where the main transaction might be a jdbc-based one for a specific datasource, for example), with the jms transaction committing right after the main transaction. if not allowed, the given connectionfactory needs to handle transaction enlistment underneath the covers.
obtain a jms topicsession that is synchronized with the current transaction, if any. mainly intended for use with the jms 1.0.2 api. (may be null) that is synchronized with a spring-managed transaction (where the main transaction might be a jdbc-based one for a specific datasource, for example), with the jms transaction committing right after the main transaction. if not allowed, the given connectionfactory needs to handle transaction enlistment underneath the covers.
obtain a jms queuesession that is synchronized with the current transaction, if any. mainly intended for use with the jms 1.0.2 api. (may be null) that is synchronized with a spring-managed transaction (where the main transaction might be a jdbc-based one for a specific datasource, for example), with the jms transaction committing right after the main transaction. if not allowed, the given connectionfactory needs to handle transaction enlistment underneath the covers.
determine whether the given jms session is transactional, that is, bound to the current thread by spring's transaction facilities.
make sure the connectionfactory has been set.
wrap the given connection with a proxy that delegates every method call to it but handles session lookup in a transaction-aware fashion.
resets the session cache as well.
wrap the given session with a proxy that delegates every method call to it but adapts close calls. this is useful for allowing application code to handle a special framework session just like an ordinary session.
checks for a cached session for the given mode.
return the default response destination, if any.
set the beanfactory to use to resolve expressions (may be null).
register a new jmslistenerendpoint alongside the the factory may be null if the default factory has to be used for that endpoint.
return a description for this endpoint. available to subclasses, for inclusion in their tostring() result.
create and start a new container using the specified factory.
create a message listener container for the given jmslistenerendpoint. this create the necessary infrastructure to honor that endpoint with regards to its configuration. the startimmediately flag determines if the container should be started immediately.
start the specified messagelistenercontainer if it should be started on startup or when start is called explicitly after startup.
build a descriptive exception message for the given jmsexception, incorporating a linked exception's message if appropriate.
close the given jms queuebrowser and ignore any thrown exception. this is useful for typical finally blocks in manual jms code.
close the given jms queuerequestor and ignore any thrown exception. this is useful for typical finally blocks in manual jms code.
convert the specified checked javax.jms.jmsexception jmsexception to a spring runtime org.springframework.jms.jmsexception jmsexception equivalent.
close the given jms session and ignore any thrown exception. this is useful for typical finally blocks in manual jms code.
commit the session if not within a jta transaction.
close the given jms messageproducer and ignore any thrown exception. this is useful for typical finally blocks in manual jms code.
close the given jms connection and ignore any thrown exception. this is useful for typical finally blocks in manual jms code.
close the given jms messageconsumer and ignore any thrown exception. this is useful for typical finally blocks in manual jms code.
rollback the session if not within a jta transaction.
unmarshal the given textmessage into an object.
template method that allows for custom message unmarshalling. invoked when #frommessage(message) is invoked with a message that is not a textmessage or bytesmessage. the default implementation throws an illegalargumentexception.
construct a new marshallingmessageconverter with the given marshaller set. if the given marshaller also implements the unmarshaller interface, it is used for both marshalling and unmarshalling. otherwise, an exception is thrown. note that all marshaller implementations in spring also implement the
this implementation unmarshals the given message into an object.
this implementation marshals the given object to a javax.jms.textmessage or the #settargettype "marshalto" property.
unmarshal the given bytesmessage into an object.
marshal the given object to a textmessage.
template method that allows for custom message marshalling. invoked when #settargettype is not messagetype#text or the default implementation throws an illegalargumentexception.
marshal the given object to a bytesmessage.
map the given object to a bytesmessage.
determine a jackson serialization view based on the given conversion hint. converter for the current conversion attempt
template method that allows for custom message mapping. invoked when #settargettype is not messagetype#text or the default implementation throws an illegalargumentexception.
determine a jackson javatype for the given jms message, typically parsing a type id message property. the default implementation parses the configured type id property name and consults the configured type id mapping. this can be overridden with a different strategy, e.g. doing some heuristics based on message origin.
convert a textmessage to a java object with the specified type.
template method that allows for custom message mapping. invoked when #settargettype is not messagetype#text or the default implementation throws an illegalargumentexception.
convert a bytesmessage to a java object with the specified type.
map the given object to a textmessage.
set a type id for the given payload object on the given jms message. the default implementation consults the configured type id mapping and sets the resulting value (either a mapped id or the raw java class name) into the configured type id message property.
specify mappings from type ids to java classes, if desired. this allows for synthetic ids in the type id message property, instead of transferring java class names. default is no custom mappings, i.e. transferring raw java class names.
this implementation creates a textmessage for a string, a bytesmessage for a byte array, a mapmessage for a map, and an objectmessage for a serializable object.
extract a map from the given mapmessage.
extract a byte array from the given bytesmessage.
create a jms mapmessage for the given map.
create a jms bytesmessage for the given byte array.
validate the given destination object, checking whether it matches the expected type.
actually receive a message from the given consumer. a no-wait receive; 0 indicates an indefinite wait attempt)
send the given remoteinvocationresult as a jms message to the originator.
create the invocation result response message. the default implementation creates a jms objectmessage for the given remoteinvocationresult object. it sets the response's correlation id to the request message's correlation id, if any; otherwise to the request message id.
callback that is invoked by #readremoteinvocation when it encounters an invalid request message. the default implementation either discards the invalid message or throws a messageformatexception - according to the "ignoreinvalidrequests" flag, which is set to "true" (that is, discard invalid messages) by default. to lead to an exception (instead of ignoring it)
execute the given remote invocation, sending an invoker request message to this accessor's target queue and waiting for a corresponding response.
actually execute the given request, sending the invoker request message to the specified target queue and waiting for a corresponding response. the default implementation is based on standard jms sendreceive, using a javax.jms.temporaryqueue for receiving the response.
create a new jms connection for this jms invoker.
resolve this accessor's target queue.
send the given jms message.
a variant of #execute(sessioncallback, boolean) that explicitly creates a non-transactional session. the given sessioncallback does not participate in an existing transaction.
execute the action specified by the given action object within a jms session. generalized version of execute(sessioncallback), allowing the jms connection to be started on the fly. use execute(sessioncallback) for the general case. starting the jms connection is just necessary for receiving messages, which is preferably achieved through the receive methods.
actually receive a jms message.
create a jms messageproducer for the given session and destination, configuring it to disable message ids andor timestamps (if necessary). delegates to #docreateproducer for creation of the raw jms messageproducer.
send a request message to the given destination and block until a reply has been received on a temporary queue created on-the-fly. return the response message or null if no message has
actually send the given jms message.
create a jms messageproducer for the given session and destination, configuring it to disable message ids andor timestamps (if necessary). delegates to #docreateproducer for creation of the raw jms messageproducer.
create a jms messageconsumer for the given session and destination. this implementation uses jms 1.1 api.
set the connectionfactory to use for the underlying jmstemplate.
actually execute the listener for a message received from the given consumer, fetching all requires resources and invoking the listener.
this implementation checks whether the session is externally synchronized. in this case, the session is not locally transacted, despite the listener container's "sessiontransacted" flag being set to "true".
perform a rollback, handling rollback exceptions properly.
process a message received from the provider. executes the listener, exposing the current jms session as thread-bound resource (if "exposelistenersession" is "true").
specify concurrency limits via a "lower-upper" string, e.g. "5-10", or a simple upper limit string, e.g. "10". this listener container will always hold on to the maximum number of consumers #setconcurrentconsumers since it is unable to scale. this property is primarily supported for configuration compatibility with generally use #setconcurrentconsumers instead.
registers this listener container as jms exceptionlistener on the shared connection.
initialize the jms sessions and messageconsumers for this container.
create a messageconsumer for the given jms session, registering a messagelistener for the specified listener.
execute the specified listener, committing or rolling back the transaction afterwards (if necessary).
invoke the specified listener: either as standard jms messagelistener or (preferably) as spring sessionawaremessagelistener.
perform a rollback, if appropriate.
invoke the registered errorhandler, if any. log at warn level otherwise.
determine the default subscription name for the given message listener.
perform a commit or message acknowledgement, as appropriate.
check the given message listener, throwing an exception if it does not correspond to a supported listener type. by default, only a standard jms messagelistener object or a spring sessionawaremessagelistener object will be accepted.
perform a rollback, handling rollback exceptions properly.
create a jms messageconsumer for the given session and destination. this implementation uses jms 1.1 api.
invoke the registered jms exceptionlistener, if any.
invoke the specified listener as spring sessionawaremessagelistener, exposing a new jms session (potentially with its own transaction) to the listener if demanded.
prepare the given connection, which is about to be registered as shared connection for this container. the default implementation sets the specified client id, if any. subclasses can override this to apply further settings.
return the shared jms connection maintained by this container. available after initialization. shared connection, or if the connection hasn't been initialized yet
try to resume all paused tasks. tasks for which rescheduling failed simply remain in paused mode.
destroy the registered jms sessions and associated messageconsumers.
create a default taskexecutor. called if no explicit taskexecutor has been specified. the default implementation builds a org.springframework.core.task.simpleasynctaskexecutor with the specified bean name (or the class name, if no bean name specified) as thread name prefix.
apply the next back-off time using the specified backoffexecution. return true if the back-off period has been applied and a new attempt to recover should be made, false if no further attempt should be made.
specify the level of caching that this listener container is allowed to apply, in the form of the name of the corresponding constant: e.g. "cache_connection".
handle the given exception that arose during setup of a listener. called for every such exception in every concurrent listener. the default implementation logs the exception at warn level if not recovered yet, and at debug level if already recovered. can be overridden in subclasses. already recovered from the present listener setup failure (this usually indicates a follow-up failure than can be ignored other than for debug log purposes)
determine whether this listener container currently has more than one idle instance among its scheduled invokers.
refresh the underlying connection, not returning before an attempt has been successful. called in case of a shared connection as well as without shared connection, so either needs to operate on the shared connection or on a temporary connection that just gets established for validation purposes. the default implementation retries until it successfully established a connection, for as long as this message listener container is running. applies the specified recovery interval between retries.
specify concurrency limits via a "lower-upper" string, e.g. "5-10", or a simple upper limit string, e.g. "10" (the lower limit will be 1 in this case). this listener container will always hold on to the minimum number of consumers ( #setconcurrentconsumers) and will slowly scale up to the maximum number of consumers #setmaxconcurrentconsumers in case of increasing load.
stop this listener container, invoking the specific callback once all listener processing has actually stopped. note: further stop(runnable) calls (before processing has actually stopped) will override the specified callback. only the latest specified callback will be invoked. if a subsequent #start() call restarts the listener container before it has fully stopped, the callback will not get invoked at all. has fully stopped
schedule a new invoker, increasing the total number of scheduled invokers for this listener container.
resolve the destination to use for this instance. the destinationresolver and session can be used to resolve a destination at runtime.
extract the message body from the given jms message. as an argument
determine a response destination for the given message. the default implementation first checks the jms reply-to it is returned; if it is null, then the configured is returned; if this too is null, then an
handle the given result object returned from the listener method, sending a response message back.
post-process the given response message before it will be sent. the default implementation sets the response's correlation id to the request message's correlation id, if any; otherwise to the request message id.
build a jms message to be sent as response based on the given result object.
send the given response message to the given destination.
invoke the specified listener method.
invoke the handler, wrapping any exception to a listenerexecutionfailedexception with a dedicated error message.
set the destinationresolver to use for resolving destination names into the jca 1.5 activationspec "destination" property. if not specified, destination names will simply be passed in as strings. if specified, destination names will be resolved into destination objects first. note that a destinationresolver is usually specified on the jmsactivationspecfactory (see standardjmsactivationspecfactory#setdestinationresolver). this is simply a shortcut for parameterizing the default jmsactivationspecfactory; it will replace any custom jmsactivationspecfactory that might have been set before.
apply the specified acknowledge mode to the activationspec object. this implementation applies the standard jca 1.5 acknowledge modes "auto-acknowledge" and "dups-ok-acknowledge". it throws an exception in case of client_acknowledge or session_transacted having been requested. (according to the constants in javax.jms.session
populate the given applicationspec object with the settings defined in the given configuration object. this implementation applies all standard jms settings, but ignores "maxconcurrency" and "prefetchsize" - not supported in standard jca 1.5.
this implementation guesses the activationspec class name from the provider's class name: e.g. "activemqresourceadapter" -> "activemqactivationspec" in the same package, or a class named "activationspecimpl" in the same package as the resourceadapter class.
this implementation supports spring's extended "maxconcurrency" and "prefetchsize" settings through detecting corresponding activationspec properties: "maxsessions""maxnumberofworks" and "maxmessagespersessions""maxmessages", respectively (following activemq's and joram's naming conventions).
this implementation maps session_transacted onto an activationspec property named "useramanagedtransaction", if available (following activemq's naming conventions).
specify concurrency limits via a "lower-upper" string, e.g. "5-10", or a simple upper limit string, e.g. "10". jca listener containers will always scale from zero to the given upper limit. a specified lower limit will effectively be ignored. this property is primarily supported for configuration compatibility with for this activation config, generally use #setmaxconcurrency instead.
making a beanfactory available is optional; if not set,
process the given jmslistener annotation on the given method, registering a corresponding endpoint for the given bean instance.
copy constructor which allows for ignoring certain entries. used for serialization without non-serializable entries.
constructor providing control over the id and timestamp header values.
a constructor for creating new message headers. this constructor is protected. see factory methods in this and sub-classes.
register a callback to execute on destruction of the specified attribute. the callback is executed when the session is closed.
extract the simp session attributes from the given message and wrap them in a simpattributes instance.
bind the given simpattributes to the current thread.
return the simpattributes currently bound to the thread or raise an
configure the prefix to use for destinations targeting a specific user. the default value is "user".
creates a new map and puts the given headers under the key effectively treats the input header map as headers to be sent out to the destination. however if the given headers already contain the key returned without changes. also if the given headers were prepared and obtained with instance is also returned without changes.
install or remove an executorchannelinterceptor that invokes a completion task once the message is handled. which an interceptor is either added or removed.
configure the value for the heart-beat settings. the first number represents how often the server will write or send a heartbeat. the second is how often the client should write. 0 means no heartbeats. by default this is set to "0, 0" unless the #settaskscheduler taskscheduler in which case the default becomes "10000,10000" (in milliseconds).
configure the org.springframework.scheduling.taskscheduler to use for providing heartbeat support. setting this property also sets the by default this is not set.
create a simplebrokermessagehandler instance with the given message channels and destination prefixes.
return the passcode header value, or null if not set.
get the heartbeat header.
set the given, single header value under the given name.
return the first header value for the given header name, if any.
add the given, single header value under the given name.
set the heartbeat header. applies to the connect and connected frames.
clean up state associated with the connection and close it. any exception arising from closing the connection are propagated.
invoked after the stomp connected frame is received. at this point the connection is ready for sending stomp messages to the broker.
after a disconnect there should be no more client frames so we can close the connection pro-actively. however, if the disconnect has a receipt header we leave the connection open and expect the server will respond with a receipt and then close the connection. stomp specification 1.2 disconnect
forward the given message to the stomp broker. the method checks whether we have an active tcp connection and have received the stomp connected frame. for client messages this should be false only if we lose the tcp connection around the same time when a client message is being forwarded, so we simply log the ignored message at debug level. for messages from within the application being sent on the "system" connection an exception is raised so that components sending the message have a chance to handle it -- by default the broker message channel is synchronous. note that if messages arrive concurrently around the same time a tcp connection is lost, there is a brief period of time before the connection is reset when one or more messages may sneak through and an attempt made to forward them. rather than synchronizing to guard against that, this method simply lets them try and fail. for client sessions that may result in an additional stomp error frame(s) being sent downstream but code handling that downstream should be idempotent in such cases.
calculate the current buffer size.
decodes one or more stomp frames from the given bytebuffer into a list of message messages. if there was enough data to parse a "content-length" header, then the value is used to determine how much more data is needed before a new attempt to decode is made. if there was not enough data to parse the "content-length", or if there is "content-length" header, every subsequent call to decode attempts to parse again with all available data. therefore the presence of a "content-length" header helps to optimize the decoding of large messages.
an overloaded version of #connect(stompsessionhandler) that accepts headers to use for the stomp connect frame.
factory method for create and configure a new session.
configure the default value for the "heart-beat" header of the stomp connect frame. the first number represents how often the client will write or send a heart-beat. the second is how often the server should write. a value of 0 means no heart-beats. by default this is set to "10000,10000" but subclasses may override that default and for example set it to "0,0" if they require a taskscheduler to be configured first. http:stomp.github.iostomp-specification-1.2.html#heart-beating
further initialize the stompheaders, for example setting the heart-beat header if necessary.
decode a single stomp frame from the given buffer into a message.
see stomp spec 1.2:  href="http:stomp.github.iostomp-specification-1.2.html#value_encoding">"value encoding".
try to read an eol incrementing the buffer position if successful.
decodes one or more stomp frames from the given buffer and returns a list of message messages. if the given bytebuffer contains only partial stomp frame content and no complete stomp frames, an empty list is returned, and the buffer is reset to to where it was. if the buffer contains one ore more stomp frames, those are returned and the buffer reset to point to the beginning of the unused partial content. the output partialmessageheaders map is used to store successfully parsed headers in case of partial content. the caller can then check if a "content-length" header was read, which helps to determine how much more content is needed before the next attempt to decode. successfully parsed partialmessageheaders in case of partial message content in cases where the partial buffer ended with a partial stomp frame
see stomp spec 1.2:  href="http:stomp.github.iostomp-specification-1.2.html#value_encoding">"value encoding".
encodes the given payload and headers into a byte[].
create a new session.
constructor to create dto from the local user session.
constructor to create dto from a local user subscription.
default constructor for json deserialization.
default constructor for json deserialization.
constructor to create user from a local user.
constructor to create dto from a local user registry.
create an instance with the given client and broker channels subscribing to handle messages from each and then sending any resolved messages to the broker channel.
configure the thread pool backing this message channel using a custom threadpooltaskexecutor.
an accessor for the messagebrokerregistry that ensures its one-time creation and initialization through #configuremessagebroker(messagebrokerregistry).
return a org.springframework.validation.validator org.springframework.validation.validators instance for validating in order, this method tries to get a validator instance:  delegating to getvalidator() first if none returned, getting an existing instance with its well-known name "mvcvalidator", created by an mvc configuration if none returned, checking the classpath for the presence of a jsr-303 implementation before creating a optionalvalidatorfactorybean returning a no-op validator instance 
create a new taskexecutorregistration for a default
enable a simple message broker and configure one or more prefixes to filter destinations targeting the broker (e.g. destinations prefixed with "topic").
enable a stomp broker relay and configure the destination prefixes supported by the message broker. check the stomp documentation of the message broker for supported destinations.
resolve placeholder values in the given array of destinations.
create an instance of simpannotationmethodmessagehandler with the given message channels and broker messaging template.
a variant of #reactornettytcpclient(string, int, reactornettycodec) that still manages the lifecycle of the tcpclient and underlying resources, but allows for direct configuration of other properties of the client through a function tcpclient>.
simple constructor with the host and port to use to connect to. this constructor manages the lifecycle of the tcpclient and underlying resources such as connectionprovider, for full control over the initialization and lifecycle of the tcpclient, use #reactornettytcpclient(tcpclient, reactornettycodec).
return all values for the specified native header. or null if none.
add the specified native header value to existing values.
a protected constructor to create new headers.
set the specified native header value replacing existing values.
return the first value for the specified native header, or null if none.
a protected constructor accepting the headers of an existing message to copy.
copy the name-value pairs from the provided map. this operation will not overwrite any existing values.
removes all headers provided via array of 'headerpatterns'. as the name suggests, array may contain simple matching patterns for header names. supported pattern styles are: "xxx", "xxx", "xxx" and "xxxyyy".
set the value for the given header name. if the provided value is null, the header will be removed.
copy the name-value pairs from the provided map. this operation will overwrite any existing values. use
a variation of #getaccessor(org.springframework.messaging.message, class) with a messageheaders instance instead of a message. this is for cases when a full message may not have been created yet.
return a mutable messageheaderaccessor for the given message attempting to match the type of accessor used to create the message headers, or otherwise wrapping the message with a messageheaderaccessor instance. this is for cases where a header needs to be updated in generic code while preserving the accessor type for downstream processing.
return the header value, or null if it does not exist or does not match the requested type.
generate the name to use to set the header defined by the specified
generate the name to use to set the header defined by the specified
a shortcut factory method for creating a message with the given payload and messageheaders. note: the given messageheaders instance is used directly in the new message, i.e. it is not copied.
returns the default content type for the payload. called when without a content type header. by default, this returns the first element of the #getsupportedmimetypes() supportedmimetypes, if any. can be overridden in sub-classes.
determine the json encoding to use for the given content type.
determine a jackson serialization view based on the given conversion hint. converter for the current conversion attempt
determine whether to log the given exception coming from a (typically a jsonmappingexception)
create an instance from a bean instance, method name, and parameter types.
assert that the target bean class is an instance of the class where the given method is declared. in some cases the actual endpoint instance at request- processing time may be a jdk dynamic proxy (lazy initialization, prototype beans, and others). endpoint classes that require proxying should prefer class-based proxy mechanisms.
if the provided instance contains a bean name rather than an object instance, the bean name is resolved before a handlermethod is created and returned.
create an instance from a bean name, a method, and a beanfactory. the method #createwithresolvedbean() may be used later to re-create the handlermethod with an initialized bean.
returns a new instance with url patterns from the current instance ("this") and the "other" instance as follows:  if there are patterns in both instances, combine the patterns in "this" with the patterns in "other" using org.springframework.util.pathmatcher#combine(string, string). if only one instance has patterns, use them. if neither instance has patterns, use an empty string (i.e. ""). 
compare the two conditions based on the destination patterns they contain. patterns are compared one at a time, from top to bottom via if all compared patterns match equally, but one instance has more patterns, it is considered a closer match. it is assumed that both instances have been obtained via that match the request and are sorted with the best matches on top.
check if any of the patterns match the given message destination and return an instance that is guaranteed to contain matching patterns, sorted via or a new condition with sorted matching patterns; or null either if a destination can not be extracted or there is no match
when this property is configured only messages to destinations matching one of the configured prefixes are eligible for handling. when there is a match the prefix is removed and only the remaining part of the destination is used for method-mapping purposes. by default, no prefixes are configured in which case all messages are eligible for handling.
register a handler method and its unique mapping. under the same mapping
check whether the given destination (of an incoming message) matches to one of the configured destination prefixes and if so return the remaining portion of the destination after the matched prefix. if there are no matching prefixes, return null. if there are no destination prefixes, return the destination as is.
detect if the given handler has any methods that can handle messages and if so register it with the extracted mapping information.
create a handlermethod instance from an object handler that is either a handler instance or a string-based bean name.
find an @messageexceptionhandler method for the given exception. the default implementation searches methods in the class hierarchy of the handlermethod first and if not found, it continues searching for additional messagingadvicebean, if any.
iterate over registered and invoke the one that supports it.
find a registered handlermethodargumentresolver that supports the given method parameter.
invoke the handler method with the given argument values.
get the method argument values for the current message, checking the provided argument values and falling back to the configured argument resolvers. the resulting array will be passed into #doinvoke.
return the method mapped to the given exception type, or null if none.
find a method to handle the given exception. use exceptiondepthcomparator if more than one match is found.
extract the exceptions this method handles.this implementation looks for sub-classes of throwable in the method signature. the method is static to ensure safe use from sub-class constructors.
validate the payload if applicable. the default implementation checks for @javax.validation.valid, spring's org.springframework.validation.annotation.validated, and custom annotations whose name starts with "valid".
create a new namedvalueinfo based on the given namedvalueinfo with sanitized values.
resolve the given annotation-specified value, potentially containing placeholders and expressions.
constructor with a conversionservice and a beanfactory. target method parameter type and #... spel expressions in default values, or null if default values are not expected to contain expressions
a null results in a false value for booleans or an exception for other primitives.
convert from the given message to the given target class.
convert the given object to serialized form, possibly using a headers and apply the given post processor.
retrieve the spring-managed session for the current thread, if any.
create a new springsessioncontext for the given hibernate sessionfactory.
return the current hibernate entity interceptor, or null if none. resolves an entity interceptor bean name via the bean factory, if necessary.
determine whether the given session is (still) physically connected to the database, that is, holds an active jdbc connection internally.
perform actual closing of the hibernate session, catching and logging any cleanup exceptions thrown.
convert the given hibernateexception to an appropriate exception from the org.springframework.dao hierarchy.
determine the datasource of the given sessionfactory.
trigger a flush on the given hibernate session, converting regular
create a new configurablejtaplatform instance with the given jta transactionmanager and optionally a given usertransaction.
set the spring jtatransactionmanager or the jta transactionmanager to be used with hibernate, if any. allows for using a spring-managed transaction manager for hibernate 5's session and cache synchronization, with the "hibernate.transaction.jta.platform" automatically set to it. a passed-in spring jtatransactionmanager needs to contain a jta case where we'll automatically set websphereextendedjtaplatform accordingly. note: if this is set, the hibernate settings should not contain a jta platform setting to avoid meaningless double configuration.
perform spring-based scanning for entity classes, registering them as annotated classes with this configuration.
check whether any of the configured entity type filters matches the current class descriptor contained in the metadata reader.
build the hibernate sessionfactory through background bootstrapping, using the given executor for a parallel initialization phase (e.g. a org.springframework.core.task.simpleasynctaskexecutor).  sessionfactory initialization will then switch into background bootstrap mode, with a sessionfactory proxy immediately returned for injection purposes instead of waiting for hibernate's bootstrapping to complete. however, note that the first actual call to a sessionfactory method will then block until hibernate's bootstrapping completed, if not ready by then. for maximum benefit, make sure to avoid early sessionfactory calls in init methods of related beans, even for metadata introspection purposes.
determine the spring resourceloader to use for hibernate metadata.
determine the hibernate metadatasources to use. can also be externally called to initialize and pre-populate a metadatasources instance which is then going to be used for sessionfactory building.
return the hibernate configuration object used to build the sessionfactory. allows for access to configuration metadata stored there (rarely needed).
return the hibernate properties, if any. mainly available for configuration through property paths that specify individual keys.
enable the specified filters on the given session.
create a close-suppressing proxy for the given hibernate session. the proxy also prepares returned query and criteria objects.
check whether write operations are allowed on the given session. default implementation throws an invaliddataaccessapiusageexception in case of flushmode.manual. can be overridden in subclasses.
prepare the given query object, applying cache settings andor a transaction timeout.
apply the given name parameter to the given query object.
prepare the given criteria object, applying cache settings andor a transaction timeout.
disable the specified filters on the given session.
execute the action specified by the given action object within a session. hibernate session to callback code
conveniently obtain the current hibernate session.
open a session for the sessionfactory that this interceptor uses. the default implementation delegates to the sessionfactory#opensession method and sets the session's flush mode to "manual".
unbind the hibernate session from the thread and close it).
open a new hibernate session according and bind it to the thread via the
look up the sessionfactory that this filter should use. the default implementation looks for a bean with the specified name in spring's root application context.
open a session for the sessionfactory that this filter uses. the default implementation delegates to the sessionfactory#opensession method and sets the session's flush mode to "manual".
open a session for the given sessionfactory. the default implementation delegates to the sessionfactory#opensession method and sets the session's flush mode to "manual".
determine the persistenceunitinfo to use for the entitymanagerfactory created by this bean. the default implementation reads in all persistence unit infos from if no entity manager name was specified, it takes the first info in the array as returned by the reader. otherwise, it checks for a matching name.
enlist this application-managed entitymanager in the current transaction.
create a container-managed extended entitymanager proxy. if this implements the entitymanagerfactoryinfo interface, the corresponding jpadialect and persistenceunitinfo will be detected accordingly. call (may be null) transactions (according to the jpa 2.1 synchronizationtype rules) management but may opt out of automatic transaction synchronization
actually create the entitymanager proxy. interface to proxy, or null for default detection of all interfaces (or null if not known in advance) or application-managed entitymanager semantics transactions (according to the jpa 2.1 synchronizationtype rules)
join an existing transaction, if not already joined. (i.e. whether failure to join is considered fatal)
actually create the entitymanager proxy. and persistenceunitinfo from or application-managed entitymanager semantics transactions (according to the jpa 2.1 synchronizationtype rules)
prepare a transaction on the given entitymanager, if possible. (to be passed into cleanuptransaction)
obtain a jpa entitymanager from the given factory. is aware of a corresponding entitymanager bound to the current thread, e.g. when using jpatransactionmanager. same as getentitymanager, but throwing the original persistenceexception. call (may be null) transactions (according to the jpa 2.1 synchronizationtype rules)
close the given jpa entitymanager, catching and logging any cleanup exceptions thrown.
apply the current transaction timeout, if any, to the given jpa query object. this method sets the jpa 2.0 query hint "javax.persistence.query.timeout" accordingly.
find an entitymanagerfactory with the given name in the given spring application context (represented as listablebeanfactory). the specified unit name will be matched against the configured persistence unit, provided that a discovered entitymanagerfactory implements the entitymanagerfactoryinfo interface. if not, the persistence unit name will be matched against the spring bean name, assuming that the entitymanagerfactory bean names follow that convention. if no unit name has been given, this method will search for a default entitymanagerfactory through listablebeanfactory#getbean(class). in which case a single bean of type entitymanagerfactory will be searched for)
convert the given runtime exception to an appropriate exception from the return null if no translation is appropriate: any other exception may have resulted from user code, and should not be translated. the most important cases like object not found or optimistic locking failure are covered here. for more fine-granular conversion, jpatransactionmanager etc support sophisticated translation of exceptions via a jpadialect. or null if the exception should not be translated
obtain a jpa entitymanager from the given factory. is aware of a corresponding entitymanager bound to the current thread, e.g. when using jpatransactionmanager. note: will return null if no thread-bound entitymanager found! call (may be null)
prepare a transaction on the given entitymanager, if possible. (as returned by preparetransaction)
close the current transaction's entitymanager. called after a transaction begin attempt failed.
create a jpa entitymanager to be used for a transaction. the default implementation checks whether the entitymanagerfactory is a spring proxy and unwraps it first.
retrieves an entitymanagerfactory by persistence unit name, if none set explicitly. falls back to a default entitymanagerfactory bean if no persistence unit specified.
eagerly initialize the jpa dialect, creating a default one for the specified entitymanagerfactory if none set. auto-detect the entitymanagerfactory's datasource, if any.
initialize the entitymanagerfactory for the given configuration.
delegate an incoming invocation from the proxy, dispatching to entitymanagerfactoryinfo or the native entitymanagerfactory accordingly.
create a proxy for the given entitymanagerfactory. we do this to be able to return a transaction-aware proxy for an application-managed entitymanager. if initialized already
close the entitymanagerfactory on bean factory shutdown.
implementation of the persistenceexceptiontranslator interface, as autodetected by spring's persistenceexceptiontranslationpostprocessor. uses the dialect's conversion if possible; otherwise falls back to standard jpa exception conversion.
this implementation invokes the standard jpa transaction.begin method. throws an invalidisolationlevelexception if a non-default isolation level is set. this implementation does not return any transaction data object, since there is no state to be kept for a standard jpa transaction. hence, subclasses do not have to care about the return value ( null) of this implementation and are free to return their own transaction data object.
obtain a new entitymanager from this accessor's entitymanagerfactory. can be overridden in subclasses to create specific entitymanager variants.
retrieves an entitymanagerfactory by persistence unit name, if none set explicitly. falls back to a default entitymanagerfactory bean if no persistence unit specified.
create a transactional entitymanager proxy for the given entitymanagerfactory. transactions (according to the jpa 2.1 synchronizationtype rules)
create a transactional entitymanager proxy for the given entitymanagerfactory. transactions (according to the jpa 2.1 synchronizationtype rules) entitymanager. allows the addition or specification of proprietary interfaces.
check whether any of the configured entity type filters matches the current class descriptor contained in the metadata reader.
prepare the persistenceunitinfos according to the configuration of this manager: scanning for persistence.xml files, parsing all matching files, configuring and post-processing them. persistenceunitinfos cannot be obtained before this preparation method has been invoked.
read all persistence unit infos from persistence.xml, as defined in the jpa specification.
determine jpa's default "meta-inform.xml" resource for use with spring's default persistence unit, if any. checks whether a "meta-inform.xml" file exists in the classpath and uses it if it is not co-located with a "meta-infpersistence.xml" file.
perform spring-based scanning for entity classes.
try to determine the persistence unit root url based on the given "defaultpersistenceunitrootlocation".
hook method allowing subclasses to customize each persistenceunitinfo. the default implementation delegates to all registered persistenceunitpostprocessors. it is usually preferable to register further entity classes, jar files etc there rather than in a subclass of this manager, to be able to reuse the post-processors. passed in as mutablepersistenceunitinfo.
this implementation delegates to the loadtimeweaver, if specified.
initialize this persistenceunitinfo with the loadtimeweaver spi interface used by spring to add instrumentation to the current class loader.
this implementation delegates to the loadtimeweaver, if specified.
parse the property xml elements.
parse the class xml elements.
parse the jar-file xml elements.
validate the given stream and return a valid dom document for parsing.
determine the persistence unit root url based on the given resource (which points to the persistence.xml file we're reading).
parse the mapping-file xml elements.
parse and build all persistence unit infos defined in the given xml files.
parse the unit info dom element.
parse the validated document and add entries to the given unit info list.
return a specified persistence context for the given unit name, as defined through the "persistencecontexts" (or "extendedpersistencecontexts") map.
find an entitymanagerfactory with the given name in the current spring application context, falling back to a single default entitymanagerfactory (if any) in case of no unit name specified.
find a single default entitymanagerfactory in the spring application context.
return a specified persistence unit for the given unit name, as defined through the "persistenceunits" map. or null if none found
look up the entitymanagerfactory that this filter should use. the default implementation looks for a bean with the specified name in spring's root application context.
convert the given hibernateexception to an appropriate exception from the org.springframework.dao hierarchy.
return a detailed description of this exception, including the expression string and position (if available) as well as the actual exception message.
insert any necessary cast and value call to convert from a boxed type to a primitive value.
called after the main expression evaluation method has been generated, this method will callback any registered fieldadders or clinitadders to add any extra information to the class representing the compiled expression.
insert any necessary numeric conversion bytecodes based upon what is on the stack and the desired target type.
insert the appropriate checkcast instruction for the supplied descriptor.
convert a type descriptor to the single character primitive descriptor.
determine the appropriate boxing instruction for a specific type (if it needs boxing) and insert the instruction into the supplied visitor.
register a clinitadder which will add code to the static initializer in the generated class to support the code produced by an ast nodes primary generatecode() method.
return if the supplied array type has a core component reference type.
determine the jvm descriptor for a specified class. unlike the other descriptors used in the compilation process, this is the one the jvm wants, so this one includes any necessary trailing semicolon (e.g. ljavalangstring; rather than ljavalangstring)
produce the correct bytecode to build an array. the opcode to use and the signature to pass along with the opcode can vary depending on the signature of the array type.
create the jvm signature descriptor for a method. this consists of the descriptors for the method parameters surrounded with parentheses, followed by the descriptor for the return type. note the descriptors here are jvm descriptors, unlike the other descriptor forms the compiler is using which do not include the trailing semicolon.
if the codeflow shows the last expression evaluated to java.lang.boolean then insert the necessary instructions to unbox that to a boolean primitive.
register a fieldadder which will add a new field to the generated class to support the code produced by an ast nodes primary generatecode() method.
for numbers, use the appropriate method on the number to convert it to the primitive type requested.
determine if the supplied descriptor is for a supported number type or boolean. the compilation process only (currently) supports certain number types. these are double, float, long and int.
create an array of descriptors from an array of classes.
deduce the descriptor for a type. descriptors are like jvm type names but missing the trailing ';' so for object the descriptor is "ljavalangobject" for int it is "i".
determine if the supplied descriptor is for a supported number. the compilation process only (currently) supports certain number types. these are double, float, long and int.
determine the appropriate boxing instruction for a specific type (if it needs boxing) and insert the instruction into the supplied visitor.
determine whether boxingunboxing can get from one type to the other. assumes at least one of the types is in boxed form (i.e. single char descriptor).
produce appropriate bytecode to store a stack item in an array. the instruction to use varies depending on whether the type is a primitive or reference type.
construct a new codeflow for the given class.
create the jvm signature descriptor for a constructor. this consists of the descriptors for the constructor parameters surrounded with parentheses, followed by the descriptor for the return type, which is always "v". note the descriptors here are jvm descriptors, unlike the other descriptor forms the compiler is using which do not include the trailing semicolon.
create the optimal instruction for loading a number on the stack.
determine whether the descriptor is for a primitive array (e.g. "[[i").
determine the appropriate t tag to use for the newarray bytecode.
produce a complete message including the prefix and with the inserts applied to the message.
attempt compilation of the supplied expression. a check is made to see if it is compilable before compilation proceeds. the check involves visiting all the nodes in the expression ast and ensuring enough state is known about them that bytecode can be generated for them. or null if compilation is not possible
load a compiled expression class. makes sure the classloaders aren't used too much because they anchor compiled classes in memory and prevent gc. if you have expressions continually recompiling over time then by replacing the classloader periodically at least some of the older variants can be garbage collected.
generate the class that encapsulates the compiled expression and define it. the generated class will be a subtype of compiledexpression. compilation during code generation
factory method for compiler instances. the returned spelcompiler will attach a class loader as the child of the given class loader and this child will be used to load compiled expressions.
used for consuming arguments for either a method or a constructor call.
eat an identifier, possibly qualified (meaning that it is dotted). todo andyc could create complete identifiers (a.b.c) here rather than a sequence of them? (a, b, c)
return the default evaluation context that will be used if none is supplied on an evaluation call.
compile the expression if it has been evaluated more than the threshold number of times to trigger compilation.
perform expression compilation. this will only succeed once exit descriptors for all nodes have been determined. if the compilation fails and has failed more than 100 times the expression is no longer considered suitable for compilation.
compute the arguments to the function, they are the children of this expression node.
execute a function represented as a java.lang.reflect.method.
determines the set of property resolvers that should be used to try and access a property on the specified target type. the resolvers are considered to be in an ordered list, however in the returned list any that are exact matches for the input target type (as opposed to 'general' resolvers that could work for any type) are placed at the start of the list. in addition, there are specific resolvers that exactly name the class in question and resolvers that name a specific class but it is a supertype of the class we have. these are put at the end of the specific resolvers set and will be tried after exactly matching accessors but before generic accessors.
attempt to read the named property from the current context object.
go through the list of registered constructor resolvers and see if any can find a constructor that takes the specified set of arguments.
create a new ordinary object and return it.
create an array and return it.
evaluates a compound expression. this involves evaluating each piece in turn and the return value from each piece is the active context object for the subsequent piece.
decode the accessexception, throwing a lightweight evaluation exception or, if the cause was a runtimeexception, throw the runtimeexception directly.
a method reference is compilable if it has been resolved to a reflectively accessible method and the child nodes (arguments to the method) are also compilable.
determines the set of property resolvers that should be used to try and access a property on the specified target type. the resolvers are considered to be in an ordered list, however in the returned list any that are exact matches for the input target type (as opposed to 'general' resolvers that could work for any type) are placed at the start of the list. in addition, there are specific resolvers that exactly name the class in question and resolvers that name a specific class but it is a supertype of the class we have. these are put at the end of the specific resolvers set and will be tried after exactly matching accessors but before generic accessors.
produce a readable representation for a given method name with specified arguments.
perform an equality check for the given operand values. this method is not just used for reflective comparisons in subclasses but also from compiled expression code, which is why it needs to be declared as public static here.
return an object that indicates whether the input descriptors are compatible. a declared descriptor is what could statically be determined (e.g. from looking at the return value of a property accessor method) whilst an actual descriptor is the type of an actual object that was returned, which may differ. for generic types with unbound type variables, the declared descriptor discovered may be 'object' but from the actual descriptor it is possible to observe that the objects are really numeric values (e.g. ints).
string format for all operators is the same
numeric comparison operators share very similar generated code, only differing in two comparison instructions.
process the string form of a number, using the specified base if supplied and return an appropriate literal to hold it. any suffix to indicate a long will be taken into account (either 'l' or 'l' is supported).
if all the components of the list are constants, or lists that themselves contain constants, then a constant list can be built to represent this node. this will speed up later getvalue calls and reduce the amount of garbage created.
ask an argument to generate its bytecode and then follow it up with any boxingunboxingcheckcasting to ensure it matches the expected parameter descriptor.
generate code that handles building the argument values for the specified method. this method will take account of whether the invoked method is a varargs method and if it is then the argument values will be appropriately packaged into an array.
check the first operand matches the regex specified as the second operand. second operand, otherwise false (e.g. the regex is invalid)
compare the left operand to see it is an instance of the type specified as the right operand. the right operand must be a class. otherwise false
if all the components of the map are constants, or listsmaps that themselves contain constants, then a constant list can be built to represent this node. this will speed up later getvalue calls and reduce the amount of garbage created.
evaluate the condition and if true evaluate the first alternative, otherwise evaluate the second alternative. a boolean or there is a problem executing the chosen alternative
implements the multiply operator directly here for certain types of supported operands and otherwise delegates to any registered overloader for types not supported here. supported operand types:  numbers string and int ('abc' 2 == 'abcabc') 
convert operand value to string using registered converter or using
walk through a possible tree of nodes that combine strings and append them all to the same (on stack) stringbuilder.
returns a boolean based on whether a value is in the range expressed. the first operand is any value whilst the second is a list of two values - those two values being the bounds allowed for the first operand (inclusive).
find a field of a certain name on a specified class.
attempt to create an optimized property accessor tailored for a property of a particular name on a particular class. the general reflectivepropertyaccessor will always work but is not optimal due to the need to lookup which reflective member (methodfield) to use each time read() is called. this method will just return the reflectivepropertyaccessor instance if it is unable to build a more optimal accessor. note: an optimal accessor is currently only usable for read attempts. do not call this method if you need a read-write accessor.
return the method suffixes for a given property name. the default implementation uses javabean conventions with additional support for properties of the form 'xy' where the method 'getxy()' is used in preference to the javabean convention of 'getxy()'.
return the method suffix for a given property name. the default implementation uses javabean conventions.
compare argument arrays and return information about whether they match. a supplied type converter and conversionallowed flag allow for matches to take into account that a type may be transformed into a different type by the converter. this variant of comparearguments also allows for a varargs match. or null if it was not a match
takes an input set of argument values and converts them to the types specified as the required parameter types. the arguments are converted 'in-place' in the input array. ( null if not varargs)
package up the arguments so that they correctly match what is expected in parametertypes. for example, if parametertypes is (int, string[]) because the second parameter was declared string..., then if arguments is [1,"a","b"] then it must be repackaged as [1,new string[]"a","b"] in order to match the expected types.
compare argument arrays and return information about whether they match. a supplied type converter and conversionallowed flag allow for matches to take into account that a type may be transformed into a different type by the converter. or null if it was not a match
convert a supplied set of arguments into the requested types. if the parametertypes are related to a varargs method then the final entry in the parametertypes array is going to be an array itself whose component type should be used as the conversion target for extraneous arguments. (for example, if the parametertypes are integer, string[] and the input arguments are integer, boolean, float then both the boolean and float must be converted to strings). this method does not repackage the arguments into a form suitable for the varargs invocation - a subsequent call to setupargumentsforvarargsinvocation handles that.
based on methodinvoker#gettypedifferenceweight(class[], object[]) but operates on typedescriptors.
check if the supplied value is the first entry in the array represented by the possiblearray value.
register a filter for methods on the given type. or null to clear any filter for the given type
locate a method on a type. there are three kinds of match that might occur:  an exact match where the types of the arguments match the types of the constructor an in-exact match where the types we are looking for are subtypes of those defined on the constructor a match where we are able to convert the arguments into those expected by the constructor, according to the registered type converter 
create a simpleevaluationcontext for the specified propertyaccessor delegates: typically a custom propertyaccessor specific to a use case (e.g. attribute resolution in a custom data structure), potentially combined with a databindingpropertyaccessor if property dereferences are needed as well.
register the specified methodresolver delegates for a combination of property access and method resolution.
register a methodfilter which will be called during method resolution for the specified type. the methodfilter may remove methods andor sort the methods which will then be used by spel as the candidates to look through for a match.
find a (possibly unqualified) type reference - first using the type name as-is, then trying any registered prefixes if the type name cannot be found.
locate a constructor on the type. there are three kinds of match that might occur:  an exact match where the types of the arguments match the types of the constructor an in-exact match where the types we are looking for are subtypes of those defined on the constructor a match where we are able to convert the arguments into those expected by the constructor, according to the registered type converter. 
create a new executor for the given method.
copes with nesting, for example '$...$...' where the correct end for the first $ is the final . matching end suffix is being sought
return true if the specified suffix can be found at the supplied position in the supplied expression string.
helper that parses given expression string using the configured parser. the expression string can contain any number of expressions all contained in "$..." markers. for instance: "foo$expr0bar$expr1". the static pieces of text will also be returned as expressions that just return that static piece of text. as a result, evaluating all returned expressions and concatenating the results produces the complete evaluated string. unwrapping is only done of the outermost delimiters found, so the string 'hello $foo$abc' would break into the pieces 'hello ' and 'foo$abc'. this means that expression languages that used $.. as part of their functionality are supported without any problem. the parsing is aware of the structure of an embedded expression. it assumes that parentheses '(', square brackets '[' and curly brackets '' must be in pairs within the expression unless they are within a string literal and a string literal starts and terminates with a single quote '.
determines if there is a type converter available in the specified context and attempts to use it to convert the supplied value to the specified type. throws an exception if conversion is not possible. of the value to the specified type is not supported
cancels all statically registered timers on shutdown, and stops the underlying timermanager (if not shared).
considers the underlying timermanager as running if it is neither suspending nor stopping.
adapt the given job object to the quartz job interface. the default implementation supports straight quartz jobs as well as runnables, which get wrapped in a delegatingjob.
create the job instance, populating it with property values taken from the scheduler context, job data map and trigger data map.
invoke the method via the methodinvoker.
add the given trigger to the scheduler, if it doesn't already exist. overwrites the trigger in any case if "overwriteexistingjobs" is set.
register all specified listeners with the scheduler.
register jobs and triggers (within a transaction, if possible).
register a list of jobdetail objects with the scheduler that this factorybean creates, to be referenced by triggers. this is not necessary when a trigger determines the jobdetail itself: in this case, the jobdetail will be implicitly registered in combination with the trigger.
this implementation applies the passed-in job data map as bean property values, and delegates to executeinternal afterwards.
create the scheduler instance for the given factory and scheduler name. called by #afterpropertiesset. the default implementation invokes schedulerfactory's getscheduler method. can be overridden for custom scheduler creation.
start the quartz scheduler, respecting the "startupdelay" setting. the scheduler asynchronously
initialize the given schedulerfactory, applying locally defined quartz properties to it.
expose the specified context attributes andor the current applicationcontext in the quartz schedulercontext.
create a schedulerfactory if necessary and apply locally defined quartz properties to it.
resolve the cache to use.
convert the collection of caches in a single expected element. throw an illegalstateexception if the collection holds more than one element
rewrite the call stack of the specified exception so that it matches the current call stack up to (included) the specified method invocation. clone the specified exception. if the exception is not serializable, the original exception is returned. if no common ancestor can be found, returns the original exception. used to make sure that a cached exception has a valid invocation context. stack up to (included) the common ancestor specified by the classname and
check for a cached exception. if the exception is found, throw it directly.
generate a default cache name for the specified method.
return the cacheinvocationparameter for the parameters that are to be used to compute the key. per the spec, if some method parameters are annotated with of the key. if none are annotated, all parameters except the parameter annotated with javax.cache.annotation.cachevalue should be part of the key. the method arguments must match the signature of the related method invocation used to compute the key
generate a key for the specified invocation.
return an identifying description for this caching operation. available to subclasses, for inclusion in their tostring() result.
return the cacheinvocationparameter for the parameter holding the value to cache. the method arguments must match the signature of the related method invocation
construct a new defaultjcacheoperationsource with the given cache manager, cache resolver and key generator suppliers, applying the corresponding default if a supplier is not resolvable.
build an ehcache cachemanager from the given configuration resource.
parse ehcache configuration from the given resource, for further use with custom cachemanager creation.
build an ehcache cachemanager from the default configuration. the cachemanager will be configured from "ehcache.xml" in the root of the class path (that is, default ehcache initialization - as defined in the ehcache docs - will apply). if no configuration file can be found, a fail-safe fallback configuration will be used.
create an ehcachecache instance.
decorate the given cache, if necessary.
create the known caches again with the current state of this manager.
copy the contents of this message to the given target message.
copy constructor for creating a new simplemailmessage from the state of an existing simplemailmessage instance.
constructor for mailsendexception.
constructor for registration of failed messages, with the messages that failed as keys, and the thrown exceptions as values. the messages should be the same that were originally passed to the invoked send method. exceptions as values
obtain and connect a transport from the underlying javamail session, passing in the specified host, port, username, and password.
validate that this instance can connect to the server that it is configured for. throws a messagingexception if the connection attempt failed.
actually send the given array of mimemessages via javamail. that the mimemessages have been created from (with same array length and indices as the "mimemessages" array), if any in case of authentication failure in case of failure when sending a message
create a new instance of the javamailsenderimpl class. initializes the #setdefaultfiletypemap "defaultfiletypemap" property with a default configurablemimefiletypemap.
obtain a transport object from the given javamail session, using the configured protocol. can be overridden in subclasses, e.g. to return a mock transport object.
return the delegate filetypemap, compiled from the mappings in the mapping file and the entries in the mappings property.
compile a filetypemap from the mappings in the given mapping file and the given mapping entries. the default implementation creates an activation framework mimetypesfiletypemap, passing in an inputstream from the mapping resource (if any) and registering the mapping lines programmatically.
determine the default java activation filetypemap for the given mimemessage. or a default configurablemimefiletypemap if none found for the message
determine the mimemultipart objects to use, which will be used to store attachments on the one hand and text(s) and inline elements on the other hand. texts and inline elements can either be stored in the root element itself (multipart_mode_mixed, multipart_mode_related) or in a nested element rather than the root element directly (multipart_mode_mixed_related). by default, the root mimemultipart element will be of type "mixed" (multipart_mode_mixed) or "related" (multipart_mode_related). the main multipart element will either be added as nested element of type "related" (multipart_mode_mixed_related) or be identical to the root element itself (multipart_mode_mixed, multipart_mode_related). object to (mixed, related, mixed_related, or no)
add an attachment to the mimemessage, taking the content from a note that the inputstream returned by the datasource implementation needs to be a fresh one on each call, as javamail will invoke appear in the mail (the content type will be determined by this) the content from, determining the inputstream and the content type
add an inline element to the mimemessage, taking the content from a the content type will be determined by the name of the given content file. do not use this for temporary files with arbitrary filenames (possibly ending in ".tmp" or the like)! note: invoke addinline after #settext; else, mail readers might not be able to resolve inline references correctly. in the body part, surrounded by angle brackets: e.g. "myid" -> "<myid>". can be referenced in html source via src="cid:myid" expressions.
add an attachment to the mimemessage, taking the content from a the content type will be determined by the name of the given content file. do not use this for temporary files with arbitrary filenames (possibly ending in ".tmp" or the like)! appear in the mail
add an inline element to the mimemessage, taking the content from a note that the inputstream returned by the datasource implementation needs to be a fresh one on each call, as javamail will invoke note: invoke addinline after #settext; else, mail readers might not be able to resolve inline references correctly. in the body part, surrounded by angle brackets: e.g. "myid" -> "<myid>". can be referenced in html source via src="cid:myid" expressions. the content from, determining the inputstream and the content type
return the underlying mime "multipartrelated" object, if any. can be used to manually add body parts, inline elements, etc. this will be nested within the root mimemultipart, in case of a multipart mail.
set the given plain text and html text as alternatives, offering both options to the email client. requires multipart mode. note: invoke #addinline after settext; else, mail readers might not be able to resolve inline references correctly.
add an attachment to the mimemessage, taking the content from an note that the inputstream returned by the inputstreamsource implementation needs to be a fresh one on each call, as javamail will invoke getinputstream() multiple times. appear in the mail (all of spring's resource implementations can be passed in here)
add an inline element to the mimemessage, taking the content from an specifying the content type explicitly. you can determine the content type for any given filename via a java activation framework's filetypemap, for example the one held by this helper. note that the inputstream returned by the inputstreamsource implementation needs to be a fresh one on each call, as javamail will invoke note: invoke addinline after settext; else, mail readers might not be able to resolve inline references correctly. in the body part, surrounded by angle brackets: e.g. "myid" -> "<myid>". can be referenced in html source via src="cid:myid" expressions.
validate the given mail address. called by all of mimemessagehelper's address setters and adders. default implementation invokes internetaddress.validate(), provided that address validation is activated for the helper instance. note that this method will just work on javamail >= 1.3. you can override it for validation on older javamail versions or for custom validation.
return the root mime "multipartmixed" object, if any. can be used to manually add attachments. this will be the direct content of the mimemessage, in case of a multipart mail.
create an activation framework datasource for the given inputstreamsource.
process the specified freemarker template with the given model and write the result to the given writer. when using this method to prepare a text for a mail to be sent with spring's mail support, consider wrapping iotemplateexception in mailpreparationexception. as keys and model objects as values
prepare the freemarker configuration and return it.
determine a freemarker templateloader for the given path. default implementation creates either a filetemplateloader or a springtemplateloader.
return a templateloader based on the given templateloader list. if more than one templateloader has been registered, a freemarker multitemplateloader needs to be created.
create a new springtemplateloader.
callback method for notifying the endpoint base class that the concrete endpoint invocation led to an exception. to be invoked by subclasses in case of the concrete endpoint throwing an exception.
set the xa transaction manager to use for wrapping endpoint invocations, enlisting the endpoint resource in each such transaction. the passed-in object may be a transaction manager which implements spring's org.springframework.transaction.jta.transactionfactory interface, or a plain javax.transaction.transactionmanager. if no transaction manager is specified, the endpoint invocation will simply not be wrapped in an xa transaction. check out your resource provider's activationspec documentation for local transaction options of your particular provider.
the standard jca 1.5 version of createendpoint. this implementation delegates to #createendpointinternal(), initializing the endpoint's xaresource before the endpoint gets invoked.
this beforedelivery implementation starts a transaction, if necessary, and exposes the endpoint classloader as current thread context classloader. note that the jca 1.7 specification does not require a resourceadapter to call this method before invoking the concrete endpoint. if this method has not been called (check #hasbeforedeliverybeencalled()), the concrete endpoint method should call beforedelivery and its sibling #afterdelivery() explicitly, as part of its own processing.
this afterdelivery implementation resets the thread context classloader and completes the transaction, if any. note that the jca 1.7 specification does not require a resourceadapter to call this method after invoking the concrete endpoint. see the explanation in #beforedelivery's javadoc.
the alternative jca 1.6 version of createendpoint. this implementation delegates to #createendpointinternal(), ignoring the specified timeout. it is only here for jca 1.6 compliance.
wrap each concrete endpoint instance with an aop proxy, exposing the message listener's interfaces as well as the endpoint spi through an aop introduction.
activates the configured message endpoint.
deactivates the configured message endpoint.
prepares the message endpoint, and automatically activates it if the "autostartup" flag is set to "true".
execute the given work on the specified taskexecutor. (or -1 if not applicable or not known)
specify the jca bootstrapcontext that contains the workmanager to delegate to.
builds the bootstrapcontext and starts the resourceadapter with it.
create a cci connection via this template's connectionfactory.
initialize the single underlying connection. closes and reinitializes the connection if an underlying connection is present already.
wrap the given connection with a proxy that delegates every method call to it but suppresses close calls. this is useful for allowing application code to handle a special framework connection just like an ordinary connection from a cci connectionfactory.
close the given connection.
make sure a connection or connectionfactory has been set.
this implementation delegates to the getconnection(connectionspec) method of the target connectionfactory, passing in the specified user credentials. if the specified username is empty, it will simply delegate to the standard
determine whether the given jca cci connection is transactional, that is, bound to the current thread by spring's transaction facilities. (may be null)
close the given connection, obtained from the given connectionfactory, if it is not managed externally (that is, not bound to the thread). (if this is null, the call will be ignored) (can be null)
actually close the given connection, obtained from the given connectionfactory. same as #releaseconnection, but throwing the original resourceexception. directly accessed by transactionawareconnectionfactoryproxy. (if this is null, the call will be ignored) (can be null)
obtain a connection from the given connectionfactory. translates resourceexceptions into the spring hierarchy of unchecked generic data access exceptions, simplifying calling code and making any exception that is thrown more meaningful. is aware of a corresponding connection bound to the current thread, for example when using ccilocaltransactionmanager. will bind a connection to the thread if transaction synchronization is active (e.g. if in a jta transaction). note: if this is specified, a new connection will be obtained for every call, without participating in a shared transactional connection. if the attempt to get a connection failed
actually obtain a cci connection from the given connectionfactory. same as #getconnection, but throwing the original resourceexception. is aware of a corresponding connection bound to the current thread, for example when using ccilocaltransactionmanager. will bind a connection to the thread if transaction synchronization is active (e.g. if in a jta transaction). directly accessed by transactionawareconnectionfactoryproxy.
wrap the given connection with a proxy that delegates every method call to it but delegates close calls to connectionfactoryutils.
execute the interaction encapsulated by this operation object. by the createinputrecord method
create a mapped record from the connectionfactory's recordfactory.
return a recordfactory for the given connectionfactory. default implementation returns the connector's recordfactory if available, falling back to a notsupportedrecordfactory placeholder. this allows to invoke a recordcreator callback with a non-null recordfactory reference in any case.
close the given cci resultset and ignore any thrown exception. this is useful for typical finally blocks in manual cci code.
create an indexed record through the connectionfactory's recordfactory.
close the given cci interaction and ignore any thrown exception. this is useful for typical finally blocks in manual cci code.
invoke the given recordcreator, converting jca resourceexceptions to spring's dataaccessexception hierarchy.
create a template derived from this template instance, inheriting the connectionfactory and other settings but overriding the connectionspec used for obtaining connections. instance is supposed to obtain connections for
obtain a ccitemplate derived from the main template instance, inheriting the connectionfactory and other settings but overriding the connectionspec used for obtaining connections. template instance is supposed to obtain connections for
this implementation loads a spring applicationcontext through the
build a spring applicationcontext for the given jca bootstrapcontext. the default implementation builds a resourceadapterapplicationcontext and delegates to #loadbeandefinitions for actually parsing the specified configuration files.
this implementation always throws a notsupportedexception.
this implementation closes the spring applicationcontext.
return a translated exception if this is appropriate, otherwise return the given exception as-is. or the raw exception if it is not
return a unique result object from the given collection. returns null if 0 result objects found; throws an exception if more than 1 instance found. result object has been found in the given collection
return a unique result object from the given collection. throws an exception if 0 or more than 1 instance found. but is not expected to contain null elements) result object has been found in the given collection has been found in the given collection
return a single result object from the given collection. throws an exception if 0 or more than 1 element found. and is also expected to contain null elements) element has been found in the given collection has been found in the given collection
return a single result object from the given collection. returns null if 0 result objects found; throws an exception if more than 1 element found. element has been found in the given collection
return a unique result object from the given collection. throws an exception if 0 or more than 1 result objects found, of if the unique result object is not convertible to the specified required type. but is not expected to contain null elements) result object has been found in the given collection at all has been found in the given collection not match the specified required type
return a single result object from the given collection. throws an exception if 0 or more than 1 element found. but is not expected to contain null elements) element has been found in the given collection has been found in the given collection
detect all persistenceexceptiontranslators in the given beanfactory. persistenceexceptiontranslators from persistenceexceptiontranslators found in the factory
create a new persistenceexceptiontranslationadvisor. persistenceexceptiontranslators from
create a new persistenceexceptiontranslationadvisor.
set an application exception that was thrown before this transaction exception, preserving the original exception despite the overriding transactionsystemexception. application exception
return an identifying description for this transaction attribute. available to subclasses, for inclusion in their tostring() result.
add an attribute for a transactional method. method names can end or start with "" for matching multiple methods.
add an attribute for a transactional method.
add an attribute for a transactional method. method names can end or start with "" for matching multiple methods.
initialize the specified #setmethodmap(java.util.map) "methodmap", if any.
check that required properties were set.
create a transaction if necessary based on the given transactionattribute. allows callers to perform custom transactionattribute lookups through the transactionattributesource. (used for monitoring and logging purposes) the hastransaction() method on transactioninfo can be used to tell if there was a transaction created.
determine the specific transaction manager to use for the given transaction.
return the transaction status of the current method invocation. mainly intended for code that wants to set the current transaction rollback-only but not throw an application exception. because the method was invoked outside an aop invocation context
general delegate for around-advice-based subclasses, delegating to several other template methods on this class. able to handle callbackpreferringplatformtransactionmanager as well as regular platformtransactionmanager implementations.
handle a throwable, completing the transaction. we may commit or roll back, depending on the configuration.
set properties with method names as keys and transaction attribute descriptors (parsed via transactionattributeeditor) as values: e.g. key = "mymethod", value = "propagation_required,readonly". note: method names are always applied to the target class, no matter if defined in an interface or the class itself. internally, a namematchtransactionattributesource will be created from the given properties.
reset the transactioninfo threadlocal. call this in all cases: exception or normal return!
execute after successful completion of call, but not after an exception was handled. do nothing if we didn't create a transaction.
prepare a transactioninfo for the given attribute and status object. (used for monitoring and logging purposes)
create a new instance of the rollbackruleattribute class. this is the preferred way to construct a rollback rule that matches the supplied exception class (and subclasses). of throwable not a throwable type or is null
parses the given properties into a nameattribute map. expects method names as keys and string attributes definitions as values, parsable into transactionattribute instances via transactionattributeeditor.
add an attribute for a transactional method. method names can be exact matches, or of the pattern "xxx", "xxx" or "xxx" for matching multiple methods.
return the list of rollbackruleattribute objects (never null).
winning rule is the shallowest rule (that is, the closest in the inheritance hierarchy to the exception). if no rule applies (-1), return false.
format is propagation_name,isolation_name,readonly,timeout_nnnn,+exception1,-exception2. null or the empty string means that the method is non transactional.
creates an advisor for this factorybean's transactioninterceptor.
determine the transaction attribute for this method invocation. defaults to the class's transaction attribute if no method attribute is found. is not transactional
parses the  tag. will with the container as necessary.
copy constructor. definition can be modified through bean property setters.
set the isolation level. must be one of the isolation constants in the transactiondefinition interface. default is isolation_default. exclusively designed for use with #propagation_required or transactions. consider switching the "validateexistingtransactions" flag to "true" on your transaction manager if you'd like isolation level declarations to get rejected when participating in an existing transaction with a different isolation level. note that a transaction manager that does not support custom isolation levels will throw an exception when given any other level than #isolation_default.
set the propagation behavior. must be one of the propagation constants in the transactiondefinition interface. default is propagation_required. exclusively designed for use with #propagation_required or transactions. consider switching the "validateexistingtransactions" flag to "true" on your transaction manager if you'd like isolation level declarations to get rejected when participating in an existing transaction with a different isolation level. note that a transaction manager that does not support custom isolation levels will throw an exception when given any other level than #isolation_default.
set the timeout to apply, as number of seconds. default is timeout_default (-1). exclusively designed for use with #propagation_required or transactions. note that a transaction manager that does not support timeouts will throw an exception when given any other timeout than #timeout_default.
set the isolation level by the name of the corresponding constant in transactiondefinition, e.g. "isolation_default". to one of the isolation_ constants or is null
set the propagation behavior by the name of the corresponding constant in transactiondefinition, e.g. "propagation_required". to one of the propagation_ constants or is null
return an identifying description for this transaction definition. available to subclasses, for inclusion in their tostring() result.
this implementation exposes the savepointmanager interface of the underlying transaction object, if any.
return the time to live for this object in milliseconds.
set the transaction rollback-only if the deadline has been reached, and throw a transactiontimedoutexception.
actually remove the value of the resource that is bound for the given key.
unbind a resource for the given key from the current thread.
deactivate transaction synchronization for the current thread. called by the transaction manager on transaction cleanup.
actually check the value of the resource that is bound for the given key.
bind the given resource for the given key to the current thread.
activate transaction synchronization for the current thread. called by a transaction manager on transaction begin.
retrieve a resource for the given key that is bound to the current thread. resource object), or null if none
register a new transaction synchronization for the current thread. typically called by resource management code. note that synchronizations can implement the they will be executed in an order according to their order value (if any).
return an unmodifiable snapshot list of all registered synchronizations for the current thread.
clear the entire transaction synchronization state for the current thread: registered synchronizations as well as the various transaction characteristics.
trigger flush callbacks on all currently registered synchronizations.
actually invoke the aftercommit methods of the given spring transactionsynchronization objects.
trigger beforecommit callbacks on all currently registered synchronizations.
trigger beforecompletion callbacks on all currently registered synchronizations.
actually invoke the aftercompletion methods of the given spring transactionsynchronization objects. constants in the transactionsynchronization interface
trigger aftercommit callbacks.
this implementation of rollback handles participating in existing transactions. delegates to dorollback and
trigger beforecommit callbacks.
this implementation of commit handles participating in existing transactions and programmatic rollback requests. delegates to isrollbackonly, docommit and rollback.
set the given transaction rollback-only. only called on rollback if the current transaction participates in an existing one. the default implementation throws an illegaltransactionstateexception, assuming that participating in existing transactions is generally not supported. subclasses are of course encouraged to provide such support.
suspend all current synchronizations and deactivate transaction synchronization for the current thread.
process an actual commit. rollback-only flags have already been checked and applied.
create a transactionstatus instance for the given arguments.
determine the actual timeout to use for the given definition. will fall back to this manager's default timeout if the transaction definition doesn't specify a non-default value.
trigger aftercompletion callbacks.
register the given list of transaction synchronizations with the existing transaction. invoked when the control of the spring transaction manager and thus all spring transaction synchronizations end, without the transaction being completed yet. this is for example the case when participating in an existing jta or ejb cmt transaction. the default implementation simply invokes the aftercompletion methods immediately, passing in "status_unknown". this is the best we can do if there's no chance to determine the actual outcome of the outer transaction.
process an actual rollback. the completed flag has already been checked.
trigger beforecompletion callbacks.
initialize transaction synchronization as appropriate.
create a transactionstatus for an existing transaction.
this implementation handles propagation behavior. delegates to and dobegin.
clean up after completion, clearing synchronization if necessary, and invoking docleanupaftercompletion.
specify the default timeout that this transaction manager should apply if there is no timeout specified at the transaction level, in seconds. default is the underlying transaction infrastructure's default timeout, e.g. typically 30 seconds in case of a jta provider, indicated by the
reactivate transaction synchronization for the current thread and resume all given synchronizations.
suspend the given transaction. suspends transaction synchronization first, then delegates to the dosuspend template method. (or null to just suspend active synchronizations, if any) (or null if neither transaction nor synchronization active)
resume outer transaction after inner transaction begin failed.
invoke dorollback, handling rollback exceptions properly.
perform a rollback, handling rollback exceptions properly.
release the savepoint that is held for the transaction.
roll back to the savepoint that is held for the transaction and release the savepoint right afterwards.
returns proxytransactionmanagementconfiguration or and aspectj values of enabletransactionmanagement#mode(), respectively.
create a custom annotationtransactionattributesource.
create a custom annotationtransactionattributesource, supporting public methods that carry the transactional annotation or the ejb3 javax.ejb.transactionattribute annotation. the transactional annotation only (typically for use with proxy-based aop), or protectedprivate methods as well (typically used with aspectj class weaving)
determine the transaction attribute for the given method or class. this implementation delegates to configured for parsing known annotations into spring's metadata attribute class. returns null if it's not transactional. can be overridden to support custom annotations that carry transaction metadata.
this implementation checks the usertransaction's rollback-only flag.
create a new springjtasynchronizationadapter for the given spring transactionsynchronization and jta transactionmanager. note that this adapter will never perform a rollback-only call on weblogic, since weblogic server is known to automatically mark the transaction as rollback-only in case of a beforecompletion exception. hence, on wls, this constructor is equivalent to the single-arg constructor. setting in case of an exception thrown in beforecompletion (can be omitted if the jta provider itself marks the transaction rollback-only in such a scenario, which is required by the jta specification as of jta 1.1)
create a new springjtasynchronizationadapter for the given spring transactionsynchronization and jta transactionmanager. note that this adapter will never perform a rollback-only call on weblogic, since weblogic server is known to automatically mark the transaction as rollback-only in case of a beforecompletion exception. hence, on wls, this constructor is equivalent to the single-arg constructor. setting in case of an exception thrown in beforecompletion (can be omitted if the jta provider itself marks the transaction rollback-only in such a scenario, which is required by the jta specification as of jta 1.1).
set the underlying jta transaction to rollback-only.
registers the synchronizations as interposed jta synchronization on the uowmanager.
look up the websphere uowmanager in jndi via the configured name.
apply the given transaction timeout. the default implementation will call
perform a jta begin on the jta usertransaction or transactionmanager. this implementation only supports standard jta functionality: that is, no per-transaction isolation levels and no transaction names. can be overridden in subclasses, for specific jta implementations. calls applyisolationlevel and applytimeout before invoking the usertransaction's begin method. behavior, isolation level, read-only flag, timeout, and transaction name
check the usertransaction as well as the transactionmanager handle, assuming standard jta requirements.
build a usertransaction handle based on the given transactionmanager.
look up the jta transactionmanager in jndi via the configured name. called by afterpropertiesset if no direct transactionmanager reference was set. can be overridden in subclasses to provide a different transactionmanager object.
register a jta synchronization on the jta transactionmanager, for calling the default implementation registers the synchronizations on the jta 1.1 transactionsynchronizationregistry, if available, or on the jta transactionmanager's current transaction - again, if available. if none of the two is available, a warning will be logged. can be overridden in subclasses, for specific jta implementations.
look up the jta usertransaction in jndi via the configured name. called by afterpropertiesset if no direct usertransaction reference was set. can be overridden in subclasses to provide a different usertransaction object.
perform a jta resume on the jta transactionmanager. can be overridden in subclasses, for specific jta implementations.
apply the given transaction isolation level. the default implementation will throw an exception for any level other than isolation_default. to be overridden in subclasses for specific jta implementations, as alternative to overriding the full #dojtabegin method. cannot be applied
this implementation returns a jtatransactionobject instance for the jta usertransaction. the usertransaction object will either be looked up freshly for the current transaction, or the cached one looked up at startup will be used. the latter is the default: most application servers use a shared singleton usertransaction that can be cached. turn off the "cacheusertransaction" flag to enforce a fresh lookup for every transaction.
look up the jta 1.1 transactionsynchronizationregistry in jndi via the configured name. can be overridden in subclasses to provide a different transactionmanager object. transactionsynchronizationregistry
perform a jta suspend on the jta transactionmanager. can be overridden in subclasses, for specific jta implementations.
configure the xstream instance with this marshaller's bean properties.
construct an xstream instance, either using one of the standard constructors or creating a custom subclass.
marshals the given graph to the given xstream hierarchicalstreamwriter. converts exceptions using #convertxstreamexception.
convert the given xstream exception to an appropriate exception from the a boolean flag is used to indicate whether this exception occurs during marshalling or unmarshalling, since xstream itself does not make this distinction in its exception hierarchy. or unmarshalling ( false)
scan the packages for classes marked with jaxb2 annotations.
template method that can be overridden by concrete jaxb marshallers for custom initialization behavior. gets called after creation of jaxb marshaller, and after the respective properties have been set. the default implementation sets the #setunmarshallerproperties(map) defined properties, the #setvalidationeventhandler(validationeventhandler) validation event handler, the #setschemas(resource[]) schemas, #setunmarshallerlistener(javax.xml.bind.unmarshaller.listener) listener, and
template method that can be overridden by concrete jaxb marshallers for custom initialization behavior. gets called after creation of jaxb marshaller, and after the respective properties have been set. the default implementation sets the #setmarshallerproperties(map) defined properties, the #setvalidationeventhandler(validationeventhandler) validation event handler, the #setschemas(resource[]) schemas, #setmarshallerlistener(javax.xml.bind.marshaller.listener) listener, and
convert the given jaxbexception to an appropriate exception from the
template method that allows for customizing of the given castor marshaller.
template method that allows for customizing of the given castor unmarshaller.
create the castor xmlcontext. subclasses can override this to create a custom context. the default implementation loads mapping files if defined, or the target class or packages if defined.
convert the given xmlexception to an appropriate exception from the a boolean flag is used to indicate whether this exception occurs during marshalling or unmarshalling, since castor itself does not make this distinction in its exception hierarchy. or unmarshalling ( false)
template method for handling staxresults. this implementation delegates to marshalxmlsteamwriter or
build a new document from this marshaller's documentbuilderfactory, as a placeholder for a dom node.
template method for handling saxresults. this implementation delegates to marshalsaxhandlers.
template method for handling domresults. this implementation delegates to marshaldomnode.
template method for handling streamsources. this implementation delegates to unmarshalinputstream or unmarshalreader.
create a documentbuilder that this marshaller will use for creating dom documents when passed an empty domsource. the resulting documentbuilderfactory is cached, so this method will only be called once.
create an xmlreader that this marshaller will when passed an empty saxsource.
unmarshals the given provided javax.xml.transform.source into an object graph. this implementation inspects the given result, and calls unmarshaldomsource, a saxsource, nor a streamsource
template method for handling saxsources. this implementation delegates to unmarshalsaxreader.
template method for handling staxsources. this implementation delegates to unmarshalxmlstreamreader or
create a documentbuilder that this marshaller will use for creating dom documents when passed an empty domsource. can be overridden in subclasses, adding further initialization of the builder.
template method for handling domsources. this implementation delegates to unmarshaldomnode. if the given source is empty, an empty source document will be created as a placeholder.
marshals the object graph with the given root into the provided javax.xml.transform.result. this implementation inspects the given result, and calls marshaldomresult, a saxresult, nor a streamresult
template method for handling streamresults. this implementation delegates to marshaloutputstream or marshalwriter, depending on what is contained in the streamresult contain an outputstream nor a writer
create a new marshallingsource with the given marshaller and content.
create a sax inputsource from the given resource. sets the system identifier to the resource's url, if available.
retrieve the url from the given resource as system id. returns null if it cannot be opened.
create a new imarshallingcontext, configured with the correct indentation.
convert the given jibxexception to an appropriate exception from the a boolean flag is used to indicate whether this exception occurs during marshalling or unmarshalling, since jibx itself does not make this distinction in its exception hierarchy. or unmarshalling ( false)
create a websocketextension with the given name and parameters.
parse the given, comma-separated string into a list of websocketextension objects. this method can be used to parse a "sec-websocket-extension" header.
returns the value of the sec-websocket-extensions header.
sets the (new) value(s) of the sec-websocket-extensions header.
returns the value of the sec-websocket-key header.
actually register the endpoints. called by #aftersingletonsinstantiated().
create a new serverendpointregistration instance from an
perform the sub-protocol negotiation based on requested and supported sub-protocols. for the list of supported sub-protocols, this method first checks if the target websockethandler is a subprotocolcapable and then also checks if any sub-protocols have been explicitly configured with
a method that can be used to associate a user with the websocket session in the process of being established. the default implementation calls subclasses can provide custom logic for associating a user with a session, for example for assigning a name to anonymous users (i.e. not fully authenticated).
filter the list of requested websocket extensions. as of 4.1, the default implementation of this method filters the list to leave only extensions that are both requested and supported.
called to initialize the encoderdecoder.
strategy method used to obtain the conversionservice. by default this method expects a bean named 'websocketconversionservice' in the
encode an object to a message.
decode the a message into an object.
constructor that associates a user with the websocket session. fallback on the user available in the underlying websocket session
create a new instance frame with the given frame content.
see "json unicode encoding" section of sockjs protocol.
create a sockjsclient with the given transports. if the list includes an xhrtransport (or more specifically an implementation of inforeceiver) the instance is used to initialize the #setinforeceiver(inforeceiver) inforeceiver property, or otherwise is defaulted to resttemplatexhrtransport.
return a timeout cleanup task to invoke if the sockjs sessions is not fully established within the retransmission timeout period calculated in request.
create a new sockjshttprequesthandler.
this method determines the sockjs path and handles sockjs static urls. session urls and raw websocket requests are delegated to abstract methods.
ensure the path does not contain a file extension, either in the filename (e.g. "jsonp.bat") or possibly after path parameters ("jsonp;setup.bat") which could be used for rfd exploits. since the last part of the path is expected to be a transport type, the presence of an extension would not work. all we need to do is check if there are any path parameters, which would have been removed from the sockjs path during request mapping, and if found reject the request.
create a transporthandlingsockjsservice with given transporthandler handler types. the provided taskscheduler should be declared as a spring bean to ensure it gets initialized at start-up and shuts down when the application stops
for internal use within a transporthandler and the (transporthandler-specific) session class.
invoked when the underlying connection is closed.
performs cleanup and notify the websockethandler.
close due to error arising from sockjs transport handling.
handle the first request for receiving messages on a sockjs http transport based session. long polling-based transports (e.g. "xhr", "jsonp") complete the request after writing the open frame. streaming-based transports ("xhr_streaming", "eventsource", and "htmlfile") leave the response open longer for further streaming of message frames but will also close it eventually after some amount of data has been sent.
handle all requests, except the first one, to receive messages on a sockjs http transport based session. long polling-based transports (e.g. "xhr", "jsonp") complete the request after writing any buffered message frames (or the next one). streaming-based transports ("xhr_streaming", "eventsource", and "htmlfile") leave the response open longer for further streaming of message frames but will also close it eventually after some amount of data has been sent.
return a handler mapping with the mapped viewcontrollers.
the default taskscheduler to use if none is registered explicitly via  class="code"> @configuration @enablewebsocket public class websocketconfig implements websocketconfigurer public void registerwebsockethandlers(websockethandlerregistry registry) registry.addhandler(myhandler(), "echo") .withsockjs() .settaskscheduler(myscheduler()); ... 
handle incoming websocket messages from clients.
the simple broker produces simpmessagetype.connect_ack that's not stomp specific and needs to be turned into a stomp connected frame.
invoked when no is configured to send an error frame to the client.
handle stomp messages going back out to websocket clients.
also automatically sets the #setdefaultheartbeat defaultheartbeat property to "10000,10000" if it is currently set to "0,0".
class constructor. sets #setdefaultheartbeat to "0,0" but will reset it back to the preferred "10000,10000" when a
an overloaded version of that accepts a fully prepared java.net.uri.
handle an inbound message from a websocket client.
register a sub-protocol handler.
find a subprotocolhandler for the given session.
handle an outbound spring message to a websocket client.
when a session is connected through a higher-level protocol it has a chance to use heartbeat management to shut down sessions that are too slow to send or receive messages. however, after a websocketsession is established and before the higher level protocol is fully connected there is a possibility for sessions to hang. this method checks and closes any sessions that have been connected for more than 60 seconds without having received a single message.
validate the given parameter count against the given declared parameters.
set the column names of the auto-generated keys.
add one or more declared parameters. used for configuring this operation when used in a bean factory. each parameter will specify sql type and (optionally) the parameter's name.
set whether prepared statements should be capable of returning auto-generated keys.
validate the parameters passed to an execute method based on declared parameters. subclasses should invoke this method before every executequery() or update() method.
set whether to use statements that are capable of returning updatable resultsets.
add anonymous parameters, specifying only their sql types as defined in the java.sql.types class. parameter ordering is significant. this method is an alternative to the #declareparameter method, which should normally be preferred.
declare a parameter for this operation. the order in which this method is called is significant when using positional parameters. it is not significant when using named parameters with named sqlparameter objects here; it remains significant when using named parameters in combination with unnamed sqlparameter objects here. the parameter's name. note that you typically use the sqlparameter class itself here, not any of its subclasses. and hence cannot be configured further
compile this query. ignores subsequent attempts to compile. been correctly initialized, for example if no datasource has been provided
validate the named parameters passed to an execute method based on declared parameters. subclasses should invoke this method before every executequery() or
check whether this operation has been compiled already; lazily compile it if not already compiled. automatically called by validateparameters.
analogous to the sqlquery.execute([]) method. this is a generic method to execute a query, taken a number of arguments. object wrapper types for primitives.
execute the stored procedure with the provided parameter values. this is a convenience method where the order of the passed in parameter values must match the order that the parameters where declared in. not be included in this map. it is legal for values to be null, and this will produce the correct behavior using a null argument to the stored procedure. output parameters will appear here, with their values after the stored procedure has been called.
declare a parameter. overridden method. parameters declared as sqlparameter and sqlinoutparameter will always be used to provide input values. in addition to this any parameter declared as sqloutparameter where an non-null input value is provided will also be used as an input paraneter. note: calls to declareparameter must be made in the same order as they appear in the database's stored procedure parameter list. names are purely used to help mapping.
overridden method to configure the preparedstatementcreatorfactory based on our declared parameters.
central execution method. all named parameter execution goes through this method. the sqlparameters. primitive parameters must be represented by their object wrapper type. the ordering of parameters is not significant since they are supplied in a sqlparametermap which is an implementation of the map interface. callback method. the jdbc operation itself doesn't rely on this parameter, but it can be useful for creating the objects of the result list. will be of the same class, although it is possible to use different types.
trigger any queued update operations to be added as a final batch.
return the number of affected rows for all already executed statements. accumulates all of flush's return values until
method to execute the update given arguments and retrieve the generated keys using a keyholder. matching named parameters specified in the sql statement
generic method to execute the update given named parameters. all other update methods invoke this method. matching named parameters specified in the sql statement
method to execute the update given arguments and retrieve the generated keys using a keyholder.
check the given number of affected rows against the specified maximum number or required number. if the actually affected rows are out of bounds
overridden method to configure the callablestatementcreatorfactory based on our declared parameters.
set the exception class for the specified error codes.
check whether the validation query can be executed on a connection from the specified datasource, with the specified interval between checks, until the specified timeout.
register a new custom translator for the specified database name.
pre-checks the arguments, calls #dotranslate, and invokes the
return the sqlerrorcodes instance for the given database. no need for a database meta-data lookup.
create a new instance of the sqlerrorcodesfactory class. not public to enforce singleton design pattern. would be private except to allow testing via overriding the do not subclass in application code.
return sqlerrorcodes for the given datasource, evaluating "databaseproductname" from the instance if no sqlerrorcodes were found.
check the customsqlexceptiontranslatorregistry for any entries.
associate the specified database name with the given datasource. definition file (must not be null)
eagerly initialize the exception translator, if demanded, creating a default one for the specified datasource if none set.
return the exception translator for this instance. creates a default sqlerrorcodesqlexceptiontranslator for the specified datasource if none set, or a
gets the sql state code from the supplied sqlexception exception. some jdbc drivers nest the actual exception from a batched update, so we might need to dig down into the nested exception. is to be extracted
create a custom dataaccessexception, based on a given exception class from a customsqlerrorcodestranslation definition. the resulting dataaccessexception. this exception should include the
return whether the given jdbc driver supports jdbc 2.0 batch updates. typically invoked right before execution of a given set of statements: to decide whether the set of sql statements should be executed through the jdbc 2.0 batch mechanism or simply in a traditional one-by-one fashion. logs a warning if the "supportsbatchupdates" methods throws an exception and simply returns false in that case.
retrieve a jdbc column value from a resultset, using the specified value type. uses the specifically typed resultset accessor methods, falling back to note that the returned value may not be assignable to the specified required type, in case of an unknown type. calling code needs to deal with this case appropriately, e.g. throwing a corresponding exception. with further conversion steps necessary)
convert a column name with underscores to the corresponding property name using "camel case". a name like "customer_number" would match a "customernumber" property name.
extract database meta-data via the given databasemetadatacallback. this method will open a connection to the database and retrieve the database meta-data. since this method is called before the exception translation feature is configured for a datasource, this method can not rely on the sqlexception translation functionality. any exceptions will be wrapped in a metadataaccessexception. this is a checked exception and any calling code should catch and handle this exception. you can just log the error and hope for the best, but there is probably a more serious error that will reappear when you try to access the database again. the databasemetadatacallback's processmetadata method
close the given jdbc statement and ignore any thrown exception. this is useful for typical finally blocks in manual jdbc code.
retrieve a jdbc column value from a resultset, using the most appropriate value type. the returned value should be a detached value object, not having any ties to the active resultset: in particular, it should not be a blob or clob object but rather a byte array or string representation, respectively. uses the getobject(index) method, but includes additional "hacks" to get around oracle 10g returning a non-standard object for its timestamp datatype and a java.sql.date for date columns leaving out the time portion: these columns will explicitly be extracted as standard
extract a common name for the target database in use even if various driversplatforms provide varying names at runtime.
call the specified method on databasemetadata for the given datasource, and extract the invocation result. or failed to invoke the specified method
close the given jdbc connection and ignore any thrown exception. this is useful for typical finally blocks in manual jdbc code.
determine the column name to use. the column name is determined based on a lookup using resultsetmetadata. this method implementation takes into account recent clarifications expressed in the jdbc 4.0 specification: columnlabel - the label for the column specified with the sql as clause. if the sql as clause was not specified, then the label is the name of the column.
close the given jdbc resultset and ignore any thrown exception. this is useful for typical finally blocks in manual jdbc code.
create a new resultsetwrappingsqlrowset for the given resultset. (usually a javax.sql.rowset.cachedrowset) the resultsetmetadata failed
executes the sql as specified by #getsequencequery().
statement to use to clean up "sequence" values. the default implementation either deletes the entire range below the current maximum value, or the specifically generated values (starting with the lowest minus 1, just preserving the maximum value) - according to the #isdeletespecificvalues() setting. (the number of values corresponds to #getcachesize())
set the default isolation level by the name of the corresponding constant in org.springframework.transaction.transactiondefinition, e.g. "isolation_serializable". if not specified, the target datasource's default will be used. note that a transaction-specific isolation value will always override any isolation setting specified at the datasource level.
applies the current isolation level value and read-only flag to the returned connection.
specify the default isolation level to use for connection retrieval, according to the jdbc java.sql.connection constants (equivalent to the corresponding spring if not specified, the target datasource's default will be used. note that a transaction-specific isolation value will always override any isolation setting specified at the datasource level.
this implementation sets the isolation level but ignores the timeout.
prepare the transactional connection right after transaction begin. the default implementation executes a "set transaction read only" statement if the #setenforcereadonly "enforcereadonly" flag is set to true and the transaction definition indicates a read-only transaction. the "set transaction read only" is understood by oracle, mysql and postgres and may work with other databases as well. if you'd like to adapt this treatment, override this method accordingly.
wraps the given connection with a proxy that delegates every method call to it but delegates close() calls to datasourceutils.
prepare the given connection before it is exposed. the default implementation applies the auto-commit flag, if necessary. can be overridden in subclasses.
specifying a custom username and password doesn't make sense with a single connection. returns the single connection if given the same username and password; throws a sqlexception else.
wrap the given connection with a proxy that delegates every method call to it but suppresses close calls.
initialize the underlying connection via the drivermanager.
determine whether there are currently thread-bound credentials, using them if available, falling back to the statically specified username and password (i.e. values of the bean properties) otherwise. delegates to #dogetconnection(string, string) with the determined credentials as parameters.
override the existing connection handle with the given connection. reset the handle if given null. used for releasing the connection on suspend (with a null argument) and setting a fresh connection on resume.
this implementation releases the given jdbc 3.0 savepoint.
this implementation creates a jdbc 3.0 savepoint and returns it.
this implementation rolls back to the given jdbc 3.0 savepoint.
prepare the given connection with the given transaction semantics.
obtain a connection from the given datasource. translates sqlexceptions into the spring hierarchy of unchecked generic data access exceptions, simplifying calling code and making any exception that is thrown more meaningful. is aware of a corresponding connection bound to the current thread, for example when using datasourcetransactionmanager. will bind a connection to the thread if transaction synchronization is active, e.g. when running within a if the attempt to get a connection failed
reset the given connection after a transaction, regarding read-only flag and isolation level.
actually obtain a jdbc connection from the given datasource. same as #getconnection, but throwing the original sqlexception. is aware of a corresponding connection bound to the current thread, for example when using datasourcetransactionmanager. will bind a connection to the thread if transaction synchronization is active (e.g. if in a jta transaction). directly accessed by transactionawaredatasourceproxy.
actually close the given connection, obtained from the given datasource. same as #releaseconnection, but throwing the original sqlexception. directly accessed by transactionawaredatasourceproxy. (if this is null, the call will be ignored) (may be null)
determine whether the given two connections are equal, asking the target connection in case of a proxy. used to detect equality even if the user passed in a raw target connection while the held one is a proxy. (potentially a target connection without proxy)
close the connection, unless a smartdatasource doesn't want us to.
apply the specified timeout - overridden by the current transaction timeout, if any - to the given jdbc statement object.
close the given connection, obtained from the given datasource, if it is not managed externally (that is, not bound to the thread). (if this is null, the call will be ignored) (may be null)
actually fetch a connection from the given datasource, defensively turning an unexpected null return value from
return the target connection, fetching it and initializing it if necessary.
check the default connection properties (auto-commit, transaction isolation), keeping them to be able to expose them correctly without fetching an actual jdbc connection from the target datasource. this will be invoked once on startup, but also for each retrieval of a target connection. if the check failed on startup (because the database was down), we'll lazily retrieve those settings.
return a connection handle that lazily fetches an actual jdbc connection when asked for a statement (or preparedstatement or callablestatement). the returned connection handle implements the connectionproxy interface, allowing to retrieve the underlying target connection.
return a connection handle that lazily fetches an actual jdbc connection when asked for a statement (or preparedstatement or callablestatement). the returned connection handle implements the connectionproxy interface, allowing to retrieve the underlying target connection.
build properties for the driver, including the given username and password (if any), and obtain a corresponding connection.
set the jdbc driver class name. this driver will get initialized on startup, registering itself with the jdk's drivermanager. note: drivermanagerdatasource is primarily intended for accessing pre-registered jdbc drivers. if you need to register a new driver, consider using simpledriverdatasource instead. alternatively, consider initializing the jdbc driver yourself before instantiating this datasource. the "driverclassname" property is mainly preserved for backwards compatibility, as well as for migrating between commons dbcp and this datasource.
builds a websphere jdbcconnectionspec object for the current settings and calls wsdatasource.getconnection(jdbcconnectionspec).
checks that the specified 'targetdatasource' actually is a websphere wsdatasource.
this constructor retrieves the websphere jdbc connection spec api, so we can get obtain specific websphere connections using reflection.
hook to initialize the embedded database. if the generateuniquedatabasename flag has been set to true, the current value of the #setdatabasename database name will be overridden with an auto-generated name. subclasses may call this method to force initialization; however, this method should only be invoked once. after calling this method, #getdatasource() returns the
factory method that returns the embeddeddatabase embedded database instance, which is also a datasource.
hook to shutdown the embedded database. subclasses may call this method to force shutdown. after calling, #getdatasource() returns null. does nothing if no embedded database has been initialized.
create a new embedded database builder with the given resourceloader.
get the singleton derbyembeddeddatabaseconfigurer instance.
get the singleton hsqlembeddeddatabaseconfigurer instance.
return a configurer instance for the given embedded database type.
returns an java.io.outputstream that ignores all data given to it.
get the singleton h2embeddeddatabaseconfigurer instance.
retrieve the current target datasource. determines the a lookup in the #settargetdatasources targetdatasources map, falls back to the specified
resolve the specified data source object into a datasource instance. the default implementation handles datasource instances and data source names (to be resolved via a #setdatasourcelookup datasourcelookup).
supports integer values for the isolation level constants as well as isolation level names as defined on the
read a script from the provided resource, using the supplied comment prefix and statement separator, and build a string containing the lines. lines beginning with the comment prefix are excluded from the results; however, line comments anywhere else  for example, within a statement  will be included in the results. to be processed (typically "--")
read a script from the provided linenumberreader, using the supplied comment prefix and statement separator, and build a string containing the lines. lines beginning with the comment prefix are excluded from the results; however, line comments anywhere else  for example, within a statement  will be included in the results. to be processed (typically "--")
does the provided sql script contain the specified delimiter?
split an sql script into separate statements delimited by the provided separator string. each individual statement will be added to the provided within the script, the provided commentprefix will be honored: any text beginning with the comment prefix and extending to the end of the line will be omitted from the output. similarly, the provided delimiters will be honored: any text enclosed in a block comment will be omitted from the output. in addition, multiple adjacent whitespace characters will be collapsed into a single space. (typically a ';' or newline character) (typically "--") never null or empty never null or empty
execute the given sql script. statement separators and comments will be removed before executing individual statements within the supplied script. warning: this method does not release the provided connection. configured and ready to use to load the sql script from in the event of an error an error on a drop statement sql script (typically "--") @value #default_statement_separator if not specified and falls back to @value #fallback_statement_separator as a last resort; may be set to @value #eof_statement_separator to signal that the script contains a single statement without a separator
set the scripts to execute to initialize or clean up the database, replacing any previously added scripts.
execute the given databasepopulator against the given datasource.
extract a value for the single column in the current row. validates that there is only one column selected, then delegates to getcolumnvalue() and also
static factory method to create a new singlecolumnrowmapper (with the required type specified only once).
convert the given column value to the specified required type. only called if the extracted column value does not match already. if the required type is string, the value will simply get stringified via tostring(). in case of a number, the value will be converted into a number, either through number conversion or through string parsing (depending on the value type). otherwise, the value will be converted to a required type using the conversionservice. (never null) (never null)
convert a list of jdbc types, as defined in java.sql.types, to a list of sqlparameter objects as used in this package.
create a sqlrowset that wraps the given resultset, representing its data in a disconnected fashion. this implementation creates a spring resultsetwrappingsqlrowset instance that wraps a standard jdbc cachedrowset instance. can be overridden to use a different implementation.
create a new argtypepreparedstatementsetter for the given arguments.
set the value for a parameter. the method used is based on the sql type of the parameter and we can handle complex types like arrays and lobs. (optional, only used for sql null and sqltypevalue) (for decimal and numeric types)
set the specified preparedstatement parameter to null, respecting database-specific peculiarities.
derive a default sql type from the given java type.
set the class that each row should be mapped to.
convert a name in camelcase to an underscored name in lower case. any upper case letters are converted to lower case with a preceding underscore.
extract the values for all columns in the current row. utilizes public setters and result set meta-data.
initialize the mapping meta-data for the given class.
initialize the given beanwrapper to be used for row mapping. to be called for each row. the default implementation applies the configured conversionservice, if any. can be overridden in subclasses.
implementation of resultsetcallbackhandler. work out column size if this is the first row, otherwise just count rows. subclasses can perform custom extraction or processing by overriding the processrow(resultset, int) method.
extract output parameters from the completed stored procedure.
throw an sqlwarningexception if encountering an actual warning. may be null, in which case this method does nothing.
query using a prepared statement, allowing for a preparedstatementcreator and a preparedstatementsetter. most other query methods use this method, but application code will always work with either a creator or a setter. connection if this is null, the sql will be assumed to contain no bind parameters.
create a map instance to be used as the results map. if #resultsmapcaseinsensitive has been set to true, a linkedcaseinsensitivemap will be created; otherwise, a
process the given resultset from a stored procedure.
create a close-suppressing proxy for the given jdbc connection. called by the execute method. the proxy also prepares returned jdbc statements, applying statement settings such as fetch size, max rows, and query timeout.
prepare the given jdbc statement (or preparedstatement or callablestatement), applying statement settings such as fetch size, max rows, and query timeout.
throw an sqlwarningexception if we're not ignoring warnings, else log the warnings (at debug level).
extract returned resultsets from the completed stored procedure.
delegate method to perform the actual compilation. subclasses can override this template method to perform their own compilation. invoked after this base class's compilation is complete.
create a preparedstatement to be used for an insert operation with generated keys.
compile this jdbcinsert using provided parameters and meta-data plus other settings. this finalizes the configuration for this object and subsequent attempts to compile are ignored. this will be implicitly called the first time an un-compiled insert is executed. for example if no datasource has been provided
delegate method to execute the batch insert.
check whether this operation has been compiled already; lazily compile it if not already compiled. automatically called by validateparameters.
delegate method to execute the insert.
delegate method that executes a batch insert using the passed-in maps of parameters.
delegate method to execute the insert, generating a single key.
delegate method to execute the insert, generating any number of keys.
method to check whether we are allowed to make any configuration changes at this time. if the class has been compiled, then no further changes to the configuration are allowed.
delegate method that executes a batch insert using the passed-in sqlparametersource sqlparametersources.
delegate method to perform the actual compilation. subclasses can override this template method to perform their own compilation. invoked after this base class's compilation is complete.
add a org.springframework.jdbc.core.rowmapper for the specified parameter or column.
check whether this operation has been compiled already; lazily compile it if not already compiled. automatically called by doexecute.
delegate method to perform the actual call processing.
add a declared parameter to the list of parameters for the call. only parameters declared as sqlparameter and sqlinoutparameter will be used to provide input values. this is different from the storedprocedure class which - for backwards compatibility reasons - allows input values to be provided for parameters declared as sqloutparameter.
compile this jdbccall using provided parameters and meta-data plus other settings. this finalizes the configuration for this object and subsequent attempts to compile are ignored. this will be implicitly called the first time an un-compiled call is executed. been correctly initialized, for example if no datasource has been provided
create a namedparameterjdbctemplate based on the configured jdbctemplate.
add a map of parameters to this parameter source. so it's possible to chain several calls together
provide access to the property names of the wrapped bean. uses support provided in the propertyaccessor interface.
build a preparedstatementcreatorfactory based on the given sql and named parameters.
build a preparedstatementcreator based on the given sql and named parameters. note: used for the update variant with generated key handling, and also delegated from #getpreparedstatementcreator(string, sqlparametersource). actual newpreparedstatementcreator call
create an array of sqlparametersource objects populated with data from the values passed in (either a map or a bean object). this will define what is included in a batch operation.
create a wrapped value if parameter has type information, plain object if not.
create a map of case insensitive parameter names together with the original name.
create an array of mapsqlparametersource objects populated with data from the values passed in. this will define what is included in a batch operation.
parse the sql statement and locate any placeholders or named parameters. named parameters are substituted for a jdbc placeholder.
convert parameter declarations from an sqlparametersource to a corresponding list of sqlparameters. this is necessary in order to reuse existing methods on jdbctemplate. the sqlparameter for a named parameter is placed in the correct position in the resulting list based on the parsed sql statement info.
find a matching parameter in the given list of declared parameters.
convert a map of named parameter values to a corresponding array. (may be null). if specified, the parameter metadata will be built into the value array in the form of sqlparametervalue objects.
parse the sql statement and locate any placeholders or named parameters. named parameters are substituted for a jdbc placeholder, and any select list is expanded to the required number of placeholders. select lists may contain an array of objects, and in that case the placeholders will be grouped and enclosed with parentheses. this allows for the use of "expression lists" in the sql statement like:  > > the parameter values passed in are used to determine the number of placeholders to be used for a select list. select lists should be limited to 100 or fewer elements. a larger number of elements is not guaranteed to be supported by the database and is strictly vendor-dependent.
convert parameter types from an sqlparametersource into a corresponding int array. this is necessary in order to reuse existing methods on jdbctemplate. any named parameter types are placed in the correct position in the object array based on the parsed sql statement info.
delegates to handlenorowfound, handlemultiplerowsfound and streamdata, according to the resultset state. converts an ioexception thrown by streamdata to a lobretrievalfailureexception.
create a new clob value with the given character stream.
create a new blob value with the given byte array.
create a new clob value with the given content string.
create a new blobclob value with the given stream.
set the specified content via the lobcreator.
load bean definitions from the database via the given sql string. the first three columns must be bean name, property name and value. any join and any other columns are permitted: e.g. it's also possible to perform a join. column names are not significant -- only the ordering of these first three columns.
return the sqlexceptiontranslator of this dao's jdbctemplate, for translating sqlexceptions in custom jdbc access code.
method supporting the meta-data processing for a table's columns.
method supporting the meta-data processing for a table.
create a tablemetadataprovider based on the database meta-data.
create a callmetadataprovider based on the database meta-data.
build the call string based on configuration and meta-data information.
create a returnresultsetparametersqloutparameter depending on the support provided by the jdbc driver used for the database in use.
match input parameter values with the parameters declared to be used in the call.
get the name of the single out parameter for this call. if there are multiple parameters, the name of the first one will be returned.
match input parameter values with the parameters declared to be used in the call.
reconcile the provided parameters with available meta-data and add new ones where appropriate.
match the provided column names and values with the list of columns used.
compare columns created from meta-data with declared columns and return a reconciled list.
build the insert string based on configuration and meta-data information.
build the array of java.sql.types based on configuration and meta-data information.
match the provided column names and values with the list of columns used.
process the procedure column meta-data.
get the result of an object path expression as a map. cannot be casted to the expected type.
get the result of an object path expression as a boolean. cannot be casted to the expected type.
add a parameter for the expression. example:  string name = system.console().readline(); list<map> books = with(object).param("name", name).get("store.book.findall book -> book.author == name "); 
get the result of an object path expression as a list. cannot be casted to the expected type.
get the result of a object path expression as a java object. e.g. given the following object document:  "store": "book": [ "category": "reference", "author": "nigel rees", "title": "sayings of the century", "price": 8.95 , "category": "fiction", "author": "evelyn waugh", "title": "sword of honour", "price": 12.99 , "category": "fiction", "author": "herman melville", "title": "moby dick", "isbn": "0-553-21311-3", "price": 8.99 , "category": "fiction", "author": "j. r. r. tolkien", "title": "the lord of the rings", "isbn": "0-395-19395-8", "price": 22.99 ], "bicycle": "color": "red", "price": 19.95  and a java object like this:   public class book private string category; private string author; private string title; private string isbn; private float price; public string getcategory() return category; public void setcategory(string category) this.category = category; public string getauthor() return author; public void setauthor(string author) this.author = author; public string gettitle() return title; public void settitle(string title) this.title = title; public string getisbn() return isbn; public void setisbn(string isbn) this.isbn = isbn; public float getprice() return price; public void setprice(float price) this.price = price;   then  book book = from(object).getobject("store.book[2]", book.class);   maps the second book to a book instance.
create a new jsonpathconfig that uses the defaultcharset when deserializing json data.
create a new jsonpathconfig that returns json numbers as either doubles and floats or bigdecimals
creates a hamcrest matcher that validates that a json document conforms to the json schema provided to this method.
creates a hamcrest matcher that validates that a json document conforms to the json schema provided to this method.
creates a hamcrest matcher that validates that a json document conforms to the json schema provided to this method.
enable logging of both the request and the response if rest assured test validation fails with the specified log detail.   this is just a shortcut for:   restassured.config = new restassuredwebtestclientconfig().logconfig(logconfig().enableloggingofrequestandresponseifvalidationfails(logdetail)); 
set the header config
set the webtestclient config
set the xml config.
set the session config.
set the encoder config
set the decoder config
set the object mapper config.
set the log config.
set the multi-part config
set the async config
set the json config.
set the parameter config
enabled logging with the specified log detail. set a logconfig to configure the print stream and pretty printing options.
note: this will set attributes on `clientrequest` in `webtestclient`; given the way `webtesclient` works under the hood, these arguments remain on client side only and will not be propagated to the `serverrequest`.
note: this will set attributes on `clientrequest` in `webtestclient`; given the way `webtesclient` works under the hood, these arguments remain on client side only and will not be propagated to the `serverrequest`.
authenticate using the given principal. used as:  restassured.authentication = principal(myprincipal);  or in a mockmvcrequestspecbuilder:  mockmvcrequestspecification req = new mockmvcrequestspecbuilder().setauth(principal(myprincipal)). .. 
authenticate using the supplied authentication instance (org.springframework.security.core.authentication from spring security). used as:  restassured.authentication = authentication(myauth);  or in a mockmvcrequestspecbuilder:  mockmvcrequestspecification req = new mockmvcrequestspecbuilder().setauth(authentication(myauth)). .. 
build a mockmvc using the given, fully initialized, i.e. refreshed, webapplicationcontext and assign it to rest assured. the org.springframework.web.servlet.dispatcherservlet will use the context to discover spring mvc infrastructure and application controllers in it. the context must have been configured with a javax.servlet.servletcontext.
reset all static configurations to their default values.
enable logging of both the request and the response if rest assureds test validation fails with the specified log detail.   this is just a shortcut for:   restassured.config = new restassuredmockmvcconfig().logconfig(logconfig().enableloggingofrequestandresponseifvalidationfails(logdetail)); 
authenticate using the given principal. used as:  restassured.authentication = principal(myprincipal);  or in a mockmvcrequestspecbuilder:  mockmvcrequestspecification req = new mockmvcrequestspecbuilder().setauth(principal(myprincipal)). .. 
authenticate using the given principal and credentials. used as:  restassured.authentication = principalwithcredentials(myprincipal, mycredentials);  or in a mockmvcrequestspecbuilder:  mockmvcrequestspecification req = new mockmvcrequestspecbuilder().setauth(principalwithcredentials(myprincipal, mycredentials)). .. 
authenticate using a requestpostprocessor. this is mainly useful when you have added the spring-security-test artifact to classpath. this allows you to do for example:  restassured.authentication = with(user("username").password("password"));  where user is statically imported from org.springframework.security.test.web.servlet.request.securitymockmvcrequestpostprocessors.
creates a responseawarematcher that extracts the given path from the response and wraps it in a org.hamcrest.matchers#equalto(object) matcher. this is useful if you have a resource that e.g. returns the given json:  "userid" : "my-id", "href" : "http:localhost:8080my-id"  you can then test it like this:  get("x").then().body("href", containspath("userid")); 
creates a responseawarematcher that extracts the given path from the response and wraps it in a org.hamcrest.matchers#equalto(object) matcher. this is useful if you have a resource that e.g. returns the given json:  "userid" : "my-id", "playerid" : "my-id"  you can then test it like this:  get("x").then().body("userid", equaltopath("playerid")); 
creates a responseawarematcher that extracts the given path from the response and wraps it in a org.hamcrest.matchers#equalto(object) matcher. this is useful if you have a resource that e.g. returns the given json:  "userid" : "my-id", "href" : "http:localhost:8080my-id"  you can then test it like this:  get("x").then().body("href", endswithpath("userid")); 
creates a responseawarematcher that extracts the given path from the response and wraps it in a org.hamcrest.matchers#equalto(object) matcher. this is useful if you have a resource that e.g. returns the given json:  "userid" : "my-id", "baseuri" : "http:localhost:8080", "href" : "http:localhost:8080my-id"  you can then test it like this:  get("x").then().body("href", startswithpath("baseuri")); 
set the multi-part config
set the header config
set the xml config.
set the decoder config
set the json config.
set the mockmvc config
set the parameter config
set the async config
set the encoder config
enabled logging with the specified log detail. set a logconfig to configure the print stream and pretty printing options.
create a new mockmvcfactory with the supplied controllers or mock mvc configureres
set a session attribute.
set session attributes.
peeks into the xmlhtml that xmlpath will parse by printing it to the console. you can continue working with xmlpath afterwards. this is mainly for debug purposes. if you want to return a prettified version of the content see #prettify(). if you want to return a prettified version of the content and also print it to the console use #prettyprint().   note that the content is not guaranteed to be looking exactly like the it does at the source. this is because once you peek the content has been downloaded and transformed into another data structure (used by xmlpath) and the xml is rendered from this data structure. 
get the result of an xml path expression as a map. for syntax details please refer to  href="http:www.groovy-lang.orgprocessing-xml.html#_manipulating_xml">this url. cannot be casted to the expected type.
peeks into the xmlhtml that xmlpath will parse by printing it to the console in a prettified manner. you can continue working with xmlpath afterwards. this is mainly for debug purposes. if you want to return a prettified version of the content see #prettify(). if you want to return a prettified version of the content and also print it to the console use #prettyprint().   note that the content is not guaranteed to be looking exactly like the it does at the source. this is because once you peek the content has been downloaded and transformed into another data structure (used by xmlpath) and the xml is rendered from this data structure. 
add a parameter for the expression. example:  string type = system.console().readline(); list<map> books = with(object).param("type", type).get("shopping.category.findall it.@type == type"); 
declares a namespace.
specify properties that will be used when parsing xml.
create a new xmlpathconfig that uses the defaultcharset when deserializing xml data.
create a new instance of a xmlpathconfig based on the properties in the supplied config.
set a value of a feature flag.
specify features that will be used when parsing xml.
set a value of a property.
disables external dtd loading.  this is a shortcut for doing:  setfeature("http:apache.orgxmlfeaturesnonvalidatingload-external-dtd", false);  
instruct rest assured to connect to a proxy using a uri.
oauth sign the request. note that this currently does not wait for a www-authenticate challenge before sending the the oauth header. all requests to all domains will be signed for this instance.
excerpt from the httpbuilder docs: oauth sign the request. note that this currently does not wait for a www-authenticate challenge before sending the the oauth header. all requests to all domains will be signed for this instance. this assumes you've already generated an accesstoken and secrettoken for the site you're targeting. for more information on how to achieve this, see the  href="https:github.commttkaysignpostblobmasterdocsgettingstarted.md#using-signpost">signpost documentation.
create a list of arguments that can be used to create parts of the path in a bodycontent expression. this is useful in situations where you have e.g. pre-defined variables that constitutes the key. for example:  string somesubpath = "else"; int index = 1; when().get().then().body("something.%s[%d]", withargs(somesubpath, index), equalto("some value")). ..   or if you have complex root paths and don't wish to duplicate the path for small variations:  get("x").then().assertthat(). root("filters.filterconfig[%d].filterconfiggroups.find it.name == 'gold' .includes"). body(withargs(0), hasitem("first")). body(withargs(1), hasitem("second")). ..   the key and arguments follows the standard  href="http:download.oracle.comjavase1,5.0docsapijavautilformatter.html#syntax">formatting syntax of java.
use form authentication with the supplied configuration.
sets a certificate to be used for ssl authentication. see class#getresource(string) for how to get a url from a resource on the classpath. 
resets the #baseuri, #basepath, #port, #authentication and #rootpath, no authentication, <empty string>, null, null, <empty list>, null, null, none, true, new restassuredconfig(), null and null.
instruct rest assured to connect to a proxy on the specified host on port 8888.
excerpt from the httpbuilder docs: oauth sign the request. note that this currently does not wait for a www-authenticate challenge before sending the the oauth header. all requests to all domains will be signed for this instance. this assumes you've already generated an accesstoken and secrettoken for the site you're targeting. for more information on how to achieve this, see the  href="https:github.commttkaysignpostblobmasterdocsgettingstarted.md#using-signpost">signpost documentation.
create a http basic authentication scheme.
oauth sign the request. note that this currently does not wait for a www-authenticate challenge before sending the the oauth header. all requests to all domains will be signed for this instance.
enable logging of both the request and the response if rest assureds test validation fails with the specified log detail.   this is just a shortcut for:   restassured.config = restassured.config().logconfig(logconfig().enableloggingofrequestandresponseifvalidationfails(logdetail)); 
create a ntlm authentication scheme.
creates a responseawarematcher that extracts the given path from the response and wraps it in a org.hamcrest.matchers#equalto(object) matcher. this is useful if you have a resource that e.g. returns the given json:  "userid" : "my-id", "playerid" : "my-id"  you can then test it like this:  get("x").then().body("userid", equaltopath("playerid")); 
creates a responseawarematcher that extracts the given path from the response and wraps it in a org.hamcrest.matchers#equalto(object) matcher. this is useful if you have a resource that e.g. returns the given json:  "userid" : "my-id", "baseuri" : "http:localhost:8080", "href" : "http:localhost:8080my-id"  you can then test it like this:  get("x").then().body("href", startswithpath("baseuri")); 
creates a responseawarematcher that extracts the given path from the response and wraps it in a org.hamcrest.matchers#equalto(object) matcher. this is useful if you have a resource that e.g. returns the given json:  "userid" : "my-id", "href" : "http:localhost:8080my-id"  you can then test it like this:  get("x").then().body("href", containspath("userid")); 
creates a responseawarematcher that extracts the given path from the response and wraps it in a org.hamcrest.matchers#equalto(object) matcher. this is useful if you have a resource that e.g. returns the given json:  "userid" : "my-id", "href" : "http:localhost:8080my-id"  you can then test it like this:  get("x").then().body("href", endswithpath("userid")); 
compose this responseawarematcher with another responseawarematcher. todo add @safevarargs when migrating to java 7+
compose this responseawarematcher with another responseawarematcher.
compose this responseawarematcher with another responseawarematcher.
compose this responseawarematcher with another responseawarematcher.
compose this responseawarematcher with another responseawarematcher.
compose this responseawarematcher with another responseawarematcher.
compose this responseawarematcher with another responseawarematcher.
compose this responseawarematcher with another responseawarematcher. todo add @safevarargs when migrating to java 7+
builds a string to be used as an http accept header value, i.e. "applicationxml, textxml"
specify a charset for this content-type
specify a charset for this content-type
an alternative way to create a cookies object from the constructor.
an alternative way to create a headers object from the constructor.
sets the version of the cookie protocol this cookie complies with. version 0 complies with the original netscape cookie specification. version 1 complies with rfc 2109. since rfc 2109 is still somewhat new, consider version 1 as experimental; do not use it yet on production sites. parameters:
specify a byte array request content to be sent with the request. this only works for the post http method. trying to do this for the other http methods will cause an exception to be thrown.  note that #setbody(byte[]) and #setcontent(byte[]) are the same except for the syntactic difference. 
enabled logging with the specified log detail. set a logconfig to configure the print stream and pretty printing options.
merge this builder with settings from another specification. note that the supplied specification can overwrite data in the current specification. the following settings are overwritten:  port authentication scheme content type request body  the following settings are merged:  parameters cookies headers filters 
specify an object request content that will automatically be serialized to json or xml and sent with the request. if the object is a primitive or  href="http:download.oracle.comjavase6docsapijavalangnumber.html">number the object will be converted to a string and put in the request body. this works for the post, put and patch methods only. trying to do this for the other http methods will cause an exception to be thrown.   note that #setbody(object) and #setcontent(object) are the same except for the syntactic difference. 
specify a path parameter. path parameters are used to improve readability of the request path. e.g. instead of writing:  expect().statuscode(200).when().get("item"+myitem.getitemnumber()+"buy"+2);  you can write:  given(). pathparameter("itemnumber", myitem.getitemnumber()). pathparameter("amount", 2). expect(). statuscode(200). when(). get("itemitemnumberbuyamount");   which improves readability and allows the path to be reusable in many tests. another alternative is to use:  expect().statuscode(200).when().get("itemitemnumberbuyamount", myitem.getitemnumber(), 2); 
specify multiple path parameter name-value pairs. path parameters are used to improve readability of the request path. e.g. instead of writing:  expect().statuscode(200).when().get("item"+myitem.getitemnumber()+"buy"+2);  you can write:  given(). pathparameters("itemnumber", myitem.getitemnumber(), "amount", 2). expect(). statuscode(200). when(). get("itemitemnumberbuyamount");   which improves readability and allows the path to be reusable in many tests. another alternative is to use:  expect().statuscode(200).when().get("itemitemnumberbuyamount", myitem.getitemnumber(), 2); 
specify an object request content that will automatically be serialized to json or xml and sent with the request using a specific object mapper. this works for the post, patch and put methods only. trying to do this for the other http methods will cause an exception to be thrown.  note that #setbody(object, objectmapper) and #setcontent(object, objectmapper) are the same except for the syntactic difference. 
add query parameters to be sent with the request as a map. this method is the same as #addparameters(java.util.map) for all http methods except post where this method can be used to differentiate between form and query params.
add parameters to be sent with the request as map.
add a form parameter to be sent with the request. this method is the same as #addparameter(string, object...) ) for all http methods except put where this method can be used to differentiate between form and query params.
if you need to specify some credentials when performing a request.
specify a string request content (such as e.g. json or xml) to be sent with the request. this works for the post, put and patch methods only. trying to do this for the other http methods will cause an exception to be thrown.  note that #setbody(string) and #setcontent(string) are the same except for the syntactic difference. 
add a query parameter to be sent with the request. this method is the same as #addparameter(string, object...) ) for all http methods except post where this method can be used to differentiate between form and query params.
specify multiple path parameter name-value pairs. path parameters are used to improve readability of the request path. e.g. instead of writing:  expect().statuscode(200).when().get("item"+myitem.getitemnumber()+"buy"+2);  you can write:  map<string,object> pathparams = new hashmap<string,object>(); pathparams.add("itemnumber",myitem.getitemnumber()); pathparams.add("amount",2); given(). pathparameters(pathparams). expect(). statuscode(200). when(). get("itemitemnumberbuyamount");   which improves readability and allows the path to be reusable in many tests. another alternative is to use:  expect().statuscode(200).when().get("itemitemnumberbuyamount", myitem.getitemnumber(), 2); 
add a parameter to be sent with the request.
add a form parameter to be sent with the request. this method is the same as #addparameter(string, java.util.collection) for all http methods except put where this method can be used to differentiate between form and query params.
add query parameters to be sent with the request as a map. this method is the same as #addparameters(java.util.map) for all http methods except post where this method can be used to differentiate between form and query params.
add a query parameter to be sent with the request. this method is the same as #addparameter(string, java.util.collection) for all http methods except post where this method can be used to differentiate between form and query params.
add a multi-value parameter to be sent with the request.
same as #expectcontent(string, org.hamcrest.matcher) expect that you can pass arguments to the path. this is useful in situations where you have e.g. pre-defined variables that constitutes the path:  string somesubpath = "else"; int index = 1; expect().body("something.%s[%d]", withargs(somesubpath, index), equalto("some value")). ..   or if you have complex root paths and don't wish to duplicate the path for small variations:  expect(). root("filters.filterconfig[%d].filterconfiggroups.find it.name == 'gold' .includes"). body(withargs(0), hasitem("first")). body(withargs(1), hasitem("second")). ..   the path and arguments follows the standard  href="http:download.oracle.comjavase1,5.0docsapijavautilformatter.html#syntax">formatting syntax of java.  note that withargs can be statically imported from the io.restassured.restassured class. 
expect that the json or xml response content conforms to one or more hamcrest matchers. json example  assume that a get request to "lotto" returns a json response containing:  "lotto": "lottoid":5, "winning-numbers":[2,45,34,23,7,5,3], "winners":[ "winnerid":23, "numbers":[2,45,34,23,3,5] , "winnerid":54, "numbers":[52,3,12,11,18,22] ]   you can verify that the lottoid is equal to 5 like this:  responsespecbuilder builder = new responsespecbuilder(); builder.expectcontent("lotto.lottoid", equalto(5)); 
merge this builder with settings from another specification. note that the supplied specification can overwrite data in the current specification. the following settings are overwritten:  content type root path status code status line  the following settings are merged:  response body expectations cookies headers 
expect that the response content conforms to one or more hamcrest matchers.
enabled logging with the specified log detail. set a logconfig to configure the print stream and pretty printing options.
set the content type of the response
build the actual response
set the status line of the response.
set a specific header
set the response body to a string
set the response body to an inputstream
clone an already existing response.
set the response body to an array of bytes
set response headers, e.g:  header first = new header("headername1", "headervalue1"); header second = new header("headername2", "headervalue2"); headers headers = new header(first, second); 
set some cookies that will be available in the response. to create cookies you can do:  cookie cookie1 = cookie.builder("username", "john").setcomment("comment 1").build(); cookie cookie2 = cookie.builder("token", 1234).setcomment("comment 2").build(); cookies cookies = new cookies(cookie1, cookie2); 
create a new multi-part specification with control name equal to file.
specify the charset for this charset.
create a new multi-part specification with control name equal to file.
set the headers for this multipart specification (replaces previous headers)
specify the charset for this charset.
create a new multi-part specification with control name equal to file.
add a header to this multipart specification.
enables logging with the supplied log detail of the request made to authenticate using form authentication using the specified logconfig. both the request and the response is logged.
enable cross-site request forgery (csrf) support when using form authentication by automatically trying to find the name and value of the csrf input field. for example if the login page looks like this:  <html> <head> <title>login<title> <head> <body> <form action="j_spring_security_check_with_csrf" method="post"> <table> <tr> <td>user:&nbsp;<td> <td><input type="text" name="j_username"><td> <tr> <tr> <td>password:<td> <td><input type="password" name="j_password"><td> <tr> <tr> <td colspan="2"><input name="submit" type="submit"><td> <tr> <table> <input type="hidden" name="_csrf" value="8adf2ea1-b246-40aa-8e13-a85fb7914341"> <form> <body> <html>  the csrf field name is called _csrf and rest assured will autodetect its name since the field name is the only hidden field on this page. if auto-detection fails you can consider using #withcsrffieldname(string).  important: when enabling csrf support then rest assured must always make an additional request to the server in order to be able to include in the csrf value which will slow down the tests.
include multiple additional fields when using form authentication by including input field values with the specified name. this is the same as #withadditionalfield(string) but for multiple fields.  important: when including an additional field without specifying a value then rest assured must always make an additional request to the server in order to be able to figure out the field value. this will slow down the tests.
include additional field when using form authentication by including input field value with the specified name. for example if the login page looks like this:  <html> <head> <title>login<title> <head> <body> <form action="j_spring_security_check_with_csrf" method="post"> <table> <tr> <td>user:&nbsp;<td> <td><input type="text" name="j_username"><td> <tr> <tr> <td>password:<td> <td><input type="password" name="j_password"><td> <tr> <tr> <td colspan="2"><input name="submit" type="submit"><td> <tr> <table> <input type="hidden" name="something" value="8adf2ea1-b246-40aa-8e13-a85fb7914341"> <form> <body> <html>  and you'd like to include the field named something as an additional form parameter in the request you can do like this:  given().auth().form(..., new formauthconfig(..).withadditionalfield("something"). ..  and then rest assured will send the form parameter something=8adf2ea1-b246-40aa-8e13-a85fb7914341  important: when including an additional field without specifying a value then rest assured must always make an additional request to the server in order to be able to figure out the field value. this will slow down the tests.
enable cross-site request forgery (csrf) support when using form authentication by including the csrf value of the input field with the specified name. for example if the login page looks like this:  <html> <head> <title>login<title> <head> <body> <form action="j_spring_security_check_with_csrf" method="post"> <table> <tr> <td>user:&nbsp;<td> <td><input type="text" name="j_username"><td> <tr> <tr> <td>password:<td> <td><input type="password" name="j_password"><td> <tr> <tr> <td colspan="2"><input name="submit" type="submit"><td> <tr> <table> <input type="hidden" name="_csrf" value="8adf2ea1-b246-40aa-8e13-a85fb7914341"> <form> <body> <html>  the csrf field name is called _csrf.  important: when enabling csrf support then rest assured must always make an additional request to the server in order to be able to include in the csrf value which will slow down the tests.
use preemptive http basic authentication. this means that the authentication details are sent in the request header regardless if the server has challenged for authentication or not.
use relaxed http validation. this means that you'll trust all hosts regardless if the ssl certificate is invalid. by using this method you don't need to specify a keystore (see #keystore(string, string) or trust store (see #truststore(java.security.keystore).
use a truststore located on the file-system. see #truststore(string, string) for more details.
use a keystore located on the file-system. see #keystore(string, string) for more details.
set the redirect config.
set the session config.
set the ssl config.
create a new restassuredconfiguration with the default configurations.
set the oauth config.
set the http client config.
set the object mapper config.
set the matcher config.
set the failure config.
create a new restassuredconfiguration with the supplied redirectconfig, httpclientconfig, logconfig,
set the log config.
set the connection config.
set the header config.
set the json config.
set the encoder config.
set the multipart config.
set the decoder config.
set the xml config.
set the parameter config.
specify the default charset to use for the specific content-type if it's not specified in the content-type header explicitly
specify the default charset to use for the specific content-type if it's not specified in the content-type header explicitly
specify the default charset of the content in the response that's assumed if no charset is explicitly specified in the response.
set a value of a feature flag.
declares a namespace and also sets #namespaceaware(boolean) to true.  note that you cannot use this to add namespaces for the org.hamcrest.xml.hasxpath matcher. this has to be done by providing a javax.xml.namespace.namespacecontext to the matcher instance.
set a value of a property.
disables external dtd loading.  this is a shortcut for doing:  setfeature("http:apache.orgxmlfeaturesnonvalidatingload-external-dtd", false);  
define headers that should be overwritten instead of merged adding headers or using request specifications. note that by default all headers are merged except the @value #accept_header_name and @value #content_type_header_name headers. for example, if the header with name header1 is not marked as overwritable (default) and you do the following:  given().header("header1", "value1").header("header1, "value2"). ..   then header1 will be sent twice in the request:  header1: value1 header1: value2   if you configure header1 to be overwritable by doing:  given(). config(restassured.config().headerconfig(headerconfig().overwriteheaderswithname("header1")). header("header1", "value1"). header("header1", "value2"). ...  then header1 will only be sent once:  header1: value2 
define headers that should be be merged instead of overwritten when adding headers or using request specifications. note that by default all headers are merged except the @value #accept_header_name and @value #content_type_header_name headers. this method is thus mainly used to change to merge behavior for headers that by default are overwritten or the revert changes of a request specification merge.
creates a new httpclientconfig instance with the @value org.apache.http.client.params.clientpnames#cookie_policy parameter set to @value org.apache.http.client.params.cookiepolicy#ignore_cookies.
add the given parameters to an already configured number of parameters.
set a http client parameter.
encodes the content (body) of the request specified with the given contenttype with the same encoder used by the supplied encoder. this is useful only if rest assured picks the wrong encoder (or can't recognize it) for the given content-type.
specify the default charset for query parameters
specify the default charset to use for the specific content-type if it's not specified in the content-type header explicitly
specify the default charset to use for the specific content-type if it's not specified in the content-type header explicitly
close connections that have idled for the amount of time specified in this config.
default object mapper configuration that uses no explicit object mapper. an object mapper will be found automatically in classpath if available. for more details see  href="http:code.google.comprest-assuredwikiusage#object_mapping">documentation.  also default object mapper factories will be used.
specify preemptive basic authentication for the proxy. will use hostname @value #default_host, port @value #default_port and scheme @value #default_scheme.
specify (preemptive) basic authentication for the proxy
decodeunescape a portion of a url, to use with the query part ensure plusasblank is true.
instantiate a logger using a specific print stream and a specific log detail and the option to pretty printing
instantiate a logger using a specific print stream and a specific log detail
checks if the potentialuri is a uri.
get a single entity with the supplied name. if there are several entities match the entityname then the last one is returned.
get all entity values of the entity with supplied name. if there's only one header matching the entity name then a list with only that header value is returned.
get a single entity value with the supplied name. if there are several headers match the headername then the last one is returned.
get all entities with the supplied name. if there's only one entity matching the entityname then a list with only that entity is returned.
find the status value that matches the given status code. status code.
default request encoder for a binary stream. acceptable argument types are:  inputstream byte[] bytearrayoutputstream closure  if a closure is given, it is executed with an outputstream passed as the single closure argument. any data sent to the stream from the body of the closure is used as the request content body.
helper method used by encoder methods to create an httpentity instance that encapsulates the request data. this may be used by any non-streaming encoder that needs to send textual data.
set the request body as a url-encoded list of parameters. this is typically used to simulate a http form post. for multi-valued parameters, enclose the values in a list, e.g. [ key1 : ['val1', 'val2'], key2 : 'etc.' ]
retrieve a encoder for the given content-type. this is called by httpbuilder to retrieve the correct encoder for a given content-type. the encoder is then used to serialize the request data in the request body. or null.
returns a map of default encoders. override this method to change what encoders are registered by default. you can of course call super.builddefaultencodermap() and then add or remove from that result as well.
accepts a collection or a javabean object which is converted to json. a map or collection will be converted to a jsonbuilder.. a string or gstring will be interpreted as valid json and passed directly as the request body (with charset conversion if necessary.)  if a closure is passed as the model, it will be executed as if it were a json object definition passed to a jsonbuilder. in order for the closure to be interpreted correctly, there must be a 'root' element immediately inside the closure. for example:  builder.post( json ) body = root first one = 1 two = '2' second = 'some string'   will return the following json string: "root":"first":"one":1,"two":"2","second":"some string"
encode the content as xml. the argument may be either an object whose tostring produces valid markup, or a closure which will be interpreted as a builder definition.
default handler used for a plain text content-type. acceptable argument types are:  closure writable reader  for closure argument, a printwriter is passed as the single argument to the closure. any data sent to the writer from the closure will be sent to the request content body.
get the httprequest class that represents this request type.
default constructor taking a name and a value. the value may be null.
utility method to convert a number of type to a uri instance. valid uri string from its tostring() result.
implementation of groovy's as operator, to allow type conversion. uribuilder instance does not represent a valid url.
the document fragment, without a preceeding '#'
adds all parameters within the scanner to the list of parameters, as encoded by encoding. for example, a scanner containing the string a=1&b=2&c=3 would add the namevaluepair namevaluepairs a=1, b=2, and c=3 to the list of parameters.  note that this method has been copied from urlencodedutils#parse(java.util.list, java.util.scanner, string) but it doesn't do url decoding. 
set the path component of this uri. the value may be absolute or relative to the current path. e.g.  def uri = new uribuilder( 'http:localhostp1p2?a=1' )  uri.path = 'p3p2' assert uri.tostring() == 'http:localhostp3p2?a=1'  uri.path = 'p2a' assert uri.tostring() == 'http:localhostp3p2a?a=1'  uri.path = '..p4' assert uri.tostring() ==   cannot be converted to a valid uri
set the query portion of the uri. for query parameters with multiple values, put the values in a list like so: uri.query = [ p1:'val1', p2:['val2', 'val3'] ] will produce a query string of ?p1=val1&p2=val2&p2=val3
remove the given query parameter from this uri's query string.
returns a string that is suitable for use as an applicationx-www-form-urlencoded list of parameters in an http put or http post.  this is a copy of urlencodedutils#format(java.util.list, string) that also handles basicnamevaluepairwithnovaluesupport. 
set the uri scheme, aka the 'protocol.' e.g. setscheme('https')
get the query string as a map for convenience. if any parameter contains multiple values (e.g. p1=one&p1=two) both values will be inserted into a list for that paramter key ([p1 : ['one','two']] ). note that this is not a "live" map. therefore, you cannot call  uri.query.a = 'bcd' you will not modify the query string but instead the generated map of parameters. instead, you need to use #removequeryparam(string) first, then call #setquery(map) which will set the entire query string. string.
add these parameters to the uribuilder's existing query string. parameters may be passed either as a single map argument, or as a list of named arguments. e.g.  uribuilder.addqueryparams( [one:1,two:2] ) uribuilder.addqueryparams( three : 3 )   if any of the parameters already exist in the uri query, these values will not replace them. multiple values for the same query parameter may be added by putting them in a list. see
returns a gzipinputstream which wraps the original entity's content stream
this implementation adds a gzipencoding and deflateencoding handler to the registry. override this method to provide a different set of defaults.
add the request and response interceptors to the httpclient, which will provide transparent decoding of the given content-encoding types. this method is called by httpbuilder and probably should not need be modified by sub-classes. a content-encoding string.
 convenience method to perform an http form patch. the response closure will be called only on a successful response.  a 'failed' response (i.e. any http status code > 399) will be handled by the registered 'failure' handler. the #defaultfailurehandler(httpresponsedecorator) default failure handler throws an httpresponseexception.  the request body (specified by a body named parameter) will be converted to a url-encoded form string unless a different requestcontenttype named parameter is passed to this method. (see encoderregistry#encodeform(map).)  represent a valid uri
set the default http proxy to be used for all requests.
parse the response data based on the given content-type. if the given content-type is contenttype#any, the content-type header from the response will be used to determine how to parse the response. content-type, or null if no parser could be found for this content-type. the parser will also return null if the response does not contain any content (e.g. in response to a head request).
valid arguments:  urieither a uri, url, or object whose tostring() method produces a valid uri string. if this parameter is not supplied, the httpbuilder's default uri is used. pathrequest path that is merged with the uri querymap of url query parameters headersmap of http headers contenttyperequest content type and accept header. if not supplied, the httpbuilder's default content-type is used. requestcontenttypecontent type for the request, if it is different from the expected response content-type bodyrequest body that will be encoded based on the given contenttype 
this is the default response.success handler. it will be executed if the response is not handled by a status-code-specific handler (i.e. response.'200'= ..) and no generic 'success' handler is given (i.e. response.success = ...) this handler simply returns the parsed data from the response body. in most cases you will probably want to define a response.success = ... handler from the request closure, which will replace the response handler defined by this method.   in practice, a user-supplied response handler closure is designed to handle streaming content so it can be read directly from the response stream without buffering, which will be much more efficient. therefore, it is recommended that request method variants be used which explicitly accept a response handler closure in these cases. response.
give a default uri to be used for all request methods that don't explicitly take a uri parameter. tostring() produces a valid uri string. see
create a requestconfigdelegate from the given arguments, execute the config closure, then pass the delegate to #dorequest(requestconfigdelegate), which actually executes the request.
set the request body. this value may be of any type supported by the associated encoderregistry request encoder. that is, the value of body will be interpreted by the encoder associated with the current #getrequestcontenttype() request content-type.
give a default uri to be used for all request methods that don't explicitly take a uri parameter, and a default content-type to be used for request encoding and response parsing. tostring() produces a valid uri string. see for common types.
creates default response handlers for status#success success and the handler map when a new httpbuilder instance is created.
set the default headers to add to all requests made by this builder instance. these values will replace any previously set default headers.
 convenience method to perform an http form post. the response closure will be called only on a successful response.  a 'failed' response (i.e. any http status code > 399) will be handled by the registered 'failure' handler. the #defaultfailurehandler(httpresponsedecorator) default failure handler throws an httpresponseexception.  the request body (specified by a body named parameter) will be converted to a url-encoded form string unless a different requestcontenttype named parameter is passed to this method. (see encoderregistry#encodeform(map).)  represent a valid uri
convenience method to perform an http get. the response closure will be called only on a successful response.   a 'failed' response (i.e. any http status code > 399) will be handled by the registered 'failure' handler. the throws an httpresponseexception. represent a valid uri
helper method to get the content-type string from the response (no charset).
set ntlm authentication credentials to be used for the given host and port.
set authentication credentials to be used for the given host and port.
oauth2 sign all requests. note that this currently does not wait for a www-authenticate challenge before sending the the oauth header. all requests to all domains will be signed for this instance.  this assumes you've already generated an accesstoken for the site you're targeting. for more information on how to achieve this, see the  href='https:github.comscribejavascribejavawikigetting-started'>scribe documentation.
oauth sign all requests. note that this currently does not wait for a www-authenticate challenge before sending the the oauth header. all requests to all domains will be signed for this instance.  this assumes you've already generated an accesstoken and secrettoken for the site you're targeting. for more information on how to achieve this, see the  href='https:github.comscribejavascribejavawikigetting-started'>scribe documentation. oauth handling and stop signing requests.
set authentication credentials to be used for the current a misnomer, since these credentials will actually work for "digest" authentication as well.
sets a certificate to be used for ssl authentication. see class#getresource(string) for how to get a url from a resource on the classpath.
set ntlm authentication credentials to be used for the current
prints the response to the print stream
returns true if the body in question probably contains human readable text. uses a small sample of code points to detect unicode control characters commonly used in binary file signatures.
change the level at which this interceptor logs.
returns an input stream containing one or more certificate pem files. this implementation just embeds the pem files in java strings; most applications will instead read this from a resource file that gets bundled with the application.
returns a trust manager that trusts certificates and none other. https services whose certificates have not been signed by these certificates will fail with a sslhandshakeexception. this can be used to replace the host platform's built-in trusted certificates with a custom set. this is useful in development where certificate authority-trusted certificates aren't available. or in production, to avoid reliance on third-party certificate authorities. see also certificatepinner, which can limit trusted certificates while still using the host platform's built-in trust store. warning: customizing trusted certificates is dangerous! relying on your own trusted certificates limits your server team's ability to update their tls certificates. by installing a specific set of trusted certificates, you take on additional operational complexity and limit your ability to migrate between certificate authorities. do not use custom trusted certificates in production without the blessing of your server's tls administrator.
returns a trust manager that trusts the vm's default certificate authorities.
returns the vm's default ssl socket factory, using trustmanager for trusted root certificates.
shows a browser url to authorize this app to act as this user.
waits for an oauth session for this client to be set.
starts a real time messaging session.
when the browser hits the redirect url, use the provided code to ask slack for a session.
see https:api.slack.commethodsrtm.start.
see https:api.slack.commethodsoauth.access.
see https:api.slack.comrtm.
see https:api.slack.comdocsoauth.
allocates and returns count fake addresses like [255.0.0.100, 255.0.0.101].
returns an okhttpclient for all tests to use as a starting point. the shared instance allows all tests to share a single connection pool, which prevents idle connections from consuming unnecessary resources while connections wait to be evicted. this client is also configured to be slightly more deterministic, returning a single ip address for all hosts, regardless of the actual number of ip addresses reported by dns.
sets the response cache to be used to read and write cached responses.
creates a java.net.cacheresponse of the correct (sub)type using information gathered from the supplied response.
extracts okhttp headers from the supplied java.net.cacheresponse. only real headers are extracted. see #extractstatusline(java.net.cacheresponse).
creates an okhttp response.body containing the supplied information.
returns headers for the header names and values in the map.
creates an okhttp response.body containing the supplied information.
creates an okhttp response using the supplied uri and urlconnection to supply the data. the urlconnection is assumed to already be connected. if this method returns
extracts okhttp headers from the supplied java.net.httpurlconnection. only real headers are extracted. see #extractstatusline(java.net.httpurlconnection).
creates an okhttp response using the supplied request and cacheresponse to supply the data.
creates an okhttp request from the supplied information. this method allows a null value for requestheaders for situations where a connection is already connected and access to the headers has been lost. see java.net.httpurlconnection#getrequestproperties() for details.
creates an java.net.httpurlconnection of the correct subclass from the supplied okhttp
convert a request header to okhttp's cookies via httpcookie. that extra step handles multiple cookies in a single request header, which cookie#parse doesn't support.
creates a urlstreamhandler as a java.net.url#seturlstreamhandlerfactory. this code configures okhttp to handle all http and https connections created with java.net.url#openconnection():  okhttpclient okhttpclient = new okhttpclient(); url.seturlstreamhandlerfactory(new okurlfactory(okhttpclient)); 
returns an immutable map containing each field to its list of values. non-null, this value is mapped to the null key.
now that we've buffered the entire request body, update the request headers and the body itself. this happens late to enable httpurlconnection users to complete the socket connection before sending request body bytes.
returns the value of the field at position. returns null if there are fewer than position headers.
returns s with control characters and non-ascii characters replaced with '?'.
throws throwable as either an ioexception, runtimeexception, or error.
returns true if either:  a specific proxy was explicitly configured for this connection. the response has already been retrieved, and a proxy was java.net.proxyselector selected in order to get it.  warning: this method may return false before attempting to connect and true afterwards.
aggressively tries to get the final http response, potentially making many http requests in the process in order to cope with redirects and authentication.
returns an input stream from the server in the case of error such as the requested file (txt, htm, html) is not found on the remote server.
consumes the field name of the specified length and the optional colon and its optional trailing space. returns the number of bytes skipped.
consumes \r, \r\n, or \n from #source.
process the next event. this will result in a single call to callback#onevent unless the data section was empty. any number of calls to
returns true if the first bytes of #source are key followed by a colon or a newline.
either writes this request to sink or measures its content length. we have one method do double-duty to make sure the counting and content are consistent, particularly when it comes to awkward operations like measuring the encoded length of header strings, or the length-in-digits of an encoded integer.
configure a factory to provide per-call scoped listeners that will receive analytic events for this client.
configure a single client scoped listener that will receive all analytic events for this client.
sets the dispatcher used to set policy and execute asynchronous requests. must not be null.
sets the proxy selection policy to be used if no #proxy proxy is specified explicitly. the proxy selector may return multiple proxies; in that case they will be tried in sequence until a successful connection is established. if unset, the proxyselector#getdefault() system-wide default proxy selector will be used.
sets the certificate pinner that constrains which certificates are trusted. by default https connections rely on only the #sslsocketfactory ssl socket factory to establish trust. pinning certificates avoids the need to trust certificate authorities.
sets the connection pool used to recycle http and https connections. if unset, a new connection pool will be used.
sets the socket factory used to secure https connections. if unset, the system default will be used. a field that okhttp needs to build a clean certificate chain. this method instead must use reflection to extract the trust manager. applications should prefer to call #sslsocketfactory(sslsocketfactory, x509trustmanager), which avoids such reflection.
sets the socket factory used to create connections. okhttp only uses the parameterless socketfactory#createsocket() createsocket() method to create unconnected sockets. overriding this method, e. g., allows the socket to be bound to a specific local address. if unset, the socketfactory#getdefault() system-wide default socket factory will be used.
sets the socket factory and trust manager used to secure https connections. if unset, the system defaults will be used. most applications should not call this method, and instead use the system defaults. those classes include special optimizations that can be lost if the implementations are decorated. if necessary, you can create and configure the defaults yourself with the following code:  trustmanagerfactory trustmanagerfactory = trustmanagerfactory.getinstance( trustmanagerfactory.getdefaultalgorithm()); trustmanagerfactory.init((keystore) null); trustmanager[] trustmanagers = trustmanagerfactory.gettrustmanagers(); if (trustmanagers.length != 1 || !(trustmanagers[0] instanceof x509trustmanager)) throw new illegalstateexception("unexpected default trust managers:" + arrays.tostring(trustmanagers)); x509trustmanager trustmanager = (x509trustmanager) trustmanagers[0]; sslcontext sslcontext = sslcontext.getinstance("tls"); sslcontext.init(null, new trustmanager[] trustmanager , null); sslsocketfactory sslsocketfactory = sslcontext.getsocketfactory(); okhttpclient client = new okhttpclient.builder() .sslsocketfactory(sslsocketfactory, trustmanager) .build(); 
sets the handler that can accept cookies from incoming http responses and provides cookies to outgoing http requests. if unset, cookiejar#no_cookies no cookies will be accepted nor provided.
uses request to connect a new web socket.
sets the verifier used to confirm that response certificates apply to requested hostnames for https connections. if unset, a default hostname verifier will be used.
sets the dns service used to lookup ip addresses for hostnames. if unset, the dns#system system-wide default dns will be used.
sets the authenticator used to respond to challenges from proxy servers. use #authenticator to set the authenticator for origin servers. if unset, the authenticator#none no authentication will be attempted.
configure the protocols used by this client to communicate with remote servers. by default this client will prefer the most efficient transport available, falling back to more ubiquitous protocols. applications should only call this method to avoid specific compatibility problems, such as web servers that behave incorrectly when http2 is enabled. the following protocols are currently supported:   href="http:www.w3.orgprotocolsrfc2616rfc2616.html">http1.1  href="https:tools.ietf.orghtmlrfc7540">h2  href="https:tools.ietf.orghtmlrfc7540#section-3.4">h2 with prior knowledge (cleartext only)  this is an evolving set. future releases include support for transitional protocols. the http1.1 transport will never be dropped. if multiple protocols are specified,  href="http:tools.ietf.orghtmldraft-ietf-tls-applayerprotoneg">alpn will be used to negotiate a transport. protocol negotiation is only attempted for https urls.  protocol#http_1_0 is not supported in this set. requests are initiated with http1.1. if the server responds with http1.0, that will be exposed by response#protocol(). protocol#h2_prior_knowledge then that must be the only protocol and https urls will not be supported. otherwise the list must contain protocol#http_1_1. the list must not contain null or protocol#http_1_0.
sets the authenticator used to respond to challenges from origin servers. use #proxyauthenticator to set the authenticator for proxy servers. if unset, the authenticator#none no authentication will be attempted.
in a future release setting this to true will be unnecessary and setting it to false will have no effect.
applies this spec to sslsocket.
returns a copy of this that omits cipher suites and tls versions not enabled by sslsocket.
returns the response as a byte array. this method loads entire response body into memory. if the response body is very large this may trigger an outofmemoryerror. prefer to stream the response body if this is a possibility for your response.
returns a new response body that transmits content. if contenttype is non-null and lacks a charset, this will use utf-8.
returns a new response body that transmits content.
attempt to enqueue this async call on executorservice. this will attempt to clean up if the executor has been shut down by reporting the call as failed.
returns the charset that should be used to encode the credentials.
returns a copy of this charset that expects a credential encoded with charset.
sets this request's cache-control header, replacing any cache control headers already present. if cachecontrol doesn't define any directives, this clears this request's cache-control headers.
sets the url target of this request. exception by calling httpurl#parse; it returns null for invalid urls.
attaches tag to the request using type as a key. tags can be read from a request using request#tag. use null to remove any existing tag assigned for type. use this api to attach timing, debugging, or other application data to a request so that you may read it in interceptors, event listeners, or callbacks.
confirms that at least one of the certificates pinned for hostname is in peercertificates. does nothing if there are no certificates pinned for hostname. okhttp calls this after a successful tls handshake, but before the connection is used. pinned for hostname.
pins certificates for pattern. info, base64-encoded and prefixed with either sha256 or sha1.
returns a certificate pinner that uses certificatechaincleaner.
returns the sha-256 of certificate's public key. in okhttp 3.1.2 and earlier, this returned a sha-1 hash of the public key. both types are supported, but sha-256 is preferred.
returns list of matching certificates' pins for the hostname. returns an empty list if the hostname does not have pinned certificates.
promotes eligible calls from #readyasynccalls to #runningasynccalls and runs them on the executor service. must not be called with synchronization because executing calls can call into user code.
set the maximum number of requests for each host to execute concurrently. this limits requests by the url's host name. note that concurrent requests to a single ip address may still exceed this limit: multiple hostnames may share an ip address or be routed through the same http proxy. if more than maxrequestsperhost requests are in flight when this is invoked, those requests will remain in flight. websocket connections to hosts do not count against this limit.
set the maximum number of requests to execute concurrently. above this requests queue in memory, waiting for the running calls to complete. if more than maxrequests requests are in flight when this is invoked, those requests will remain in flight.
returns a snapshot of the calls currently awaiting execution.
cancel all calls currently enqueued or executing. includes calls executed both call#execute() synchronously and call#enqueue asynchronously.
returns a snapshot of the calls currently being executed.
returns headers for the alternating header names and values. there must be an even number of arguments, and they must alternate between header names and values.
add a header line without any validation. only appropriate for headers from the remote peer or cache.
adds all headers from an existing collection.
add a field with the specified value without any validation. only appropriate for headers from the remote peer or cache.
set a field with the specified date. if the field is not found, it is added. if the field is found, the existing values are replaced.
set a field with the specified instant. if the field is not found, it is added. if the field is found, the existing values are replaced.
returns headers for the header names and values in the map.
add a header with the specified name and formatted date. does validation of header names and value.
add a header with the specified name and formatted instant. does validation of header names and value.
returns an immutable case-insensitive set of header names.
equivalent to build().get(name), but potentially faster.
returns the last value corresponding to the specified field parsed as an http date, or null if either the field is absent or cannot be parsed as a date.
add an header line containing a field name, a literal colon, and a value.
returns an immutable list of the header values for name.
returns an iterator over the urls in this cache. this iterator doesn't throw concurrentmodificationexception, but if new responses are added while iterating, their urls will not be returned. if existing responses are evicted during iteration, they will be absent (unless they were already returned). the iterator supports iterator#remove. removing a url from the iterator evicts the corresponding response from the cache. use this to evict selected responses.
reads an entry from an input stream. a typical entry looks like this:  http:google.comfoo get 2 accept-language: fr-ca accept-charset: utf-8 http1.1 200 ok 3 content-type: imagepng content-length: 100 cache-control: max-age=600  a typical https file looks like this:  https:google.comfoo get 2 accept-language: fr-ca accept-charset: utf-8 http1.1 200 ok 3 content-type: imagepng content-length: 100 cache-control: max-age=600 aes_256_with_md5 2 base64-encoded peercertificate[0] base64-encoded peercertificate[1] -1 tlsv1.2  the file is newline separated. the first two lines are the url and the request method. next is the number of http vary request header lines, followed by those lines. next is the response status line, followed by the number of http response header lines, followed by those lines. https responses also contain ssl session information. this begins with a blank line, and then a line containing the cipher suite. next is the length of the peer certificate chain. these certificates are base64-encoded and appear each on their own line. the next line contains the length of the local certificate chain. these certificates are also base64-encoded and appear each on their own line. a length of -1 is used to encode a null array. the last line is optional. if present, it contains the tls version.
peeks up to bytecount bytes from the response body and returns them as a new response body. if fewer than bytecount bytes are in the response body, the full response body is returned. if more than bytecount bytes are in the response body, the returned value will be truncated to bytecount bytes. it is an error to call this method after the body has been consumed. warning: this method loads the requested bytes into memory. most applications should set a modest limit on bytecount, such as 1 mib.
closes the response body. equivalent to body().close(). it is an error to close a response that is not eligible for a body. this includes the responses returned from #cacheresponse, #networkresponse, and #priorresponse().
add a part to the body.
either writes this request to sink or measures its content length. we have one method do double-duty to make sure the counting and content are consistent, particularly when it comes to awkward operations like measuring the encoded length of header strings, or the length-in-digits of an encoded integer.
appends a quoted-string to a stringbuilder. rfc 2388 is rather vague about how one should escape special characters in form-data parameters, and as it turns out firefox and chrome actually do rather different things, and both say in their comments that they're not really sure what the right approach is. we go with chrome's behavior (which also experimentally seems to match what ie does), but if you actually want to have a good chance of things working, please avoid double-quotes, newlines, percent signs, and the like in your field names.
assemble the specified parts into a request body.
set the mime type. expected values for type are #mixed (the default), #alternative, #digest, #parallel and #form.
returns a list of encoded path segments like ["a", "b", "c"] for the url http:hostabc. this list is never empty though it may contain a single empty string.  summary=""> url encodedpathsegments()  http:host [""]  http:hostabc ["a", "b", "c"]  http:hostab%20cd ["a", "b%20c", "d"] 
returns this url as a uri java.net.uri. because uri is more strict than this class, the returned uri may be semantically different from this url:  characters forbidden by uri like [ and | will be escaped. invalid percent-encoded sequences like %xx will be encoded like %25xx. whitespace and control characters in the fragment will be stripped.  these differences may have a significant consequence when the uri is interpreted by a webserver. for this reason the uri uri class and this method should be avoided.
returns this url's encoded fragment, like "abc" for http:host#abc. this returns null if the url has no fragment.  summary=""> url encodedfragment()  http:hostnull  http:host# ""  http:host#abc "abc"  http:host#abc|def "abc|def" 
returns the first query parameter named name decoded using utf-8, or null if there is no such query parameter.  summary=""> url queryparameter("a")  http:hostnull  http:host?null  http:host?a=apple&k=key+lime "apple"  http:host?a=apple&a=apricot "apple"  http:host?a=apple&b "apple" 
returns the number of '' and '\' slashes in input, starting at pos.
returns the query of this url, encoded for use in http resource resolution. the returned string may be null (for urls with no query), empty (for urls with an empty query) or non-empty (all other urls).  summary=""> url encodedquery()  http:hostnull  http:host? ""  http:host?a=apple&k=key+lime "a=apple&k=key+lime"  http:host?a=apple&a=apricot "a=apple&a=apricot"  http:host?a=apple&b "a=apple&b" 
returns the index of the ':' in input that is after scheme characters. returns -1 if
adds a set of encoded path segments separated by a slash (either \ or ). if segment.
returns this url's query, like "abc" for http:host?abc. most callers should prefer #queryparametername and #queryparametervalue because these methods offer direct access to individual query parameters.  summary=""> url query()  http:hostnull  http:host? ""  http:host?a=apple&k=key+lime  lime"  http:host?a=apple&a=apricot "a=apple&a=apricot"  http:host?a=apple&b "a=apple&b" 
returns 80 if scheme.equals("http"), 443 if scheme.equals("https") and -1 otherwise.
removes a path segment. when this method returns the last segment is always "", which means the encoded path will have a trailing ''. popping "abc" yields "ab". in this case the list of path segments goes from ["a", "b", "c", ""] to ["a", "b", ""]. popping "abc" also yields "ab". the list of path segments goes from ["a", "b", "c"] to ["a", "b", ""].
returns a builder for the url that would be retrieved by following link from this url, or null if the resulting url is not well-formed.
returns the password, or an empty string if none is set.  summary=""> url encodedpassword()  http:host ""  http:username@host ""  http:username:password@host "password"  http:a%20b:c%20d@host "c%20d" 
returns all values for the query parameter name ordered by their appearance in this url. for example this returns ["banana"] for queryparametervalue("b") on   summary=""> url queryparametervalues("a") queryparametervalues("b")  http:host [] []  http:host? [] []  http:host?a=apple&k=key+lime ["apple"] []  http:host?a=apple&a=apricot ["apple", "apricot"] []  http:host?a=apple&b ["apple"] [null] 
adds a path segment. if the input is ".." or equivalent, this pops a path segment.
returns this url as a url java.net.url.
adds the pre-encoded query parameter to this url's query string.
finds the first ':' in input, skipping characters between square braces "[...]".
returns the distinct query parameter names in this url, like ["a", "b"] for  if this url has no query this returns the empty set.  summary=""> url queryparameternames()  http:host []  http:host? [""]  http:host?a=apple&k=key+lime ["a", "k"]  http:host?a=apple&a=apricot ["a"]  http:host?a=apple&b ["a", "b"] 
re-encodes the components of this url so that it satisfies (obsolete) rfc 2396, which is particularly strict for certain components.
cuts encodedquery up into alternating parameter names and values. this divides a query string like  into the list ["subject", "math", "easy", null, "problem", "5-2=3"]. note that values may be null and may contain '=' characters.
returns the entire path of this url encoded for use in http resource resolution. the returned path will start with "".  summary=""> url encodedpath()  http:host ""  http:hostabc "abc"  http:hostab%20cd "ab%20cd" 
encodes the query parameter using utf-8 and adds it to this url's query string.
returns a substring of input on the range [pos..limit) with the following transformations:  tabs, newlines, form feeds and carriage returns are skipped. in queries, ' ' is encoded to '+' and '+' is encoded to "%2b". characters in encodeset are percent-encoded. control characters and non-ascii characters are percent-encoded. all other characters are copied without transformation. 
returns the username, or an empty string if none is set.  summary=""> url encodedusername()  http:host ""  http:username@host "username"  http:username:password@host "username"  http:a%20b:c%20d@host "a%20b" 
address.
returns the protocol identified by protocol.
returns a media type for string.
returns a new request body that transmits content. if contenttype is non-null and lacks a charset, this will use utf-8.
returns a new request body that transmits the content of file.
returns a new request body that transmits content.
returns a new request body that transmits content.
for older cipher suites because the prefix is ssl_ instead of tls_.
for older cipher suites because the prefix is ssl_ instead of tls_.
returns the remote peer's principle, or null if that peer is anonymous.
returns the local principle, or null if this peer is anonymous.
necessary for example.com to match www.example.com under rfc 2965. this extra dot is ignored by more recent specifications.
returns the index of the next date character in input, or if invert the index of the next non-date character in input.
returns the positive value if attributevalue is positive, or long#min_value if it is either 0 or negative. if the value is positive but out of range, this returns long#max_value.
returns a domain string like example.com for an input domain like example.com or .example.com.
parse a date as specified in rfc 6265, section 5.1.1.
returns true if this cookie should be included on a request to url. in addition to this check callers should also confirm that this cookie has not expired.
returns all of the cookies from a set of http response headers.
sets the minimum number of seconds that a response will continue to be fresh for. if the response will be stale when minfresh have elapsed, the cached response will not be used and a network request will be made. timeunit#seconds precision; finer precision will be lost.
accept cached responses that have exceeded their freshness lifetime by up to maxstale. if unspecified, stale cache responses will not be used. timeunit#seconds precision; finer precision will be lost.
sets the maximum age of a cached response. if the cache response's age exceeds maxage, it will not be used and a network request will be made. timeunit#seconds precision; finer precision will be lost.
returns the cache directives of headers. this honors both cache-control and pragma headers if they are present.
decodes an ipv6 address like 1111:2222:3333:4444:5555:6666:7777:8888 or ::1.
closes socket, ignoring any checked exceptions. does nothing if socket is null.
closes closeable, ignoring any checked exceptions. does nothing if closeable is null.
returns an immutable copy of map.
equivalent to string.substring(pos, limit).trim().
returns the index of the first character in input that contains a character in delimiters. returns limit if there is no such character.
reads until in is exhausted or the deadline has been reached. this is careful to not extend the deadline if one exists already.
if host is an ip address, this returns the ip address in canonical form. otherwise this performs idn toascii encoding and canonicalize the result to lowercase. for example this converts .net to xn--n3h.net, and www.google.com to if the result contains unsupported ascii characters.
returns the index of the first character in input that is delimiter. returns limit if there is no such character.
returns true if an http request for a and b can reuse a connection.
decodes an ipv4 address suffix of an ipv6 address, like 1111::5555:6666:192.168.0.1.
returns the index of the first character in input that is either a control character (like \u0000 or \n) or a non-ascii character. returns -1 if input has no such characters.
returns true if there is an element in first that is also in second. this method terminates if any intersection is found. the sizes of both arguments are assumed to be so small, and the likelihood of an intersection so great, that it is not worth the cpu cost of sorting or the memory cost of hashing.
increments pos until input[pos] is not ascii whitespace. stops at limit.
decrements limit until input[limit - 1] is not ascii whitespace. stops at
encodes an ipv6 address in canonical form according to rfc 5952.
returns an array containing only elements found in first and also in second. the returned elements are in the same order as in first.
closes serversocket, ignoring any checked exceptions. does nothing if serversocket is null.
remove the transmitter from the connection's list of allocations. returns a socket that the caller should close.
returns a new exchange to carry a new request and response.
release the connection if it is no longer needed. this is called after each exchange completes and after the call signals that no more exchanges are expected.
immediately closes the socket connection if it's currently held. use this to interrupt an in-flight request from any thread. it's the caller's responsibility to close the request body and response body streams; otherwise resources may be leaked. this method is safe to be called concurrently, but provides limited guarantees. if a transport layer connection has been established (such as a http2 stream) that is terminated. otherwise if a socket connection is being established, that is terminated.
prepare to create a stream to carry request. this prefers to use the existing connection if it exists.
copy bytecount bytes from the file at pos into to source. it is the caller's responsibility to make sure there are sufficient bytes to read: if there aren't this method throws an eofexception.
write bytecount bytes from source to the file at pos.
creates a new relay that reads a live stream from upstream, using file to share that data with other sources. warning: callers to this method must immediately call #newsource to create a source and close that when they're done. otherwise a handle to file will be leaked.
returns a new source that returns the same bytes as upstream. returns null if this relay has been closed and no further sources are possible. in that case callers should retry after building a new relay with #read.
creates a relay that reads a recorded stream from file. warning: callers to this method must immediately call #newsource to create a source and close that when they're done. otherwise a handle to file will be leaked.
selects where to find the bytes for a read and read them. this is one of three sources. upstream: in this case the current thread is assigned as the upstream reader. we read bytes from upstream and copy them to both the file and to the buffer. finally we release the upstream reader lock and return the new bytes. the file in this case we copy bytes from the file to the sink. the buffer in this case the bytes are immediately copied into sink and the number of bytes copied is returned. if upstream would be selected but another thread is already reading upstream this will block until that read completes. it is possible to time out while waiting for that.
returns the date for value. returns null if the value couldn't be parsed.
returns the path to request, like the '' in 'get http1.1'. never empty, even if the request url is. includes the query component if it exists.
returns the request status line, like "get http1.1". this is exposed to the application by http2.
returns a 'cookie' http request header with all cookies, like a=b; c=d.
figures out the http request to make in response to receiving userresponse. this will either add authentication headers, follow redirects or handle a client request timeout. if a follow-up is either unnecessary or not applicable, this returns null.
report and attempt to recover from a failure to communicate with a server. returns true if be recovered if the body is buffered or if the failure occurred before the request has been sent.
returns true if any commas were skipped.
returns the subset of the headers in response's request that impact the content of response's body.
returns the names of the request headers that need to be checked for equality when caching.
returns true if the response must have a (possibly 0-length) body. see rfc 7231.
parse rfc 7235 challenges. this is awkward because we need to look ahead to know how to interpret a token. for example, the first line has a parameter namevalue pair and the second line has a single token68:  www-authenticate: digest foo=bar www-authenticate: digest foo=  similarly, the first line has one challenge and the second line has two challenges:  www-authenticate: digest ,foo=bar www-authenticate: digest ,foo 
reads a double-quoted string, unescaping quoted pairs like \" to the 2nd character in each sequence. returns the unescaped string, or null if the buffer isn't prefixed with a double-quoted string.
returns the next index in input at or after pos that contains a character from
consumes and returns a non-empty token, terminating at special characters in #token_delimiters. returns null if the buffer is empty or prefixed with a delimiter.
returns the next non-whitespace character in input that is white space. result is undefined if input contains newline characters.
returns the subset of the headers in requestheaders that impact the content of response's body.
returns a request that creates a tls tunnel via an http proxy. everything in the tunnel request is sent unencrypted to the proxy server, so tunnels include only the minimum set of headers. this avoids sending potentially sensitive data like http cookies to the proxy unencrypted. in order to support preemptive authentication we pass a fake auth failed response to the authenticator. this gives the authenticator the option to customize the connect request. it can decline to do so by returning null, in which case okhttp will use it as-is
track a failure using this connection. this may prevent both the connection and its route from being used for future exchanges.
returns true if this connection is ready to host new streams.
when settings are received, adjust the allocation limit.
to make an https connection over an http proxy, send an unencrypted connect request to create the proxy connection. this may need to be retried if the proxy requires authorization.
returns true if this connection can carry a stream allocation to address. if non-null
returns true if this connection's route has the same address as any of routes.
does all the work necessary to build a full http or https connection on a raw socket.
does all the work to build an https connection over a proxy tunnel. the catch here is that a proxy server can issue an auth challenge and then close the connection.
notify this pool that connection has become idle. returns true if the connection has been removed from the pool and should be closed.
track a bad route in the route database. other routes will be attempted first.
performs maintenance on this pool, evicting the connection that has been idle the longest if either it has exceeded the keep alive limit or the idle connections limit. returns the duration in nanos to sleep until the next scheduled call to this method. returns -1 if no further cleanups are required.
prunes any leaked transmitters and then returns the number of remaining live transmitters on application code has abandoned them. leak detection is imprecise and relies on garbage collection.
attempts to acquire a recycled connection to address for transmitter. returns true if a connection was acquired. if routes is non-null these are the resolved routes (ie. ip addresses) for the connection. this is used to coalesce related domains to the same http2 connection, such as
reports a failure to complete a connection. determines the next connectionspec to try, if any. #configuresecuresocket(sslsocket) or false if not
returns true if any later connectionspec in the fallback strategy looks possible based on the supplied sslsocket. it assumes that a future socket will have the same capabilities as the supplied socket.
configures the supplied sslsocket to connect to the specified host using an appropriate
returns the next proxy to try. may be proxy.no_proxy but never null.
prepares the socket addresses to attempt for the current proxy or host.
prepares the proxy servers to try.
finds a connection and returns it if it is healthy. if it is unhealthy the process is repeated until a healthy connection is found.
returns a connection to host a new stream. this prefers the existing connection if it exists, then the pool, finally building a new connection.
returns true if certificate matches ipaddress.
returns true iff hostname matches the domain name pattern. .android.com.
returns true if certificate matches hostname.
returns a cleaned chain for chain. this method throws if the complete chain to a trusted ca certificate cannot be constructed. this is unexpected unless the trust root index in this class has a different trust manager than what was used to establish chain.
returns true if toverify was signed by signingcert's public key.
parses the dn and returns the most significant attribute value for an attribute type, or null if none found.
returns a new source that writes bytes to cacherequest as they are read by the source consumer. this is careful to discard bytes left over when the stream is closed; otherwise we may never exhaust the source stream and therefore not complete the cached response.
combines cached headers with a network headers as defined by rfc 7234, 4.3.4.
returns an iterator over the cache's current entries. this iterator doesn't throw concurrentmodificationexception, but if new entries are added while iterating, those new entries will not be returned by the iterator. if existing entries are removed during iteration, they will be absent (unless they were already returned). if there are io problems during iteration, this iterator fails silently. for example, if the hosting filesystem becomes unreachable, the iterator will omit elements rather than throwing exceptions. the caller must snapshot#close close each snapshot returned by the returned iterator supports iterator#remove.
returns a snapshot of the entry named key, or null if it doesn't exist is not currently readable. if a value is returned, it is moved to the head of the lru queue.
create a cache which will reside in directory. this cache is lazily initialized on first access and will be created if it does not exist.
append space-prefixed lengths to writer.
returns an unbuffered input stream to read the last committed value, or null if no value has been committed.
deletes all stored values from the cache. in-flight edits will complete normally but their values will not be stored.
closes this cache. stored values will remain on the filesystem.
we only rebuild the journal when it will halve the size of the journal and eliminate at least 2000 ops.
aborts this edit. this releases the edit lock so another edit may be started on the same key.
commits this edit so it is visible to readers. this releases the edit lock so another edit may be started on the same key.
creates a new journal that omits redundant information. this replaces the current journal if it exists.
computes the initial size and collects garbage as a part of opening the cache. dirty entries are assumed to be inconsistent and will be deleted.
force buffered operations to the filesystem.
returns a new unbuffered output stream to write the value at index. if the underlying output stream encounters errors when writing to the filesystem, this edit will be aborted when #commit is called. the returned output stream does not throw ioexceptions.
returns a snapshot of this entry. this opens all streams eagerly to guarantee that we see a single published snapshot. if we opened streams lazily then the streams could come from different edits.
prevents this editor from completing normally. this is necessary either when the edit causes an io error, or if the target entry is evicted while this editor is active. in either case we delete the editor's created files and prevent new files from being created. note that once an editor has been detached it is possible for another editor to edit the entry.
changes the maximum number of bytes the cache can store and queues a job to trim the existing store, if necessary.
drops the entry for key if it exists and can be removed. if the entry for key is currently being edited, that edit will complete normally but its value will not be stored.
returns a strategy to use assuming the request can use the network.
returns true if response can be stored to later serve another request.
returns a strategy to satisfy request using the a cached response response.
returns the number of milliseconds that the response was fresh for, starting from the served date.
for testing: force this web socket to release its threads.
attempts to remove a single frame from a queue and send it. this prefers to write urgent pongs before less urgent messages and close frames. for example it's possible that a caller will enqueue messages followed by pongs, but this sends pongs followed by messages. pongs are always written in the order they were enqueued. if a frame cannot be sent - because there are none enqueued or because the web socket is not connected - this does nothing and returns false. otherwise this returns true and the caller should immediately invoke this method again until it returns false. this method may only be invoked by the writer thread. there may be only thread invoking this method at a time.
receive frames until there are no more. invoked only by the reader thread.
for testing: receive a single frame and return true if there are more frames to read. invoked only by the reader thread.
stream a message payload as a series of frames. this allows control frames to be interleaved between parts of the message.
send a close frame with optional code and reason. href="http:tools.ietf.orghtmlrfc6455#section-7.4">section 7.4 of rfc 6455 or 0.
reads a message body into across one or more frames. control frames that occur between fragments will be processed. if the message payload is masked this will unmask as it's being processed.
attempt to match the host runtime to a capable platform implementation.
returns an object that holds a stack trace created at the moment this method is executed. this should be used specifically for java.io.closeable objects and in conjunction with
returns the concatenation of 8-bit, length prefixed protocol names. http:tools.ietf.orghtmldraft-agl-tls-nextprotoneg-04#page-4
sets the delegate of timeout to timeout#none and resets its underlying timeout to the default configuration. use this to avoid unexpected sharing of timeouts between pooled connections.
reads headers or trailers.
returns bytes of a request header for sending on an http transport.
the response body from a connect should be empty, but if it is not then we should consume it before proceeding.
closes the cache entry and makes the socket available for reuse. this should be invoked when the end of the body has been reached.
except for in tests that don't check for a connection preface.
callers of this method are not thread safe, and sometimes on application threads. most often, this method will be called to send a buffer worth of data to the peer. writes are subject to the write window of the stream and the connection. until there is a window sufficient to send bytecount, the caller will block. for example, a user of write window will block. zero bytecount writes are not subject to flow control and will not block. the only use case for zero bytecount is closing a flushed output stream.
degrades this connection such that new streams can neither be created locally, nor accepted from the remote peer. existing streams are not impacted. this is intended to permit an endpoint to gracefully stop accepting new requests without harming previously established streams.
eagerly reads bytecount bytes from the source before launching a background task to process the data. this avoids corrupting the stream.
returns a new server-initiated stream. corresponds to flag_fin.
merges settings into this peer's settings and sends them to the remote peer.
returns headers for a name value block containing an http2 response.
creates a frame reader with max header table size of 4096.
like #wait, but throws an interruptedioexception when interrupted instead of the more awkward interruptedexception.
accept headers from the network and store them until the client calls #takeheaders, or
returns the trailers. it is only safe to call this once the source stream has been completely exhausted.
abnormally terminate this stream. this enqueues a rst_stream frame and returns immediately.
returns a sink that can be used to write data to the peer. #writeheaders has not yet been sent.
returns true if this stream was closed.
removes and returns the stream's received response headers, blocking if necessary until headers have been received. if the returned list contains multiple blocks of headers the blocks will be delimited by 'null'.
abnormally terminate this stream. this blocks until the rst_stream frame has been transmitted.
emit a single data frame to the connection. the frame's size be limited by this stream's write window. this method will block until the write window is nonempty.
sends a reply to an incoming stream. corresponds to flag_fin. response body exists and will be written immediately.
an http2 response cannot contain uppercase header characters and must be treated as malformed.
reads a potentially huffman encoded byte string.
read bytecount bytes of headers from the source stream. this implementation does not propagate the never indexed flag of a header.
index == -1 when new.
writes other into this. if any setting is populated by this and other, the value and flags from other will be kept.
http2 only. send a push promise header block. a push promise contains all the headers that pertain to a server-initiated request, and a sent as a part of the response to streamid. the promisedstreamid has a priority of one greater than streamid. and :path.
send a connection-level ping to the peer. ack indicates this is a reply. the data in
inform peer that an additional windowsizeincrement bytes can be sent on streamid, or the connection if streamid is zero.
implementations must send multiple frames as necessary. #maxdatalength.
write okhttp's settings to the peer.
tell the peer to stop creating streams and that we last processed lastgoodstreamid, or zero if no streams were processed.
applies peersettings and then sends a settings ack.
construct an internal node.
visible for testing.
returns the effective top-level domain plus one (etld+1) by referencing the public suffix list. returns null if the domain is a public suffix or a private address. here are some examples:  assertequals("google.com", geteffectivetldplusone("google.com")); assertequals("google.com", geteffectivetldplusone("www.google.com")); assertnull(geteffectivetldplusone("com")); assertnull(geteffectivetldplusone("localhost")); assertnull(geteffectivetldplusone("mymacbook"));  encoded.
adds a subject alternative name (san) to the certificate. this is usually a literal hostname, a literal ip address, or a hostname pattern. if no subject alternative names are added that extension will be omitted.
set this certificate to be a signing certificate, with up to maxintermediatecas intermediate signing certificates beneath it. by default this certificate cannot not sign other certificates. set this to 0 so this certificate can sign other certificates (but those certificates cannot themselves sign certificates). set this to 1 so this certificate can sign intermediate certificates that can themselves sign certificates. add one for each additional layer of intermediates to permit.
returns the rsa private key encoded in  href="https:tools.ietf.orghtmlrfc5208">pkcs #8  href="https:tools.ietf.orghtmlrfc7468">pem format.
returns the rsa private key encoded in  href="https:tools.ietf.orghtmlrfc8017">pkcs #1  href="https:tools.ietf.orghtmlrfc7468">pem format.
returns the certificate encoded in  href="https:tools.ietf.orghtmlrfc7468">pem format.
sets the certificate to be valid in [notbefore..notafter]. both endpoints are specified in the format of system#currenttimemillis(). specify -1l for both values to use the default interval, 24 hours starting when the certificate is created.
configure the certificate chain to use when being authenticated. the first certificate is the held certificate, further certificates are included in the handshake so the peer can build a trusted path to a trusted root certificate. the chain should include all intermediate certificates but does not need the root certificate that we expect to be known by the remote peer. the peer already has that certificate so transmitting it is unnecessary.
returns an ssl client for this host's localhost address.
returns a trust manager that trusts trustedcertificates.
returns a key manager for the held certificate and its chain. returns an empty key manager if
indicates the protocols supported by alpn on incoming https connections. this list is ignored when #setprotocolnegotiationenabled negotiation is disabled.
returns a url for connecting to this server.
reads a request and writes its response. returns true if further calls should be attempted on the socket.
starts the server and binds to the given socket address.
transfer bytes from source to sink until either bytecount bytes have been transferred or source is exhausted. the transfer is throttled according to policy.
respond to connect requests until a switch_to_ssl_at_end response is dispatched.
sets the response body to body, chunked every maxchunksize bytes.
returns once the duplex conversation completes successfully.
